[2021-11-03 20:49:59,707][ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephuser/.cephdeploy.conf
[2021-11-03 20:49:59,707][ceph_deploy.cli][INFO  ] Invoked (1.5.39): /bin/ceph-deploy new mon1
[2021-11-03 20:49:59,707][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2021-11-03 20:49:59,707][ceph_deploy.cli][INFO  ]  username                      : None
[2021-11-03 20:49:59,707][ceph_deploy.cli][INFO  ]  func                          : <function new at 0x7f74b440b668>
[2021-11-03 20:49:59,707][ceph_deploy.cli][INFO  ]  verbose                       : False
[2021-11-03 20:49:59,708][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2021-11-03 20:49:59,708][ceph_deploy.cli][INFO  ]  quiet                         : False
[2021-11-03 20:49:59,708][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f74b3b7eef0>
[2021-11-03 20:49:59,708][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2021-11-03 20:49:59,708][ceph_deploy.cli][INFO  ]  ssh_copykey                   : True
[2021-11-03 20:49:59,708][ceph_deploy.cli][INFO  ]  mon                           : ['mon1']
[2021-11-03 20:49:59,708][ceph_deploy.cli][INFO  ]  public_network                : None
[2021-11-03 20:49:59,708][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2021-11-03 20:49:59,708][ceph_deploy.cli][INFO  ]  cluster_network               : None
[2021-11-03 20:49:59,708][ceph_deploy.cli][INFO  ]  default_release               : False
[2021-11-03 20:49:59,708][ceph_deploy.cli][INFO  ]  fsid                          : None
[2021-11-03 20:49:59,708][ceph_deploy.new][DEBUG ] Creating new cluster named ceph
[2021-11-03 20:49:59,708][ceph_deploy.new][INFO  ] making sure passwordless SSH succeeds
[2021-11-03 20:49:59,735][mon1][DEBUG ] connected to host: ceph-admin 
[2021-11-03 20:49:59,740][mon1][INFO  ] Running command: ssh -CT -o BatchMode=yes mon1
[2021-11-03 20:50:00,069][mon1][DEBUG ] connection detected need for sudo
[2021-11-03 20:50:00,236][mon1][DEBUG ] connected to host: mon1 
[2021-11-03 20:50:00,237][mon1][DEBUG ] detect platform information from remote host
[2021-11-03 20:50:00,254][mon1][DEBUG ] detect machine type
[2021-11-03 20:50:00,258][mon1][DEBUG ] find the location of an executable
[2021-11-03 20:50:00,261][mon1][INFO  ] Running command: sudo /usr/sbin/ip link show
[2021-11-03 20:50:00,271][mon1][INFO  ] Running command: sudo /usr/sbin/ip addr show
[2021-11-03 20:50:00,280][mon1][DEBUG ] IP addresses found: [u'192.168.1.93']
[2021-11-03 20:50:00,280][ceph_deploy.new][DEBUG ] Resolving host mon1
[2021-11-03 20:50:00,280][ceph_deploy.new][DEBUG ] Monitor mon1 at 192.168.1.93
[2021-11-03 20:50:00,280][ceph_deploy.new][DEBUG ] Monitor initial members are ['mon1']
[2021-11-03 20:50:00,280][ceph_deploy.new][DEBUG ] Monitor addrs are ['192.168.1.93']
[2021-11-03 20:50:00,280][ceph_deploy.new][DEBUG ] Creating a random mon key...
[2021-11-03 20:50:00,280][ceph_deploy.new][DEBUG ] Writing monitor keyring to ceph.mon.keyring...
[2021-11-03 20:50:00,280][ceph_deploy.new][DEBUG ] Writing initial config to ceph.conf...
[2021-11-03 20:50:28,076][ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephuser/.cephdeploy.conf
[2021-11-03 20:50:28,077][ceph_deploy.cli][INFO  ] Invoked (1.5.39): /bin/ceph-deploy mon create-initial
[2021-11-03 20:50:28,077][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2021-11-03 20:50:28,077][ceph_deploy.cli][INFO  ]  username                      : None
[2021-11-03 20:50:28,077][ceph_deploy.cli][INFO  ]  verbose                       : False
[2021-11-03 20:50:28,077][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2021-11-03 20:50:28,077][ceph_deploy.cli][INFO  ]  subcommand                    : create-initial
[2021-11-03 20:50:28,078][ceph_deploy.cli][INFO  ]  quiet                         : False
[2021-11-03 20:50:28,078][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fdde58ccef0>
[2021-11-03 20:50:28,078][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2021-11-03 20:50:28,078][ceph_deploy.cli][INFO  ]  func                          : <function mon at 0x7fdde58b2758>
[2021-11-03 20:50:28,078][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2021-11-03 20:50:28,078][ceph_deploy.cli][INFO  ]  default_release               : False
[2021-11-03 20:50:28,078][ceph_deploy.cli][INFO  ]  keyrings                      : None
[2021-11-03 20:50:28,079][ceph_deploy.mon][DEBUG ] Deploying mon, cluster ceph hosts mon1
[2021-11-03 20:50:28,079][ceph_deploy.mon][DEBUG ] detecting platform for host mon1 ...
[2021-11-03 20:50:28,251][mon1][DEBUG ] connection detected need for sudo
[2021-11-03 20:50:28,425][mon1][DEBUG ] connected to host: mon1 
[2021-11-03 20:50:28,426][mon1][DEBUG ] detect platform information from remote host
[2021-11-03 20:50:28,445][mon1][DEBUG ] detect machine type
[2021-11-03 20:50:28,450][mon1][DEBUG ] find the location of an executable
[2021-11-03 20:50:28,452][ceph_deploy.mon][INFO  ] distro info: CentOS Linux 7.9.2009 Core
[2021-11-03 20:50:28,452][mon1][DEBUG ] determining if provided host has same hostname in remote
[2021-11-03 20:50:28,452][mon1][DEBUG ] get remote short hostname
[2021-11-03 20:50:28,453][mon1][DEBUG ] deploying mon to mon1
[2021-11-03 20:50:28,453][mon1][DEBUG ] get remote short hostname
[2021-11-03 20:50:28,454][mon1][DEBUG ] remote hostname: mon1
[2021-11-03 20:50:28,456][mon1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2021-11-03 20:50:28,458][ceph_deploy.mon][ERROR ] RuntimeError: config file /etc/ceph/ceph.conf exists with different content; use --overwrite-conf to overwrite
[2021-11-03 20:50:28,458][ceph_deploy][ERROR ] GenericError: Failed to create 1 monitors

[2021-11-03 20:50:42,309][ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephuser/.cephdeploy.conf
[2021-11-03 20:50:42,309][ceph_deploy.cli][INFO  ] Invoked (1.5.39): /bin/ceph-deploy --overwrite-conf mon create-initial
[2021-11-03 20:50:42,309][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2021-11-03 20:50:42,309][ceph_deploy.cli][INFO  ]  username                      : None
[2021-11-03 20:50:42,309][ceph_deploy.cli][INFO  ]  verbose                       : False
[2021-11-03 20:50:42,309][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2021-11-03 20:50:42,309][ceph_deploy.cli][INFO  ]  subcommand                    : create-initial
[2021-11-03 20:50:42,309][ceph_deploy.cli][INFO  ]  quiet                         : False
[2021-11-03 20:50:42,309][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f8d981b4ef0>
[2021-11-03 20:50:42,309][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2021-11-03 20:50:42,310][ceph_deploy.cli][INFO  ]  func                          : <function mon at 0x7f8d98199758>
[2021-11-03 20:50:42,310][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2021-11-03 20:50:42,310][ceph_deploy.cli][INFO  ]  default_release               : False
[2021-11-03 20:50:42,310][ceph_deploy.cli][INFO  ]  keyrings                      : None
[2021-11-03 20:50:42,311][ceph_deploy.mon][DEBUG ] Deploying mon, cluster ceph hosts mon1
[2021-11-03 20:50:42,311][ceph_deploy.mon][DEBUG ] detecting platform for host mon1 ...
[2021-11-03 20:50:42,476][mon1][DEBUG ] connection detected need for sudo
[2021-11-03 20:50:42,643][mon1][DEBUG ] connected to host: mon1 
[2021-11-03 20:50:42,643][mon1][DEBUG ] detect platform information from remote host
[2021-11-03 20:50:42,660][mon1][DEBUG ] detect machine type
[2021-11-03 20:50:42,665][mon1][DEBUG ] find the location of an executable
[2021-11-03 20:50:42,666][ceph_deploy.mon][INFO  ] distro info: CentOS Linux 7.9.2009 Core
[2021-11-03 20:50:42,666][mon1][DEBUG ] determining if provided host has same hostname in remote
[2021-11-03 20:50:42,666][mon1][DEBUG ] get remote short hostname
[2021-11-03 20:50:42,667][mon1][DEBUG ] deploying mon to mon1
[2021-11-03 20:50:42,668][mon1][DEBUG ] get remote short hostname
[2021-11-03 20:50:42,669][mon1][DEBUG ] remote hostname: mon1
[2021-11-03 20:50:42,671][mon1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2021-11-03 20:50:42,673][mon1][DEBUG ] create the mon path if it does not exist
[2021-11-03 20:50:42,674][mon1][DEBUG ] checking for done path: /var/lib/ceph/mon/ceph-mon1/done
[2021-11-03 20:50:42,675][mon1][DEBUG ] done path does not exist: /var/lib/ceph/mon/ceph-mon1/done
[2021-11-03 20:50:42,676][mon1][INFO  ] creating keyring file: /var/lib/ceph/tmp/ceph-mon1.mon.keyring
[2021-11-03 20:50:42,676][mon1][DEBUG ] create the monitor keyring file
[2021-11-03 20:50:42,679][mon1][INFO  ] Running command: sudo ceph-mon --cluster ceph --mkfs -i mon1 --keyring /var/lib/ceph/tmp/ceph-mon1.mon.keyring --setuser 167 --setgroup 167
[2021-11-03 20:50:42,747][mon1][DEBUG ] ceph-mon: renaming mon.noname-a 192.168.1.93:6789/0 to mon.mon1
[2021-11-03 20:50:42,748][mon1][DEBUG ] ceph-mon: set fsid to 67d9e224-a6f7-43d3-ae36-e6100c59258e
[2021-11-03 20:50:42,861][mon1][DEBUG ] ceph-mon: created monfs at /var/lib/ceph/mon/ceph-mon1 for mon.mon1
[2021-11-03 20:50:42,862][mon1][INFO  ] unlinking keyring file /var/lib/ceph/tmp/ceph-mon1.mon.keyring
[2021-11-03 20:50:42,863][mon1][DEBUG ] create a done file to avoid re-doing the mon deployment
[2021-11-03 20:50:42,864][mon1][DEBUG ] create the init path if it does not exist
[2021-11-03 20:50:42,866][mon1][INFO  ] Running command: sudo systemctl enable ceph.target
[2021-11-03 20:50:42,934][mon1][INFO  ] Running command: sudo systemctl enable ceph-mon@mon1
[2021-11-03 20:50:42,944][mon1][WARNING] Created symlink from /etc/systemd/system/ceph-mon.target.wants/ceph-mon@mon1.service to /usr/lib/systemd/system/ceph-mon@.service.
[2021-11-03 20:50:43,010][mon1][INFO  ] Running command: sudo systemctl start ceph-mon@mon1
[2021-11-03 20:50:45,097][mon1][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.mon1.asok mon_status
[2021-11-03 20:50:45,212][mon1][DEBUG ] ********************************************************************************
[2021-11-03 20:50:45,212][mon1][DEBUG ] status for monitor: mon.mon1
[2021-11-03 20:50:45,213][mon1][DEBUG ] {
[2021-11-03 20:50:45,213][mon1][DEBUG ]   "election_epoch": 3, 
[2021-11-03 20:50:45,213][mon1][DEBUG ]   "extra_probe_peers": [], 
[2021-11-03 20:50:45,213][mon1][DEBUG ]   "monmap": {
[2021-11-03 20:50:45,213][mon1][DEBUG ]     "created": "2021-11-03 20:50:42.718657", 
[2021-11-03 20:50:45,213][mon1][DEBUG ]     "epoch": 1, 
[2021-11-03 20:50:45,213][mon1][DEBUG ]     "fsid": "67d9e224-a6f7-43d3-ae36-e6100c59258e", 
[2021-11-03 20:50:45,213][mon1][DEBUG ]     "modified": "2021-11-03 20:50:42.718657", 
[2021-11-03 20:50:45,213][mon1][DEBUG ]     "mons": [
[2021-11-03 20:50:45,213][mon1][DEBUG ]       {
[2021-11-03 20:50:45,213][mon1][DEBUG ]         "addr": "192.168.1.93:6789/0", 
[2021-11-03 20:50:45,213][mon1][DEBUG ]         "name": "mon1", 
[2021-11-03 20:50:45,213][mon1][DEBUG ]         "rank": 0
[2021-11-03 20:50:45,213][mon1][DEBUG ]       }
[2021-11-03 20:50:45,213][mon1][DEBUG ]     ]
[2021-11-03 20:50:45,213][mon1][DEBUG ]   }, 
[2021-11-03 20:50:45,213][mon1][DEBUG ]   "name": "mon1", 
[2021-11-03 20:50:45,213][mon1][DEBUG ]   "outside_quorum": [], 
[2021-11-03 20:50:45,213][mon1][DEBUG ]   "quorum": [
[2021-11-03 20:50:45,213][mon1][DEBUG ]     0
[2021-11-03 20:50:45,213][mon1][DEBUG ]   ], 
[2021-11-03 20:50:45,213][mon1][DEBUG ]   "rank": 0, 
[2021-11-03 20:50:45,214][mon1][DEBUG ]   "state": "leader", 
[2021-11-03 20:50:45,214][mon1][DEBUG ]   "sync_provider": []
[2021-11-03 20:50:45,214][mon1][DEBUG ] }
[2021-11-03 20:50:45,214][mon1][DEBUG ] ********************************************************************************
[2021-11-03 20:50:45,214][mon1][INFO  ] monitor: mon.mon1 is running
[2021-11-03 20:50:45,218][mon1][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.mon1.asok mon_status
[2021-11-03 20:50:45,282][ceph_deploy.mon][INFO  ] processing monitor mon.mon1
[2021-11-03 20:50:45,446][mon1][DEBUG ] connection detected need for sudo
[2021-11-03 20:50:45,613][mon1][DEBUG ] connected to host: mon1 
[2021-11-03 20:50:45,613][mon1][DEBUG ] detect platform information from remote host
[2021-11-03 20:50:45,630][mon1][DEBUG ] detect machine type
[2021-11-03 20:50:45,635][mon1][DEBUG ] find the location of an executable
[2021-11-03 20:50:45,637][mon1][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.mon1.asok mon_status
[2021-11-03 20:50:45,702][ceph_deploy.mon][INFO  ] mon.mon1 monitor has reached quorum!
[2021-11-03 20:50:45,702][ceph_deploy.mon][INFO  ] all initial monitors are running and have formed quorum
[2021-11-03 20:50:45,702][ceph_deploy.mon][INFO  ] Running gatherkeys...
[2021-11-03 20:50:45,703][ceph_deploy.gatherkeys][INFO  ] Storing keys in temp directory /tmp/tmpOCqEkb
[2021-11-03 20:50:45,866][mon1][DEBUG ] connection detected need for sudo
[2021-11-03 20:50:46,033][mon1][DEBUG ] connected to host: mon1 
[2021-11-03 20:50:46,034][mon1][DEBUG ] detect platform information from remote host
[2021-11-03 20:50:46,051][mon1][DEBUG ] detect machine type
[2021-11-03 20:50:46,055][mon1][DEBUG ] get remote short hostname
[2021-11-03 20:50:46,057][mon1][DEBUG ] fetch remote file
[2021-11-03 20:50:46,059][mon1][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --admin-daemon=/var/run/ceph/ceph-mon.mon1.asok mon_status
[2021-11-03 20:50:46,125][mon1][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-mon1/keyring auth get client.admin
[2021-11-03 20:50:46,292][mon1][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-mon1/keyring auth get client.bootstrap-mds
[2021-11-03 20:50:46,459][mon1][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-mon1/keyring auth get client.bootstrap-mgr
[2021-11-03 20:50:46,626][mon1][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-mon1/keyring auth get-or-create client.bootstrap-mgr mon allow profile bootstrap-mgr
[2021-11-03 20:50:46,893][mon1][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-mon1/keyring auth get client.bootstrap-osd
[2021-11-03 20:50:47,060][mon1][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-mon1/keyring auth get client.bootstrap-rgw
[2021-11-03 20:50:47,225][ceph_deploy.gatherkeys][INFO  ] Storing ceph.client.admin.keyring
[2021-11-03 20:50:47,225][ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-mds.keyring
[2021-11-03 20:50:47,225][ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-mgr.keyring
[2021-11-03 20:50:47,226][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.mon.keyring' already exists
[2021-11-03 20:50:47,226][ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-osd.keyring
[2021-11-03 20:50:47,226][ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-rgw.keyring
[2021-11-03 20:50:47,226][ceph_deploy.gatherkeys][INFO  ] Destroy temp directory /tmp/tmpOCqEkb
[2021-11-03 20:50:52,628][ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephuser/.cephdeploy.conf
[2021-11-03 20:50:52,629][ceph_deploy.cli][INFO  ] Invoked (1.5.39): /bin/ceph-deploy gatherkeys mon1
[2021-11-03 20:50:52,629][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2021-11-03 20:50:52,629][ceph_deploy.cli][INFO  ]  username                      : None
[2021-11-03 20:50:52,629][ceph_deploy.cli][INFO  ]  verbose                       : False
[2021-11-03 20:50:52,629][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2021-11-03 20:50:52,629][ceph_deploy.cli][INFO  ]  quiet                         : False
[2021-11-03 20:50:52,629][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f641415c830>
[2021-11-03 20:50:52,629][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2021-11-03 20:50:52,629][ceph_deploy.cli][INFO  ]  mon                           : ['mon1']
[2021-11-03 20:50:52,629][ceph_deploy.cli][INFO  ]  func                          : <function gatherkeys at 0x7f641411f230>
[2021-11-03 20:50:52,629][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2021-11-03 20:50:52,629][ceph_deploy.cli][INFO  ]  default_release               : False
[2021-11-03 20:50:52,630][ceph_deploy.gatherkeys][INFO  ] Storing keys in temp directory /tmp/tmpPe71S_
[2021-11-03 20:50:52,800][mon1][DEBUG ] connection detected need for sudo
[2021-11-03 20:50:52,974][mon1][DEBUG ] connected to host: mon1 
[2021-11-03 20:50:52,974][mon1][DEBUG ] detect platform information from remote host
[2021-11-03 20:50:52,992][mon1][DEBUG ] detect machine type
[2021-11-03 20:50:52,997][mon1][DEBUG ] get remote short hostname
[2021-11-03 20:50:52,998][mon1][DEBUG ] fetch remote file
[2021-11-03 20:50:53,001][mon1][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --admin-daemon=/var/run/ceph/ceph-mon.mon1.asok mon_status
[2021-11-03 20:50:53,120][mon1][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-mon1/keyring auth get client.admin
[2021-11-03 20:50:53,287][mon1][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-mon1/keyring auth get client.bootstrap-mds
[2021-11-03 20:50:53,454][mon1][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-mon1/keyring auth get client.bootstrap-mgr
[2021-11-03 20:50:53,621][mon1][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-mon1/keyring auth get client.bootstrap-osd
[2021-11-03 20:50:53,788][mon1][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-mon1/keyring auth get client.bootstrap-rgw
[2021-11-03 20:50:53,953][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.client.admin.keyring' already exists
[2021-11-03 20:50:53,954][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.bootstrap-mds.keyring' already exists
[2021-11-03 20:50:53,954][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.bootstrap-mgr.keyring' already exists
[2021-11-03 20:50:53,954][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.mon.keyring' already exists
[2021-11-03 20:50:53,954][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.bootstrap-osd.keyring' already exists
[2021-11-03 20:50:53,954][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.bootstrap-rgw.keyring' already exists
[2021-11-03 20:50:53,954][ceph_deploy.gatherkeys][INFO  ] Destroy temp directory /tmp/tmpPe71S_
[2021-11-03 20:51:03,578][ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephuser/.cephdeploy.conf
[2021-11-03 20:51:03,578][ceph_deploy.cli][INFO  ] Invoked (1.5.39): /bin/ceph-deploy disk list osd1 osd2 osd3 osd4 osd5
[2021-11-03 20:51:03,578][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2021-11-03 20:51:03,578][ceph_deploy.cli][INFO  ]  username                      : None
[2021-11-03 20:51:03,579][ceph_deploy.cli][INFO  ]  verbose                       : False
[2021-11-03 20:51:03,579][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2021-11-03 20:51:03,579][ceph_deploy.cli][INFO  ]  subcommand                    : list
[2021-11-03 20:51:03,579][ceph_deploy.cli][INFO  ]  quiet                         : False
[2021-11-03 20:51:03,579][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f6ca8df6ea8>
[2021-11-03 20:51:03,579][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2021-11-03 20:51:03,579][ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7f6ca8dd10c8>
[2021-11-03 20:51:03,579][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2021-11-03 20:51:03,579][ceph_deploy.cli][INFO  ]  default_release               : False
[2021-11-03 20:51:03,579][ceph_deploy.cli][INFO  ]  disk                          : [('osd1', None, None), ('osd2', None, None), ('osd3', None, None), ('osd4', None, None), ('osd5', None, None)]
[2021-11-03 20:51:03,746][osd1][DEBUG ] connection detected need for sudo
[2021-11-03 20:51:03,917][osd1][DEBUG ] connected to host: osd1 
[2021-11-03 20:51:03,917][osd1][DEBUG ] detect platform information from remote host
[2021-11-03 20:51:03,935][osd1][DEBUG ] detect machine type
[2021-11-03 20:51:03,940][osd1][DEBUG ] find the location of an executable
[2021-11-03 20:51:03,941][ceph_deploy.osd][INFO  ] Distro info: CentOS Linux 7.9.2009 Core
[2021-11-03 20:51:03,941][ceph_deploy.osd][DEBUG ] Listing disks on osd1...
[2021-11-03 20:51:03,941][osd1][DEBUG ] find the location of an executable
[2021-11-03 20:51:03,944][osd1][INFO  ] Running command: sudo /usr/sbin/ceph-disk list
[2021-11-03 20:51:04,263][osd1][DEBUG ] /dev/dm-0 other, xfs, mounted on /
[2021-11-03 20:51:04,263][osd1][DEBUG ] /dev/dm-1 swap, swap
[2021-11-03 20:51:04,263][osd1][DEBUG ] /dev/sda :
[2021-11-03 20:51:04,263][osd1][DEBUG ]  /dev/sda2 other, LVM2_member
[2021-11-03 20:51:04,263][osd1][DEBUG ]  /dev/sda1 other, xfs, mounted on /boot
[2021-11-03 20:51:04,263][osd1][DEBUG ] /dev/sdb other, xfs
[2021-11-03 20:51:04,264][osd1][DEBUG ] /dev/sr0 other, unknown
[2021-11-03 20:51:04,420][osd2][DEBUG ] connection detected need for sudo
[2021-11-03 20:51:04,591][osd2][DEBUG ] connected to host: osd2 
[2021-11-03 20:51:04,592][osd2][DEBUG ] detect platform information from remote host
[2021-11-03 20:51:04,611][osd2][DEBUG ] detect machine type
[2021-11-03 20:51:04,616][osd2][DEBUG ] find the location of an executable
[2021-11-03 20:51:04,617][ceph_deploy.osd][INFO  ] Distro info: CentOS Linux 7.9.2009 Core
[2021-11-03 20:51:04,617][ceph_deploy.osd][DEBUG ] Listing disks on osd2...
[2021-11-03 20:51:04,617][osd2][DEBUG ] find the location of an executable
[2021-11-03 20:51:04,621][osd2][INFO  ] Running command: sudo /usr/sbin/ceph-disk list
[2021-11-03 20:51:04,888][osd2][DEBUG ] /dev/dm-0 other, xfs, mounted on /
[2021-11-03 20:51:04,888][osd2][DEBUG ] /dev/dm-1 swap, swap
[2021-11-03 20:51:04,888][osd2][DEBUG ] /dev/sda :
[2021-11-03 20:51:04,888][osd2][DEBUG ]  /dev/sda2 other, LVM2_member
[2021-11-03 20:51:04,888][osd2][DEBUG ]  /dev/sda1 other, xfs, mounted on /boot
[2021-11-03 20:51:04,888][osd2][DEBUG ] /dev/sdb other, xfs
[2021-11-03 20:51:04,888][osd2][DEBUG ] /dev/sr0 other, unknown
[2021-11-03 20:51:05,044][osd3][DEBUG ] connection detected need for sudo
[2021-11-03 20:51:05,215][osd3][DEBUG ] connected to host: osd3 
[2021-11-03 20:51:05,216][osd3][DEBUG ] detect platform information from remote host
[2021-11-03 20:51:05,233][osd3][DEBUG ] detect machine type
[2021-11-03 20:51:05,238][osd3][DEBUG ] find the location of an executable
[2021-11-03 20:51:05,239][ceph_deploy.osd][INFO  ] Distro info: CentOS Linux 7.9.2009 Core
[2021-11-03 20:51:05,239][ceph_deploy.osd][DEBUG ] Listing disks on osd3...
[2021-11-03 20:51:05,239][osd3][DEBUG ] find the location of an executable
[2021-11-03 20:51:05,242][osd3][INFO  ] Running command: sudo /usr/sbin/ceph-disk list
[2021-11-03 20:51:05,409][osd3][DEBUG ] /dev/dm-0 other, xfs, mounted on /
[2021-11-03 20:51:05,409][osd3][DEBUG ] /dev/dm-1 swap, swap
[2021-11-03 20:51:05,409][osd3][DEBUG ] /dev/sda :
[2021-11-03 20:51:05,409][osd3][DEBUG ]  /dev/sda2 other, LVM2_member
[2021-11-03 20:51:05,409][osd3][DEBUG ]  /dev/sda1 other, xfs, mounted on /boot
[2021-11-03 20:51:05,409][osd3][DEBUG ] /dev/sdb other, xfs
[2021-11-03 20:51:05,562][osd4][DEBUG ] connection detected need for sudo
[2021-11-03 20:51:05,730][osd4][DEBUG ] connected to host: osd4 
[2021-11-03 20:51:05,730][osd4][DEBUG ] detect platform information from remote host
[2021-11-03 20:51:05,747][osd4][DEBUG ] detect machine type
[2021-11-03 20:51:05,752][osd4][DEBUG ] find the location of an executable
[2021-11-03 20:51:05,753][ceph_deploy.osd][INFO  ] Distro info: CentOS Linux 7.9.2009 Core
[2021-11-03 20:51:05,753][ceph_deploy.osd][DEBUG ] Listing disks on osd4...
[2021-11-03 20:51:05,754][osd4][DEBUG ] find the location of an executable
[2021-11-03 20:51:05,756][osd4][INFO  ] Running command: sudo /usr/sbin/ceph-disk list
[2021-11-03 20:51:05,923][osd4][DEBUG ] /dev/dm-0 other, xfs, mounted on /
[2021-11-03 20:51:05,923][osd4][DEBUG ] /dev/dm-1 swap, swap
[2021-11-03 20:51:05,923][osd4][DEBUG ] /dev/sda :
[2021-11-03 20:51:05,923][osd4][DEBUG ]  /dev/sda2 other, LVM2_member
[2021-11-03 20:51:05,923][osd4][DEBUG ]  /dev/sda1 other, xfs, mounted on /boot
[2021-11-03 20:51:05,923][osd4][DEBUG ] /dev/sdb other, xfs
[2021-11-03 20:51:06,076][osd5][DEBUG ] connection detected need for sudo
[2021-11-03 20:51:06,243][osd5][DEBUG ] connected to host: osd5 
[2021-11-03 20:51:06,244][osd5][DEBUG ] detect platform information from remote host
[2021-11-03 20:51:06,260][osd5][DEBUG ] detect machine type
[2021-11-03 20:51:06,265][osd5][DEBUG ] find the location of an executable
[2021-11-03 20:51:06,266][ceph_deploy.osd][INFO  ] Distro info: CentOS Linux 7.9.2009 Core
[2021-11-03 20:51:06,266][ceph_deploy.osd][DEBUG ] Listing disks on osd5...
[2021-11-03 20:51:06,266][osd5][DEBUG ] find the location of an executable
[2021-11-03 20:51:06,269][osd5][INFO  ] Running command: sudo /usr/sbin/ceph-disk list
[2021-11-03 20:51:06,436][osd5][DEBUG ] /dev/dm-0 other, xfs, mounted on /
[2021-11-03 20:51:06,436][osd5][DEBUG ] /dev/dm-1 swap, swap
[2021-11-03 20:51:06,436][osd5][DEBUG ] /dev/sda :
[2021-11-03 20:51:06,436][osd5][DEBUG ]  /dev/sda2 other, LVM2_member
[2021-11-03 20:51:06,436][osd5][DEBUG ]  /dev/sda1 other, xfs, mounted on /boot
[2021-11-03 20:51:06,436][osd5][DEBUG ] /dev/sdb other, xfs
[2021-11-03 20:51:28,170][ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephuser/.cephdeploy.conf
[2021-11-03 20:51:28,170][ceph_deploy.cli][INFO  ] Invoked (1.5.39): /bin/ceph-deploy disk zap osd1:/dev/sdb osd2:/dev/sdb osd3:/dev/sdb osd4:/dev/sdb osd5:/dev/sdb
[2021-11-03 20:51:28,170][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2021-11-03 20:51:28,170][ceph_deploy.cli][INFO  ]  username                      : None
[2021-11-03 20:51:28,170][ceph_deploy.cli][INFO  ]  verbose                       : False
[2021-11-03 20:51:28,170][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2021-11-03 20:51:28,170][ceph_deploy.cli][INFO  ]  subcommand                    : zap
[2021-11-03 20:51:28,170][ceph_deploy.cli][INFO  ]  quiet                         : False
[2021-11-03 20:51:28,170][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f6ff33a3ea8>
[2021-11-03 20:51:28,170][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2021-11-03 20:51:28,171][ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7f6ff337e0c8>
[2021-11-03 20:51:28,171][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2021-11-03 20:51:28,171][ceph_deploy.cli][INFO  ]  default_release               : False
[2021-11-03 20:51:28,171][ceph_deploy.cli][INFO  ]  disk                          : [('osd1', '/dev/sdb', None), ('osd2', '/dev/sdb', None), ('osd3', '/dev/sdb', None), ('osd4', '/dev/sdb', None), ('osd5', '/dev/sdb', None)]
[2021-11-03 20:51:28,171][ceph_deploy.osd][DEBUG ] zapping /dev/sdb on osd1
[2021-11-03 20:51:28,339][osd1][DEBUG ] connection detected need for sudo
[2021-11-03 20:51:28,516][osd1][DEBUG ] connected to host: osd1 
[2021-11-03 20:51:28,517][osd1][DEBUG ] detect platform information from remote host
[2021-11-03 20:51:28,534][osd1][DEBUG ] detect machine type
[2021-11-03 20:51:28,539][osd1][DEBUG ] find the location of an executable
[2021-11-03 20:51:28,541][ceph_deploy.osd][INFO  ] Distro info: CentOS Linux 7.9.2009 Core
[2021-11-03 20:51:28,541][osd1][DEBUG ] zeroing last few blocks of device
[2021-11-03 20:51:28,542][osd1][DEBUG ] find the location of an executable
[2021-11-03 20:51:28,545][osd1][INFO  ] Running command: sudo /usr/sbin/ceph-disk zap /dev/sdb
[2021-11-03 20:51:29,817][osd1][DEBUG ] Creating new GPT entries.
[2021-11-03 20:51:29,817][osd1][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[2021-11-03 20:51:29,817][osd1][DEBUG ] other utilities.
[2021-11-03 20:51:30,884][osd1][DEBUG ] Creating new GPT entries.
[2021-11-03 20:51:30,884][osd1][DEBUG ] The operation has completed successfully.
[2021-11-03 20:51:30,948][ceph_deploy.osd][DEBUG ] zapping /dev/sdb on osd2
[2021-11-03 20:51:31,104][osd2][DEBUG ] connection detected need for sudo
[2021-11-03 20:51:31,276][osd2][DEBUG ] connected to host: osd2 
[2021-11-03 20:51:31,277][osd2][DEBUG ] detect platform information from remote host
[2021-11-03 20:51:31,295][osd2][DEBUG ] detect machine type
[2021-11-03 20:51:31,300][osd2][DEBUG ] find the location of an executable
[2021-11-03 20:51:31,301][ceph_deploy.osd][INFO  ] Distro info: CentOS Linux 7.9.2009 Core
[2021-11-03 20:51:31,301][osd2][DEBUG ] zeroing last few blocks of device
[2021-11-03 20:51:31,303][osd2][DEBUG ] find the location of an executable
[2021-11-03 20:51:31,305][osd2][INFO  ] Running command: sudo /usr/sbin/ceph-disk zap /dev/sdb
[2021-11-03 20:51:32,575][osd2][DEBUG ] Creating new GPT entries.
[2021-11-03 20:51:32,576][osd2][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[2021-11-03 20:51:32,576][osd2][DEBUG ] other utilities.
[2021-11-03 20:51:33,598][osd2][DEBUG ] Creating new GPT entries.
[2021-11-03 20:51:33,599][osd2][DEBUG ] The operation has completed successfully.
[2021-11-03 20:51:33,663][ceph_deploy.osd][DEBUG ] zapping /dev/sdb on osd3
[2021-11-03 20:51:33,820][osd3][DEBUG ] connection detected need for sudo
[2021-11-03 20:51:33,989][osd3][DEBUG ] connected to host: osd3 
[2021-11-03 20:51:33,990][osd3][DEBUG ] detect platform information from remote host
[2021-11-03 20:51:34,007][osd3][DEBUG ] detect machine type
[2021-11-03 20:51:34,013][osd3][DEBUG ] find the location of an executable
[2021-11-03 20:51:34,017][ceph_deploy.osd][INFO  ] Distro info: CentOS Linux 7.9.2009 Core
[2021-11-03 20:51:34,018][osd3][DEBUG ] zeroing last few blocks of device
[2021-11-03 20:51:34,020][osd3][DEBUG ] find the location of an executable
[2021-11-03 20:51:34,022][osd3][INFO  ] Running command: sudo /usr/sbin/ceph-disk zap /dev/sdb
[2021-11-03 20:51:35,255][osd3][DEBUG ] Creating new GPT entries.
[2021-11-03 20:51:35,255][osd3][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[2021-11-03 20:51:35,255][osd3][DEBUG ] other utilities.
[2021-11-03 20:51:36,276][osd3][DEBUG ] Creating new GPT entries.
[2021-11-03 20:51:36,276][osd3][DEBUG ] The operation has completed successfully.
[2021-11-03 20:51:36,340][ceph_deploy.osd][DEBUG ] zapping /dev/sdb on osd4
[2021-11-03 20:51:36,491][osd4][DEBUG ] connection detected need for sudo
[2021-11-03 20:51:36,656][osd4][DEBUG ] connected to host: osd4 
[2021-11-03 20:51:36,657][osd4][DEBUG ] detect platform information from remote host
[2021-11-03 20:51:36,673][osd4][DEBUG ] detect machine type
[2021-11-03 20:51:36,677][osd4][DEBUG ] find the location of an executable
[2021-11-03 20:51:36,679][ceph_deploy.osd][INFO  ] Distro info: CentOS Linux 7.9.2009 Core
[2021-11-03 20:51:36,679][osd4][DEBUG ] zeroing last few blocks of device
[2021-11-03 20:51:36,680][osd4][DEBUG ] find the location of an executable
[2021-11-03 20:51:36,682][osd4][INFO  ] Running command: sudo /usr/sbin/ceph-disk zap /dev/sdb
[2021-11-03 20:51:37,902][osd4][DEBUG ] Creating new GPT entries.
[2021-11-03 20:51:37,902][osd4][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[2021-11-03 20:51:37,902][osd4][DEBUG ] other utilities.
[2021-11-03 20:51:38,919][osd4][DEBUG ] Creating new GPT entries.
[2021-11-03 20:51:38,919][osd4][DEBUG ] The operation has completed successfully.
[2021-11-03 20:51:38,983][ceph_deploy.osd][DEBUG ] zapping /dev/sdb on osd5
[2021-11-03 20:51:39,140][osd5][DEBUG ] connection detected need for sudo
[2021-11-03 20:51:39,312][osd5][DEBUG ] connected to host: osd5 
[2021-11-03 20:51:39,312][osd5][DEBUG ] detect platform information from remote host
[2021-11-03 20:51:39,330][osd5][DEBUG ] detect machine type
[2021-11-03 20:51:39,335][osd5][DEBUG ] find the location of an executable
[2021-11-03 20:51:39,336][ceph_deploy.osd][INFO  ] Distro info: CentOS Linux 7.9.2009 Core
[2021-11-03 20:51:39,336][osd5][DEBUG ] zeroing last few blocks of device
[2021-11-03 20:51:39,337][osd5][DEBUG ] find the location of an executable
[2021-11-03 20:51:39,339][osd5][INFO  ] Running command: sudo /usr/sbin/ceph-disk zap /dev/sdb
[2021-11-03 20:51:40,509][osd5][DEBUG ] Creating new GPT entries.
[2021-11-03 20:51:40,509][osd5][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[2021-11-03 20:51:40,509][osd5][DEBUG ] other utilities.
[2021-11-03 20:51:41,576][osd5][DEBUG ] Creating new GPT entries.
[2021-11-03 20:51:41,576][osd5][DEBUG ] The operation has completed successfully.
[2021-11-03 20:52:01,585][ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephuser/.cephdeploy.conf
[2021-11-03 20:52:01,586][ceph_deploy.cli][INFO  ] Invoked (1.5.39): /bin/ceph-deploy osd prepare osd1:/dev/sdb osd2:/dev/sdb osd3:/dev/sdb osd4:/dev/sdb osd5:/dev/sdb
[2021-11-03 20:52:01,586][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2021-11-03 20:52:01,586][ceph_deploy.cli][INFO  ]  username                      : None
[2021-11-03 20:52:01,586][ceph_deploy.cli][INFO  ]  block_db                      : None
[2021-11-03 20:52:01,586][ceph_deploy.cli][INFO  ]  disk                          : [('osd1', '/dev/sdb', None), ('osd2', '/dev/sdb', None), ('osd3', '/dev/sdb', None), ('osd4', '/dev/sdb', None), ('osd5', '/dev/sdb', None)]
[2021-11-03 20:52:01,586][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2021-11-03 20:52:01,586][ceph_deploy.cli][INFO  ]  verbose                       : False
[2021-11-03 20:52:01,586][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2021-11-03 20:52:01,586][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2021-11-03 20:52:01,586][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2021-11-03 20:52:01,586][ceph_deploy.cli][INFO  ]  subcommand                    : prepare
[2021-11-03 20:52:01,586][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2021-11-03 20:52:01,586][ceph_deploy.cli][INFO  ]  quiet                         : False
[2021-11-03 20:52:01,586][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fac343bc0e0>
[2021-11-03 20:52:01,586][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2021-11-03 20:52:01,586][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2021-11-03 20:52:01,586][ceph_deploy.cli][INFO  ]  filestore                     : None
[2021-11-03 20:52:01,586][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fac34409050>
[2021-11-03 20:52:01,586][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2021-11-03 20:52:01,586][ceph_deploy.cli][INFO  ]  default_release               : False
[2021-11-03 20:52:01,587][ceph_deploy.cli][INFO  ]  zap_disk                      : False
[2021-11-03 20:52:01,587][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks osd1:/dev/sdb: osd2:/dev/sdb: osd3:/dev/sdb: osd4:/dev/sdb: osd5:/dev/sdb:
[2021-11-03 20:52:01,754][osd1][DEBUG ] connection detected need for sudo
[2021-11-03 20:52:01,923][osd1][DEBUG ] connected to host: osd1 
[2021-11-03 20:52:01,924][osd1][DEBUG ] detect platform information from remote host
[2021-11-03 20:52:01,941][osd1][DEBUG ] detect machine type
[2021-11-03 20:52:01,946][osd1][DEBUG ] find the location of an executable
[2021-11-03 20:52:01,948][ceph_deploy.osd][INFO  ] Distro info: CentOS Linux 7.9.2009 Core
[2021-11-03 20:52:01,948][ceph_deploy.osd][DEBUG ] Deploying osd to osd1
[2021-11-03 20:52:01,948][osd1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2021-11-03 20:52:01,951][osd1][WARNING] osd keyring does not exist yet, creating one
[2021-11-03 20:52:01,951][osd1][DEBUG ] create a keyring file
[2021-11-03 20:52:01,953][ceph_deploy.osd][DEBUG ] Preparing host osd1 disk /dev/sdb journal None activate False
[2021-11-03 20:52:01,953][osd1][DEBUG ] find the location of an executable
[2021-11-03 20:52:01,956][osd1][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdb
[2021-11-03 20:52:02,074][osd1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2021-11-03 20:52:02,074][osd1][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --log-file $run_dir/$cluster-osd-check.log --cluster ceph --setuser ceph --setgroup ceph
[2021-11-03 20:52:02,074][osd1][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --log-file $run_dir/$cluster-osd-check.log --cluster ceph --setuser ceph --setgroup ceph
[2021-11-03 20:52:02,090][osd1][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --log-file $run_dir/$cluster-osd-check.log --cluster ceph --setuser ceph --setgroup ceph
[2021-11-03 20:52:02,122][osd1][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-11-03 20:52:02,122][osd1][WARNING] set_type: Will colocate journal with data on /dev/sdb
[2021-11-03 20:52:02,122][osd1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2021-11-03 20:52:02,125][osd1][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-11-03 20:52:02,126][osd1][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-11-03 20:52:02,126][osd1][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-11-03 20:52:02,126][osd1][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[2021-11-03 20:52:02,141][osd1][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[2021-11-03 20:52:02,145][osd1][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[2021-11-03 20:52:02,152][osd1][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[2021-11-03 20:52:02,168][osd1][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-11-03 20:52:02,168][osd1][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-11-03 20:52:02,168][osd1][WARNING] ptype_tobe_for_name: name = journal
[2021-11-03 20:52:02,169][osd1][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-11-03 20:52:02,169][osd1][WARNING] create_partition: Creating journal partition num 2 size 5120 on /dev/sdb
[2021-11-03 20:52:02,169][osd1][WARNING] command_check_call: Running command: /sbin/sgdisk --new=2:0:+5120M --change-name=2:ceph journal --partition-guid=2:5e7a9f8b-ee97-4b41-9805-ccf92e8a3f18 --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sdb
[2021-11-03 20:52:03,386][osd1][DEBUG ] Setting name!
[2021-11-03 20:52:03,386][osd1][DEBUG ] partNum is 1
[2021-11-03 20:52:03,386][osd1][DEBUG ] REALLY setting name!
[2021-11-03 20:52:03,386][osd1][DEBUG ] The operation has completed successfully.
[2021-11-03 20:52:03,386][osd1][WARNING] update_partition: Calling partprobe on created device /dev/sdb
[2021-11-03 20:52:03,386][osd1][WARNING] command_check_call: Running command: /usr/bin/udevadm settle --timeout=600
[2021-11-03 20:52:03,501][osd1][WARNING] command: Running command: /usr/bin/flock -s /dev/sdb /sbin/partprobe /dev/sdb
[2021-11-03 20:52:03,501][osd1][WARNING] command_check_call: Running command: /usr/bin/udevadm settle --timeout=600
[2021-11-03 20:52:03,615][osd1][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-11-03 20:52:03,615][osd1][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-11-03 20:52:03,615][osd1][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb2 uuid path is /sys/dev/block/8:18/dm/uuid
[2021-11-03 20:52:03,615][osd1][WARNING] prepare_device: Journal is GPT partition /dev/disk/by-partuuid/5e7a9f8b-ee97-4b41-9805-ccf92e8a3f18
[2021-11-03 20:52:03,615][osd1][WARNING] prepare_device: Journal is GPT partition /dev/disk/by-partuuid/5e7a9f8b-ee97-4b41-9805-ccf92e8a3f18
[2021-11-03 20:52:03,615][osd1][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-11-03 20:52:03,615][osd1][WARNING] set_data_partition: Creating osd partition on /dev/sdb
[2021-11-03 20:52:03,615][osd1][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-11-03 20:52:03,616][osd1][WARNING] ptype_tobe_for_name: name = data
[2021-11-03 20:52:03,616][osd1][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-11-03 20:52:03,616][osd1][WARNING] create_partition: Creating data partition num 1 size 0 on /dev/sdb
[2021-11-03 20:52:03,616][osd1][WARNING] command_check_call: Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:c0949dfc-f62a-44ad-be88-0ddc74834596 --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be --mbrtogpt -- /dev/sdb
[2021-11-03 20:52:04,833][osd1][DEBUG ] Setting name!
[2021-11-03 20:52:04,833][osd1][DEBUG ] partNum is 0
[2021-11-03 20:52:04,833][osd1][DEBUG ] REALLY setting name!
[2021-11-03 20:52:04,833][osd1][DEBUG ] The operation has completed successfully.
[2021-11-03 20:52:04,833][osd1][WARNING] update_partition: Calling partprobe on created device /dev/sdb
[2021-11-03 20:52:04,833][osd1][WARNING] command_check_call: Running command: /usr/bin/udevadm settle --timeout=600
[2021-11-03 20:52:04,998][osd1][WARNING] command: Running command: /usr/bin/flock -s /dev/sdb /sbin/partprobe /dev/sdb
[2021-11-03 20:52:05,063][osd1][WARNING] command_check_call: Running command: /usr/bin/udevadm settle --timeout=600
[2021-11-03 20:52:05,177][osd1][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-11-03 20:52:05,177][osd1][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-11-03 20:52:05,177][osd1][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb1 uuid path is /sys/dev/block/8:17/dm/uuid
[2021-11-03 20:52:05,178][osd1][WARNING] populate_data_path_device: Creating xfs fs on /dev/sdb1
[2021-11-03 20:52:05,178][osd1][WARNING] command_check_call: Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/sdb1
[2021-11-03 20:52:06,094][osd1][DEBUG ] Discarding blocks...Done.
[2021-11-03 20:52:06,094][osd1][DEBUG ] meta-data=/dev/sdb1              isize=2048   agcount=4, agsize=196543 blks
[2021-11-03 20:52:06,094][osd1][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=1
[2021-11-03 20:52:06,094][osd1][DEBUG ]          =                       crc=1        finobt=0, sparse=0
[2021-11-03 20:52:06,094][osd1][DEBUG ] data     =                       bsize=4096   blocks=786171, imaxpct=25
[2021-11-03 20:52:06,095][osd1][DEBUG ]          =                       sunit=0      swidth=0 blks
[2021-11-03 20:52:06,095][osd1][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0 ftype=1
[2021-11-03 20:52:06,095][osd1][DEBUG ] log      =internal log           bsize=4096   blocks=2560, version=2
[2021-11-03 20:52:06,095][osd1][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[2021-11-03 20:52:06,095][osd1][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[2021-11-03 20:52:06,095][osd1][WARNING] mount: Mounting /dev/sdb1 on /var/lib/ceph/tmp/mnt.O_zOIO with options noatime,inode64
[2021-11-03 20:52:06,095][osd1][WARNING] command_check_call: Running command: /usr/bin/mount -t xfs -o noatime,inode64 -- /dev/sdb1 /var/lib/ceph/tmp/mnt.O_zOIO
[2021-11-03 20:52:06,095][osd1][WARNING] command: Running command: /sbin/restorecon /var/lib/ceph/tmp/mnt.O_zOIO
[2021-11-03 20:52:06,095][osd1][WARNING] populate_data_path: Preparing osd data dir /var/lib/ceph/tmp/mnt.O_zOIO
[2021-11-03 20:52:06,159][osd1][WARNING] command: Running command: /sbin/restorecon -R /var/lib/ceph/tmp/mnt.O_zOIO/ceph_fsid.9719.tmp
[2021-11-03 20:52:06,159][osd1][WARNING] command: Running command: /usr/bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.O_zOIO/ceph_fsid.9719.tmp
[2021-11-03 20:52:06,166][osd1][WARNING] command: Running command: /sbin/restorecon -R /var/lib/ceph/tmp/mnt.O_zOIO/fsid.9719.tmp
[2021-11-03 20:52:06,168][osd1][WARNING] command: Running command: /usr/bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.O_zOIO/fsid.9719.tmp
[2021-11-03 20:52:06,200][osd1][WARNING] command: Running command: /sbin/restorecon -R /var/lib/ceph/tmp/mnt.O_zOIO/magic.9719.tmp
[2021-11-03 20:52:06,200][osd1][WARNING] command: Running command: /usr/bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.O_zOIO/magic.9719.tmp
[2021-11-03 20:52:06,215][osd1][WARNING] command: Running command: /sbin/restorecon -R /var/lib/ceph/tmp/mnt.O_zOIO/journal_uuid.9719.tmp
[2021-11-03 20:52:06,216][osd1][WARNING] command: Running command: /usr/bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.O_zOIO/journal_uuid.9719.tmp
[2021-11-03 20:52:06,217][osd1][WARNING] adjust_symlink: Creating symlink /var/lib/ceph/tmp/mnt.O_zOIO/journal -> /dev/disk/by-partuuid/5e7a9f8b-ee97-4b41-9805-ccf92e8a3f18
[2021-11-03 20:52:06,217][osd1][WARNING] command: Running command: /sbin/restorecon -R /var/lib/ceph/tmp/mnt.O_zOIO
[2021-11-03 20:52:06,221][osd1][WARNING] command: Running command: /usr/bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.O_zOIO
[2021-11-03 20:52:06,224][osd1][WARNING] unmount: Unmounting /var/lib/ceph/tmp/mnt.O_zOIO
[2021-11-03 20:52:06,224][osd1][WARNING] command_check_call: Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.O_zOIO
[2021-11-03 20:52:06,388][osd1][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-11-03 20:52:06,388][osd1][WARNING] command_check_call: Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/sdb
[2021-11-03 20:52:07,456][osd1][DEBUG ] Warning: The kernel is still using the old partition table.
[2021-11-03 20:52:07,456][osd1][DEBUG ] The new table will be used at the next reboot.
[2021-11-03 20:52:07,456][osd1][DEBUG ] The operation has completed successfully.
[2021-11-03 20:52:07,456][osd1][WARNING] update_partition: Calling partprobe on prepared device /dev/sdb
[2021-11-03 20:52:07,456][osd1][WARNING] command_check_call: Running command: /usr/bin/udevadm settle --timeout=600
[2021-11-03 20:52:07,464][osd1][WARNING] command: Running command: /usr/bin/flock -s /dev/sdb /sbin/partprobe /dev/sdb
[2021-11-03 20:52:07,628][osd1][WARNING] command_check_call: Running command: /usr/bin/udevadm settle --timeout=600
[2021-11-03 20:52:07,993][osd1][WARNING] command_check_call: Running command: /usr/bin/udevadm trigger --action=add --sysname-match sdb1
[2021-11-03 20:52:12,998][osd1][INFO  ] checking OSD status...
[2021-11-03 20:52:12,999][osd1][DEBUG ] find the location of an executable
[2021-11-03 20:52:13,002][osd1][INFO  ] Running command: sudo /bin/ceph --cluster=ceph osd stat --format=json
[2021-11-03 20:52:13,117][ceph_deploy.osd][DEBUG ] Host osd1 is now ready for osd use.
[2021-11-03 20:52:13,282][osd2][DEBUG ] connection detected need for sudo
[2021-11-03 20:52:13,467][osd2][DEBUG ] connected to host: osd2 
[2021-11-03 20:52:13,468][osd2][DEBUG ] detect platform information from remote host
[2021-11-03 20:52:13,487][osd2][DEBUG ] detect machine type
[2021-11-03 20:52:13,493][osd2][DEBUG ] find the location of an executable
[2021-11-03 20:52:13,494][ceph_deploy.osd][INFO  ] Distro info: CentOS Linux 7.9.2009 Core
[2021-11-03 20:52:13,494][ceph_deploy.osd][DEBUG ] Deploying osd to osd2
[2021-11-03 20:52:13,494][osd2][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2021-11-03 20:52:13,497][osd2][WARNING] osd keyring does not exist yet, creating one
[2021-11-03 20:52:13,497][osd2][DEBUG ] create a keyring file
[2021-11-03 20:52:13,499][ceph_deploy.osd][DEBUG ] Preparing host osd2 disk /dev/sdb journal None activate False
[2021-11-03 20:52:13,499][osd2][DEBUG ] find the location of an executable
[2021-11-03 20:52:13,502][osd2][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdb
[2021-11-03 20:52:13,619][osd2][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2021-11-03 20:52:13,619][osd2][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --log-file $run_dir/$cluster-osd-check.log --cluster ceph --setuser ceph --setgroup ceph
[2021-11-03 20:52:13,635][osd2][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --log-file $run_dir/$cluster-osd-check.log --cluster ceph --setuser ceph --setgroup ceph
[2021-11-03 20:52:13,651][osd2][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --log-file $run_dir/$cluster-osd-check.log --cluster ceph --setuser ceph --setgroup ceph
[2021-11-03 20:52:13,666][osd2][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-11-03 20:52:13,667][osd2][WARNING] set_type: Will colocate journal with data on /dev/sdb
[2021-11-03 20:52:13,667][osd2][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2021-11-03 20:52:13,682][osd2][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-11-03 20:52:13,682][osd2][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-11-03 20:52:13,683][osd2][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-11-03 20:52:13,683][osd2][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[2021-11-03 20:52:13,690][osd2][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[2021-11-03 20:52:13,706][osd2][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[2021-11-03 20:52:13,707][osd2][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[2021-11-03 20:52:13,723][osd2][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-11-03 20:52:13,723][osd2][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-11-03 20:52:13,724][osd2][WARNING] ptype_tobe_for_name: name = journal
[2021-11-03 20:52:13,724][osd2][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-11-03 20:52:13,724][osd2][WARNING] create_partition: Creating journal partition num 2 size 5120 on /dev/sdb
[2021-11-03 20:52:13,724][osd2][WARNING] command_check_call: Running command: /sbin/sgdisk --new=2:0:+5120M --change-name=2:ceph journal --partition-guid=2:08f6be7f-7726-4a71-a67b-79386659ab6d --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sdb
[2021-11-03 20:52:14,841][osd2][DEBUG ] Setting name!
[2021-11-03 20:52:14,841][osd2][DEBUG ] partNum is 1
[2021-11-03 20:52:14,841][osd2][DEBUG ] REALLY setting name!
[2021-11-03 20:52:14,841][osd2][DEBUG ] The operation has completed successfully.
[2021-11-03 20:52:14,842][osd2][WARNING] update_partition: Calling partprobe on created device /dev/sdb
[2021-11-03 20:52:14,842][osd2][WARNING] command_check_call: Running command: /usr/bin/udevadm settle --timeout=600
[2021-11-03 20:52:14,956][osd2][WARNING] command: Running command: /usr/bin/flock -s /dev/sdb /sbin/partprobe /dev/sdb
[2021-11-03 20:52:15,120][osd2][WARNING] command_check_call: Running command: /usr/bin/udevadm settle --timeout=600
[2021-11-03 20:52:15,234][osd2][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-11-03 20:52:15,235][osd2][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-11-03 20:52:15,235][osd2][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb2 uuid path is /sys/dev/block/8:18/dm/uuid
[2021-11-03 20:52:15,235][osd2][WARNING] prepare_device: Journal is GPT partition /dev/disk/by-partuuid/08f6be7f-7726-4a71-a67b-79386659ab6d
[2021-11-03 20:52:15,235][osd2][WARNING] prepare_device: Journal is GPT partition /dev/disk/by-partuuid/08f6be7f-7726-4a71-a67b-79386659ab6d
[2021-11-03 20:52:15,235][osd2][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-11-03 20:52:15,235][osd2][WARNING] set_data_partition: Creating osd partition on /dev/sdb
[2021-11-03 20:52:15,235][osd2][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-11-03 20:52:15,235][osd2][WARNING] ptype_tobe_for_name: name = data
[2021-11-03 20:52:15,235][osd2][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-11-03 20:52:15,235][osd2][WARNING] create_partition: Creating data partition num 1 size 0 on /dev/sdb
[2021-11-03 20:52:15,235][osd2][WARNING] command_check_call: Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:dba47fdc-702c-4021-9bd7-c4f19f39ab75 --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be --mbrtogpt -- /dev/sdb
[2021-11-03 20:52:16,403][osd2][DEBUG ] Setting name!
[2021-11-03 20:52:16,403][osd2][DEBUG ] partNum is 0
[2021-11-03 20:52:16,403][osd2][DEBUG ] REALLY setting name!
[2021-11-03 20:52:16,403][osd2][DEBUG ] The operation has completed successfully.
[2021-11-03 20:52:16,403][osd2][WARNING] update_partition: Calling partprobe on created device /dev/sdb
[2021-11-03 20:52:16,403][osd2][WARNING] command_check_call: Running command: /usr/bin/udevadm settle --timeout=600
[2021-11-03 20:52:16,517][osd2][WARNING] command: Running command: /usr/bin/flock -s /dev/sdb /sbin/partprobe /dev/sdb
[2021-11-03 20:52:16,581][osd2][WARNING] command_check_call: Running command: /usr/bin/udevadm settle --timeout=600
[2021-11-03 20:52:16,745][osd2][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-11-03 20:52:16,746][osd2][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-11-03 20:52:16,746][osd2][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb1 uuid path is /sys/dev/block/8:17/dm/uuid
[2021-11-03 20:52:16,746][osd2][WARNING] populate_data_path_device: Creating xfs fs on /dev/sdb1
[2021-11-03 20:52:16,746][osd2][WARNING] command_check_call: Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/sdb1
[2021-11-03 20:52:17,513][osd2][DEBUG ] Discarding blocks...Done.
[2021-11-03 20:52:17,513][osd2][DEBUG ] meta-data=/dev/sdb1              isize=2048   agcount=4, agsize=196543 blks
[2021-11-03 20:52:17,513][osd2][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=1
[2021-11-03 20:52:17,513][osd2][DEBUG ]          =                       crc=1        finobt=0, sparse=0
[2021-11-03 20:52:17,513][osd2][DEBUG ] data     =                       bsize=4096   blocks=786171, imaxpct=25
[2021-11-03 20:52:17,513][osd2][DEBUG ]          =                       sunit=0      swidth=0 blks
[2021-11-03 20:52:17,513][osd2][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0 ftype=1
[2021-11-03 20:52:17,513][osd2][DEBUG ] log      =internal log           bsize=4096   blocks=2560, version=2
[2021-11-03 20:52:17,513][osd2][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[2021-11-03 20:52:17,513][osd2][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[2021-11-03 20:52:17,513][osd2][WARNING] mount: Mounting /dev/sdb1 on /var/lib/ceph/tmp/mnt.f6JZjD with options noatime,inode64
[2021-11-03 20:52:17,514][osd2][WARNING] command_check_call: Running command: /usr/bin/mount -t xfs -o noatime,inode64 -- /dev/sdb1 /var/lib/ceph/tmp/mnt.f6JZjD
[2021-11-03 20:52:17,514][osd2][WARNING] command: Running command: /sbin/restorecon /var/lib/ceph/tmp/mnt.f6JZjD
[2021-11-03 20:52:17,514][osd2][WARNING] populate_data_path: Preparing osd data dir /var/lib/ceph/tmp/mnt.f6JZjD
[2021-11-03 20:52:17,628][osd2][WARNING] command: Running command: /sbin/restorecon -R /var/lib/ceph/tmp/mnt.f6JZjD/ceph_fsid.9719.tmp
[2021-11-03 20:52:17,628][osd2][WARNING] command: Running command: /usr/bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.f6JZjD/ceph_fsid.9719.tmp
[2021-11-03 20:52:17,628][osd2][WARNING] command: Running command: /sbin/restorecon -R /var/lib/ceph/tmp/mnt.f6JZjD/fsid.9719.tmp
[2021-11-03 20:52:17,628][osd2][WARNING] command: Running command: /usr/bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.f6JZjD/fsid.9719.tmp
[2021-11-03 20:52:17,660][osd2][WARNING] command: Running command: /sbin/restorecon -R /var/lib/ceph/tmp/mnt.f6JZjD/magic.9719.tmp
[2021-11-03 20:52:17,660][osd2][WARNING] command: Running command: /usr/bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.f6JZjD/magic.9719.tmp
[2021-11-03 20:52:17,724][osd2][WARNING] command: Running command: /sbin/restorecon -R /var/lib/ceph/tmp/mnt.f6JZjD/journal_uuid.9719.tmp
[2021-11-03 20:52:17,724][osd2][WARNING] command: Running command: /usr/bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.f6JZjD/journal_uuid.9719.tmp
[2021-11-03 20:52:17,724][osd2][WARNING] adjust_symlink: Creating symlink /var/lib/ceph/tmp/mnt.f6JZjD/journal -> /dev/disk/by-partuuid/08f6be7f-7726-4a71-a67b-79386659ab6d
[2021-11-03 20:52:17,724][osd2][WARNING] command: Running command: /sbin/restorecon -R /var/lib/ceph/tmp/mnt.f6JZjD
[2021-11-03 20:52:17,724][osd2][WARNING] command: Running command: /usr/bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.f6JZjD
[2021-11-03 20:52:17,724][osd2][WARNING] unmount: Unmounting /var/lib/ceph/tmp/mnt.f6JZjD
[2021-11-03 20:52:17,724][osd2][WARNING] command_check_call: Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.f6JZjD
[2021-11-03 20:52:17,939][osd2][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-11-03 20:52:17,939][osd2][WARNING] command_check_call: Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/sdb
[2021-11-03 20:52:19,107][osd2][DEBUG ] Warning: The kernel is still using the old partition table.
[2021-11-03 20:52:19,108][osd2][DEBUG ] The new table will be used at the next reboot.
[2021-11-03 20:52:19,108][osd2][DEBUG ] The operation has completed successfully.
[2021-11-03 20:52:19,108][osd2][WARNING] update_partition: Calling partprobe on prepared device /dev/sdb
[2021-11-03 20:52:19,108][osd2][WARNING] command_check_call: Running command: /usr/bin/udevadm settle --timeout=600
[2021-11-03 20:52:19,108][osd2][WARNING] command: Running command: /usr/bin/flock -s /dev/sdb /sbin/partprobe /dev/sdb
[2021-11-03 20:52:19,224][osd2][WARNING] command_check_call: Running command: /usr/bin/udevadm settle --timeout=600
[2021-11-03 20:52:20,096][osd2][WARNING] command_check_call: Running command: /usr/bin/udevadm trigger --action=add --sysname-match sdb1
[2021-11-03 20:52:25,101][osd2][INFO  ] checking OSD status...
[2021-11-03 20:52:25,101][osd2][DEBUG ] find the location of an executable
[2021-11-03 20:52:25,104][osd2][INFO  ] Running command: sudo /bin/ceph --cluster=ceph osd stat --format=json
[2021-11-03 20:52:25,219][ceph_deploy.osd][DEBUG ] Host osd2 is now ready for osd use.
[2021-11-03 20:52:25,373][osd3][DEBUG ] connection detected need for sudo
[2021-11-03 20:52:25,540][osd3][DEBUG ] connected to host: osd3 
[2021-11-03 20:52:25,541][osd3][DEBUG ] detect platform information from remote host
[2021-11-03 20:52:25,557][osd3][DEBUG ] detect machine type
[2021-11-03 20:52:25,562][osd3][DEBUG ] find the location of an executable
[2021-11-03 20:52:25,563][ceph_deploy.osd][INFO  ] Distro info: CentOS Linux 7.9.2009 Core
[2021-11-03 20:52:25,563][ceph_deploy.osd][DEBUG ] Deploying osd to osd3
[2021-11-03 20:52:25,563][osd3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2021-11-03 20:52:25,565][osd3][WARNING] osd keyring does not exist yet, creating one
[2021-11-03 20:52:25,566][osd3][DEBUG ] create a keyring file
[2021-11-03 20:52:25,567][ceph_deploy.osd][DEBUG ] Preparing host osd3 disk /dev/sdb journal None activate False
[2021-11-03 20:52:25,567][osd3][DEBUG ] find the location of an executable
[2021-11-03 20:52:25,570][osd3][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdb
[2021-11-03 20:52:25,636][osd3][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2021-11-03 20:52:25,668][osd3][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --log-file $run_dir/$cluster-osd-check.log --cluster ceph --setuser ceph --setgroup ceph
[2021-11-03 20:52:25,675][osd3][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --log-file $run_dir/$cluster-osd-check.log --cluster ceph --setuser ceph --setgroup ceph
[2021-11-03 20:52:25,691][osd3][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --log-file $run_dir/$cluster-osd-check.log --cluster ceph --setuser ceph --setgroup ceph
[2021-11-03 20:52:25,707][osd3][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-11-03 20:52:25,707][osd3][WARNING] set_type: Will colocate journal with data on /dev/sdb
[2021-11-03 20:52:25,707][osd3][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2021-11-03 20:52:25,722][osd3][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-11-03 20:52:25,723][osd3][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-11-03 20:52:25,723][osd3][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-11-03 20:52:25,723][osd3][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[2021-11-03 20:52:25,730][osd3][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[2021-11-03 20:52:25,738][osd3][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[2021-11-03 20:52:25,745][osd3][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[2021-11-03 20:52:25,753][osd3][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-11-03 20:52:25,753][osd3][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-11-03 20:52:25,753][osd3][WARNING] ptype_tobe_for_name: name = journal
[2021-11-03 20:52:25,753][osd3][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-11-03 20:52:25,753][osd3][WARNING] create_partition: Creating journal partition num 2 size 5120 on /dev/sdb
[2021-11-03 20:52:25,753][osd3][WARNING] command_check_call: Running command: /sbin/sgdisk --new=2:0:+5120M --change-name=2:ceph journal --partition-guid=2:c31d3c58-0139-477f-96d8-2ab4139674cd --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sdb
[2021-11-03 20:52:26,870][osd3][DEBUG ] Setting name!
[2021-11-03 20:52:26,870][osd3][DEBUG ] partNum is 1
[2021-11-03 20:52:26,870][osd3][DEBUG ] REALLY setting name!
[2021-11-03 20:52:26,870][osd3][DEBUG ] The operation has completed successfully.
[2021-11-03 20:52:26,870][osd3][WARNING] update_partition: Calling partprobe on created device /dev/sdb
[2021-11-03 20:52:26,870][osd3][WARNING] command_check_call: Running command: /usr/bin/udevadm settle --timeout=600
[2021-11-03 20:52:26,984][osd3][WARNING] command: Running command: /usr/bin/flock -s /dev/sdb /sbin/partprobe /dev/sdb
[2021-11-03 20:52:27,000][osd3][WARNING] command_check_call: Running command: /usr/bin/udevadm settle --timeout=600
[2021-11-03 20:52:27,064][osd3][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-11-03 20:52:27,064][osd3][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-11-03 20:52:27,064][osd3][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb2 uuid path is /sys/dev/block/8:18/dm/uuid
[2021-11-03 20:52:27,064][osd3][WARNING] prepare_device: Journal is GPT partition /dev/disk/by-partuuid/c31d3c58-0139-477f-96d8-2ab4139674cd
[2021-11-03 20:52:27,064][osd3][WARNING] prepare_device: Journal is GPT partition /dev/disk/by-partuuid/c31d3c58-0139-477f-96d8-2ab4139674cd
[2021-11-03 20:52:27,064][osd3][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-11-03 20:52:27,065][osd3][WARNING] set_data_partition: Creating osd partition on /dev/sdb
[2021-11-03 20:52:27,065][osd3][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-11-03 20:52:27,065][osd3][WARNING] ptype_tobe_for_name: name = data
[2021-11-03 20:52:27,065][osd3][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-11-03 20:52:27,065][osd3][WARNING] create_partition: Creating data partition num 1 size 0 on /dev/sdb
[2021-11-03 20:52:27,065][osd3][WARNING] command_check_call: Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:2f33abcf-3c86-45ad-8729-caa774a24dd7 --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be --mbrtogpt -- /dev/sdb
[2021-11-03 20:52:28,182][osd3][DEBUG ] Setting name!
[2021-11-03 20:52:28,182][osd3][DEBUG ] partNum is 0
[2021-11-03 20:52:28,182][osd3][DEBUG ] REALLY setting name!
[2021-11-03 20:52:28,182][osd3][DEBUG ] The operation has completed successfully.
[2021-11-03 20:52:28,182][osd3][WARNING] update_partition: Calling partprobe on created device /dev/sdb
[2021-11-03 20:52:28,182][osd3][WARNING] command_check_call: Running command: /usr/bin/udevadm settle --timeout=600
[2021-11-03 20:52:28,296][osd3][WARNING] command: Running command: /usr/bin/flock -s /dev/sdb /sbin/partprobe /dev/sdb
[2021-11-03 20:52:28,360][osd3][WARNING] command_check_call: Running command: /usr/bin/udevadm settle --timeout=600
[2021-11-03 20:52:28,474][osd3][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-11-03 20:52:28,474][osd3][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-11-03 20:52:28,474][osd3][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb1 uuid path is /sys/dev/block/8:17/dm/uuid
[2021-11-03 20:52:28,474][osd3][WARNING] populate_data_path_device: Creating xfs fs on /dev/sdb1
[2021-11-03 20:52:28,475][osd3][WARNING] command_check_call: Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/sdb1
[2021-11-03 20:52:29,140][osd3][DEBUG ] Discarding blocks...Done.
[2021-11-03 20:52:29,140][osd3][DEBUG ] meta-data=/dev/sdb1              isize=2048   agcount=4, agsize=196543 blks
[2021-11-03 20:52:29,140][osd3][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=1
[2021-11-03 20:52:29,141][osd3][DEBUG ]          =                       crc=1        finobt=0, sparse=0
[2021-11-03 20:52:29,141][osd3][DEBUG ] data     =                       bsize=4096   blocks=786171, imaxpct=25
[2021-11-03 20:52:29,141][osd3][DEBUG ]          =                       sunit=0      swidth=0 blks
[2021-11-03 20:52:29,141][osd3][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0 ftype=1
[2021-11-03 20:52:29,141][osd3][DEBUG ] log      =internal log           bsize=4096   blocks=2560, version=2
[2021-11-03 20:52:29,141][osd3][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[2021-11-03 20:52:29,141][osd3][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[2021-11-03 20:52:29,141][osd3][WARNING] mount: Mounting /dev/sdb1 on /var/lib/ceph/tmp/mnt.dxRwvd with options noatime,inode64
[2021-11-03 20:52:29,141][osd3][WARNING] command_check_call: Running command: /usr/bin/mount -t xfs -o noatime,inode64 -- /dev/sdb1 /var/lib/ceph/tmp/mnt.dxRwvd
[2021-11-03 20:52:29,157][osd3][WARNING] command: Running command: /sbin/restorecon /var/lib/ceph/tmp/mnt.dxRwvd
[2021-11-03 20:52:29,157][osd3][WARNING] populate_data_path: Preparing osd data dir /var/lib/ceph/tmp/mnt.dxRwvd
[2021-11-03 20:52:29,271][osd3][WARNING] command: Running command: /sbin/restorecon -R /var/lib/ceph/tmp/mnt.dxRwvd/ceph_fsid.9455.tmp
[2021-11-03 20:52:29,271][osd3][WARNING] command: Running command: /usr/bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.dxRwvd/ceph_fsid.9455.tmp
[2021-11-03 20:52:29,271][osd3][WARNING] command: Running command: /sbin/restorecon -R /var/lib/ceph/tmp/mnt.dxRwvd/fsid.9455.tmp
[2021-11-03 20:52:29,271][osd3][WARNING] command: Running command: /usr/bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.dxRwvd/fsid.9455.tmp
[2021-11-03 20:52:29,279][osd3][WARNING] command: Running command: /sbin/restorecon -R /var/lib/ceph/tmp/mnt.dxRwvd/magic.9455.tmp
[2021-11-03 20:52:29,279][osd3][WARNING] command: Running command: /usr/bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.dxRwvd/magic.9455.tmp
[2021-11-03 20:52:29,310][osd3][WARNING] command: Running command: /sbin/restorecon -R /var/lib/ceph/tmp/mnt.dxRwvd/journal_uuid.9455.tmp
[2021-11-03 20:52:29,311][osd3][WARNING] command: Running command: /usr/bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.dxRwvd/journal_uuid.9455.tmp
[2021-11-03 20:52:29,311][osd3][WARNING] adjust_symlink: Creating symlink /var/lib/ceph/tmp/mnt.dxRwvd/journal -> /dev/disk/by-partuuid/c31d3c58-0139-477f-96d8-2ab4139674cd
[2021-11-03 20:52:29,311][osd3][WARNING] command: Running command: /sbin/restorecon -R /var/lib/ceph/tmp/mnt.dxRwvd
[2021-11-03 20:52:29,311][osd3][WARNING] command: Running command: /usr/bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.dxRwvd
[2021-11-03 20:52:29,311][osd3][WARNING] unmount: Unmounting /var/lib/ceph/tmp/mnt.dxRwvd
[2021-11-03 20:52:29,311][osd3][WARNING] command_check_call: Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.dxRwvd
[2021-11-03 20:52:29,425][osd3][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-11-03 20:52:29,425][osd3][WARNING] command_check_call: Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/sdb
[2021-11-03 20:52:30,492][osd3][DEBUG ] Warning: The kernel is still using the old partition table.
[2021-11-03 20:52:30,492][osd3][DEBUG ] The new table will be used at the next reboot.
[2021-11-03 20:52:30,492][osd3][DEBUG ] The operation has completed successfully.
[2021-11-03 20:52:30,492][osd3][WARNING] update_partition: Calling partprobe on prepared device /dev/sdb
[2021-11-03 20:52:30,493][osd3][WARNING] command_check_call: Running command: /usr/bin/udevadm settle --timeout=600
[2021-11-03 20:52:30,494][osd3][WARNING] command: Running command: /usr/bin/flock -s /dev/sdb /sbin/partprobe /dev/sdb
[2021-11-03 20:52:30,608][osd3][WARNING] command_check_call: Running command: /usr/bin/udevadm settle --timeout=600
[2021-11-03 20:52:30,640][osd3][WARNING] command_check_call: Running command: /usr/bin/udevadm trigger --action=add --sysname-match sdb1
[2021-11-03 20:52:35,652][osd3][INFO  ] checking OSD status...
[2021-11-03 20:52:35,653][osd3][DEBUG ] find the location of an executable
[2021-11-03 20:52:35,655][osd3][INFO  ] Running command: sudo /bin/ceph --cluster=ceph osd stat --format=json
[2021-11-03 20:52:35,770][ceph_deploy.osd][DEBUG ] Host osd3 is now ready for osd use.
[2021-11-03 20:52:35,924][osd4][DEBUG ] connection detected need for sudo
[2021-11-03 20:52:36,090][osd4][DEBUG ] connected to host: osd4 
[2021-11-03 20:52:36,091][osd4][DEBUG ] detect platform information from remote host
[2021-11-03 20:52:36,107][osd4][DEBUG ] detect machine type
[2021-11-03 20:52:36,112][osd4][DEBUG ] find the location of an executable
[2021-11-03 20:52:36,113][ceph_deploy.osd][INFO  ] Distro info: CentOS Linux 7.9.2009 Core
[2021-11-03 20:52:36,113][ceph_deploy.osd][DEBUG ] Deploying osd to osd4
[2021-11-03 20:52:36,113][osd4][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2021-11-03 20:52:36,115][osd4][WARNING] osd keyring does not exist yet, creating one
[2021-11-03 20:52:36,115][osd4][DEBUG ] create a keyring file
[2021-11-03 20:52:36,117][ceph_deploy.osd][DEBUG ] Preparing host osd4 disk /dev/sdb journal None activate False
[2021-11-03 20:52:36,117][osd4][DEBUG ] find the location of an executable
[2021-11-03 20:52:36,120][osd4][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdb
[2021-11-03 20:52:36,186][osd4][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2021-11-03 20:52:36,202][osd4][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --log-file $run_dir/$cluster-osd-check.log --cluster ceph --setuser ceph --setgroup ceph
[2021-11-03 20:52:36,233][osd4][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --log-file $run_dir/$cluster-osd-check.log --cluster ceph --setuser ceph --setgroup ceph
[2021-11-03 20:52:36,235][osd4][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --log-file $run_dir/$cluster-osd-check.log --cluster ceph --setuser ceph --setgroup ceph
[2021-11-03 20:52:36,266][osd4][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-11-03 20:52:36,267][osd4][WARNING] set_type: Will colocate journal with data on /dev/sdb
[2021-11-03 20:52:36,267][osd4][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2021-11-03 20:52:36,267][osd4][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-11-03 20:52:36,267][osd4][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-11-03 20:52:36,267][osd4][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-11-03 20:52:36,267][osd4][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[2021-11-03 20:52:36,274][osd4][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[2021-11-03 20:52:36,290][osd4][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[2021-11-03 20:52:36,291][osd4][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[2021-11-03 20:52:36,307][osd4][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-11-03 20:52:36,307][osd4][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-11-03 20:52:36,307][osd4][WARNING] ptype_tobe_for_name: name = journal
[2021-11-03 20:52:36,307][osd4][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-11-03 20:52:36,307][osd4][WARNING] create_partition: Creating journal partition num 2 size 5120 on /dev/sdb
[2021-11-03 20:52:36,307][osd4][WARNING] command_check_call: Running command: /sbin/sgdisk --new=2:0:+5120M --change-name=2:ceph journal --partition-guid=2:de062dce-09b6-444b-8c4d-443932bb6831 --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sdb
[2021-11-03 20:52:37,424][osd4][DEBUG ] Setting name!
[2021-11-03 20:52:37,425][osd4][DEBUG ] partNum is 1
[2021-11-03 20:52:37,425][osd4][DEBUG ] REALLY setting name!
[2021-11-03 20:52:37,425][osd4][DEBUG ] The operation has completed successfully.
[2021-11-03 20:52:37,425][osd4][WARNING] update_partition: Calling partprobe on created device /dev/sdb
[2021-11-03 20:52:37,425][osd4][WARNING] command_check_call: Running command: /usr/bin/udevadm settle --timeout=600
[2021-11-03 20:52:37,539][osd4][WARNING] command: Running command: /usr/bin/flock -s /dev/sdb /sbin/partprobe /dev/sdb
[2021-11-03 20:52:37,603][osd4][WARNING] command_check_call: Running command: /usr/bin/udevadm settle --timeout=600
[2021-11-03 20:52:37,717][osd4][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-11-03 20:52:37,717][osd4][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-11-03 20:52:37,718][osd4][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb2 uuid path is /sys/dev/block/8:18/dm/uuid
[2021-11-03 20:52:37,718][osd4][WARNING] prepare_device: Journal is GPT partition /dev/disk/by-partuuid/de062dce-09b6-444b-8c4d-443932bb6831
[2021-11-03 20:52:37,718][osd4][WARNING] prepare_device: Journal is GPT partition /dev/disk/by-partuuid/de062dce-09b6-444b-8c4d-443932bb6831
[2021-11-03 20:52:37,718][osd4][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-11-03 20:52:37,718][osd4][WARNING] set_data_partition: Creating osd partition on /dev/sdb
[2021-11-03 20:52:37,718][osd4][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-11-03 20:52:37,718][osd4][WARNING] ptype_tobe_for_name: name = data
[2021-11-03 20:52:37,718][osd4][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-11-03 20:52:37,718][osd4][WARNING] create_partition: Creating data partition num 1 size 0 on /dev/sdb
[2021-11-03 20:52:37,718][osd4][WARNING] command_check_call: Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:aa17af76-99b7-4682-90ef-bd512efa2dc0 --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be --mbrtogpt -- /dev/sdb
[2021-11-03 20:52:38,785][osd4][DEBUG ] Setting name!
[2021-11-03 20:52:38,785][osd4][DEBUG ] partNum is 0
[2021-11-03 20:52:38,785][osd4][DEBUG ] REALLY setting name!
[2021-11-03 20:52:38,785][osd4][DEBUG ] The operation has completed successfully.
[2021-11-03 20:52:38,785][osd4][WARNING] update_partition: Calling partprobe on created device /dev/sdb
[2021-11-03 20:52:38,785][osd4][WARNING] command_check_call: Running command: /usr/bin/udevadm settle --timeout=600
[2021-11-03 20:52:38,899][osd4][WARNING] command: Running command: /usr/bin/flock -s /dev/sdb /sbin/partprobe /dev/sdb
[2021-11-03 20:52:38,963][osd4][WARNING] command_check_call: Running command: /usr/bin/udevadm settle --timeout=600
[2021-11-03 20:52:39,077][osd4][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-11-03 20:52:39,077][osd4][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-11-03 20:52:39,078][osd4][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb1 uuid path is /sys/dev/block/8:17/dm/uuid
[2021-11-03 20:52:39,078][osd4][WARNING] populate_data_path_device: Creating xfs fs on /dev/sdb1
[2021-11-03 20:52:39,078][osd4][WARNING] command_check_call: Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/sdb1
[2021-11-03 20:52:40,345][osd4][DEBUG ] Discarding blocks...Done.
[2021-11-03 20:52:40,345][osd4][DEBUG ] meta-data=/dev/sdb1              isize=2048   agcount=4, agsize=196543 blks
[2021-11-03 20:52:40,346][osd4][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=1
[2021-11-03 20:52:40,346][osd4][DEBUG ]          =                       crc=1        finobt=0, sparse=0
[2021-11-03 20:52:40,346][osd4][DEBUG ] data     =                       bsize=4096   blocks=786171, imaxpct=25
[2021-11-03 20:52:40,346][osd4][DEBUG ]          =                       sunit=0      swidth=0 blks
[2021-11-03 20:52:40,346][osd4][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0 ftype=1
[2021-11-03 20:52:40,346][osd4][DEBUG ] log      =internal log           bsize=4096   blocks=2560, version=2
[2021-11-03 20:52:40,346][osd4][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[2021-11-03 20:52:40,346][osd4][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[2021-11-03 20:52:40,346][osd4][WARNING] mount: Mounting /dev/sdb1 on /var/lib/ceph/tmp/mnt._igds0 with options noatime,inode64
[2021-11-03 20:52:40,347][osd4][WARNING] command_check_call: Running command: /usr/bin/mount -t xfs -o noatime,inode64 -- /dev/sdb1 /var/lib/ceph/tmp/mnt._igds0
[2021-11-03 20:52:40,355][osd4][WARNING] command: Running command: /sbin/restorecon /var/lib/ceph/tmp/mnt._igds0
[2021-11-03 20:52:40,358][osd4][WARNING] populate_data_path: Preparing osd data dir /var/lib/ceph/tmp/mnt._igds0
[2021-11-03 20:52:40,473][osd4][WARNING] command: Running command: /sbin/restorecon -R /var/lib/ceph/tmp/mnt._igds0/ceph_fsid.9453.tmp
[2021-11-03 20:52:40,473][osd4][WARNING] command: Running command: /usr/bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt._igds0/ceph_fsid.9453.tmp
[2021-11-03 20:52:40,476][osd4][WARNING] command: Running command: /sbin/restorecon -R /var/lib/ceph/tmp/mnt._igds0/fsid.9453.tmp
[2021-11-03 20:52:40,480][osd4][WARNING] command: Running command: /usr/bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt._igds0/fsid.9453.tmp
[2021-11-03 20:52:40,512][osd4][WARNING] command: Running command: /sbin/restorecon -R /var/lib/ceph/tmp/mnt._igds0/magic.9453.tmp
[2021-11-03 20:52:40,512][osd4][WARNING] command: Running command: /usr/bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt._igds0/magic.9453.tmp
[2021-11-03 20:52:40,578][osd4][WARNING] command: Running command: /sbin/restorecon -R /var/lib/ceph/tmp/mnt._igds0/journal_uuid.9453.tmp
[2021-11-03 20:52:40,578][osd4][WARNING] command: Running command: /usr/bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt._igds0/journal_uuid.9453.tmp
[2021-11-03 20:52:40,579][osd4][WARNING] adjust_symlink: Creating symlink /var/lib/ceph/tmp/mnt._igds0/journal -> /dev/disk/by-partuuid/de062dce-09b6-444b-8c4d-443932bb6831
[2021-11-03 20:52:40,579][osd4][WARNING] command: Running command: /sbin/restorecon -R /var/lib/ceph/tmp/mnt._igds0
[2021-11-03 20:52:40,579][osd4][WARNING] command: Running command: /usr/bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt._igds0
[2021-11-03 20:52:40,579][osd4][WARNING] unmount: Unmounting /var/lib/ceph/tmp/mnt._igds0
[2021-11-03 20:52:40,579][osd4][WARNING] command_check_call: Running command: /bin/umount -- /var/lib/ceph/tmp/mnt._igds0
[2021-11-03 20:52:40,693][osd4][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-11-03 20:52:40,693][osd4][WARNING] command_check_call: Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/sdb
[2021-11-03 20:52:41,810][osd4][DEBUG ] Warning: The kernel is still using the old partition table.
[2021-11-03 20:52:41,811][osd4][DEBUG ] The new table will be used at the next reboot.
[2021-11-03 20:52:41,811][osd4][DEBUG ] The operation has completed successfully.
[2021-11-03 20:52:41,811][osd4][WARNING] update_partition: Calling partprobe on prepared device /dev/sdb
[2021-11-03 20:52:41,811][osd4][WARNING] command_check_call: Running command: /usr/bin/udevadm settle --timeout=600
[2021-11-03 20:52:41,811][osd4][WARNING] command: Running command: /usr/bin/flock -s /dev/sdb /sbin/partprobe /dev/sdb
[2021-11-03 20:52:41,975][osd4][WARNING] command_check_call: Running command: /usr/bin/udevadm settle --timeout=600
[2021-11-03 20:52:41,975][osd4][WARNING] command_check_call: Running command: /usr/bin/udevadm trigger --action=add --sysname-match sdb1
[2021-11-03 20:52:46,986][osd4][INFO  ] checking OSD status...
[2021-11-03 20:52:46,986][osd4][DEBUG ] find the location of an executable
[2021-11-03 20:52:46,989][osd4][INFO  ] Running command: sudo /bin/ceph --cluster=ceph osd stat --format=json
[2021-11-03 20:52:47,107][ceph_deploy.osd][DEBUG ] Host osd4 is now ready for osd use.
[2021-11-03 20:52:47,260][osd5][DEBUG ] connection detected need for sudo
[2021-11-03 20:52:47,427][osd5][DEBUG ] connected to host: osd5 
[2021-11-03 20:52:47,427][osd5][DEBUG ] detect platform information from remote host
[2021-11-03 20:52:47,444][osd5][DEBUG ] detect machine type
[2021-11-03 20:52:47,449][osd5][DEBUG ] find the location of an executable
[2021-11-03 20:52:47,450][ceph_deploy.osd][INFO  ] Distro info: CentOS Linux 7.9.2009 Core
[2021-11-03 20:52:47,450][ceph_deploy.osd][DEBUG ] Deploying osd to osd5
[2021-11-03 20:52:47,450][osd5][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2021-11-03 20:52:47,452][osd5][WARNING] osd keyring does not exist yet, creating one
[2021-11-03 20:52:47,452][osd5][DEBUG ] create a keyring file
[2021-11-03 20:52:47,454][ceph_deploy.osd][DEBUG ] Preparing host osd5 disk /dev/sdb journal None activate False
[2021-11-03 20:52:47,454][osd5][DEBUG ] find the location of an executable
[2021-11-03 20:52:47,457][osd5][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdb
[2021-11-03 20:52:47,523][osd5][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2021-11-03 20:52:47,539][osd5][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --log-file $run_dir/$cluster-osd-check.log --cluster ceph --setuser ceph --setgroup ceph
[2021-11-03 20:52:47,554][osd5][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --log-file $run_dir/$cluster-osd-check.log --cluster ceph --setuser ceph --setgroup ceph
[2021-11-03 20:52:47,570][osd5][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --log-file $run_dir/$cluster-osd-check.log --cluster ceph --setuser ceph --setgroup ceph
[2021-11-03 20:52:47,586][osd5][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-11-03 20:52:47,586][osd5][WARNING] set_type: Will colocate journal with data on /dev/sdb
[2021-11-03 20:52:47,586][osd5][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2021-11-03 20:52:47,601][osd5][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-11-03 20:52:47,602][osd5][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-11-03 20:52:47,602][osd5][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-11-03 20:52:47,602][osd5][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[2021-11-03 20:52:47,609][osd5][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[2021-11-03 20:52:47,617][osd5][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[2021-11-03 20:52:47,624][osd5][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[2021-11-03 20:52:47,632][osd5][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-11-03 20:52:47,632][osd5][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-11-03 20:52:47,632][osd5][WARNING] ptype_tobe_for_name: name = journal
[2021-11-03 20:52:47,632][osd5][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-11-03 20:52:47,632][osd5][WARNING] create_partition: Creating journal partition num 2 size 5120 on /dev/sdb
[2021-11-03 20:52:47,632][osd5][WARNING] command_check_call: Running command: /sbin/sgdisk --new=2:0:+5120M --change-name=2:ceph journal --partition-guid=2:fd454c3d-e38e-4568-a671-d07996eaeb19 --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sdb
[2021-11-03 20:52:48,699][osd5][DEBUG ] Setting name!
[2021-11-03 20:52:48,699][osd5][DEBUG ] partNum is 1
[2021-11-03 20:52:48,699][osd5][DEBUG ] REALLY setting name!
[2021-11-03 20:52:48,699][osd5][DEBUG ] The operation has completed successfully.
[2021-11-03 20:52:48,699][osd5][WARNING] update_partition: Calling partprobe on created device /dev/sdb
[2021-11-03 20:52:48,699][osd5][WARNING] command_check_call: Running command: /usr/bin/udevadm settle --timeout=600
[2021-11-03 20:52:48,813][osd5][WARNING] command: Running command: /usr/bin/flock -s /dev/sdb /sbin/partprobe /dev/sdb
[2021-11-03 20:52:48,845][osd5][WARNING] command_check_call: Running command: /usr/bin/udevadm settle --timeout=600
[2021-11-03 20:52:48,959][osd5][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-11-03 20:52:48,959][osd5][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-11-03 20:52:48,960][osd5][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb2 uuid path is /sys/dev/block/8:18/dm/uuid
[2021-11-03 20:52:48,960][osd5][WARNING] prepare_device: Journal is GPT partition /dev/disk/by-partuuid/fd454c3d-e38e-4568-a671-d07996eaeb19
[2021-11-03 20:52:48,960][osd5][WARNING] prepare_device: Journal is GPT partition /dev/disk/by-partuuid/fd454c3d-e38e-4568-a671-d07996eaeb19
[2021-11-03 20:52:48,960][osd5][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-11-03 20:52:48,960][osd5][WARNING] set_data_partition: Creating osd partition on /dev/sdb
[2021-11-03 20:52:48,960][osd5][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-11-03 20:52:48,960][osd5][WARNING] ptype_tobe_for_name: name = data
[2021-11-03 20:52:48,960][osd5][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-11-03 20:52:48,960][osd5][WARNING] create_partition: Creating data partition num 1 size 0 on /dev/sdb
[2021-11-03 20:52:48,960][osd5][WARNING] command_check_call: Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:2ab0c24a-641f-42af-8952-866fa3506b2a --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be --mbrtogpt -- /dev/sdb
[2021-11-03 20:52:50,077][osd5][DEBUG ] Setting name!
[2021-11-03 20:52:50,077][osd5][DEBUG ] partNum is 0
[2021-11-03 20:52:50,077][osd5][DEBUG ] REALLY setting name!
[2021-11-03 20:52:50,077][osd5][DEBUG ] The operation has completed successfully.
[2021-11-03 20:52:50,077][osd5][WARNING] update_partition: Calling partprobe on created device /dev/sdb
[2021-11-03 20:52:50,078][osd5][WARNING] command_check_call: Running command: /usr/bin/udevadm settle --timeout=600
[2021-11-03 20:52:50,191][osd5][WARNING] command: Running command: /usr/bin/flock -s /dev/sdb /sbin/partprobe /dev/sdb
[2021-11-03 20:52:50,255][osd5][WARNING] command_check_call: Running command: /usr/bin/udevadm settle --timeout=600
[2021-11-03 20:52:50,369][osd5][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-11-03 20:52:50,370][osd5][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-11-03 20:52:50,370][osd5][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb1 uuid path is /sys/dev/block/8:17/dm/uuid
[2021-11-03 20:52:50,370][osd5][WARNING] populate_data_path_device: Creating xfs fs on /dev/sdb1
[2021-11-03 20:52:50,370][osd5][WARNING] command_check_call: Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/sdb1
[2021-11-03 20:52:50,935][osd5][DEBUG ] Discarding blocks...Done.
[2021-11-03 20:52:50,935][osd5][DEBUG ] meta-data=/dev/sdb1              isize=2048   agcount=4, agsize=196543 blks
[2021-11-03 20:52:50,936][osd5][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=1
[2021-11-03 20:52:50,936][osd5][DEBUG ]          =                       crc=1        finobt=0, sparse=0
[2021-11-03 20:52:50,936][osd5][DEBUG ] data     =                       bsize=4096   blocks=786171, imaxpct=25
[2021-11-03 20:52:50,936][osd5][DEBUG ]          =                       sunit=0      swidth=0 blks
[2021-11-03 20:52:50,936][osd5][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0 ftype=1
[2021-11-03 20:52:50,936][osd5][DEBUG ] log      =internal log           bsize=4096   blocks=2560, version=2
[2021-11-03 20:52:50,936][osd5][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[2021-11-03 20:52:50,936][osd5][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[2021-11-03 20:52:50,936][osd5][WARNING] mount: Mounting /dev/sdb1 on /var/lib/ceph/tmp/mnt.0L9sLv with options noatime,inode64
[2021-11-03 20:52:50,936][osd5][WARNING] command_check_call: Running command: /usr/bin/mount -t xfs -o noatime,inode64 -- /dev/sdb1 /var/lib/ceph/tmp/mnt.0L9sLv
[2021-11-03 20:52:50,936][osd5][WARNING] command: Running command: /sbin/restorecon /var/lib/ceph/tmp/mnt.0L9sLv
[2021-11-03 20:52:50,936][osd5][WARNING] populate_data_path: Preparing osd data dir /var/lib/ceph/tmp/mnt.0L9sLv
[2021-11-03 20:52:51,001][osd5][WARNING] command: Running command: /sbin/restorecon -R /var/lib/ceph/tmp/mnt.0L9sLv/ceph_fsid.9751.tmp
[2021-11-03 20:52:51,001][osd5][WARNING] command: Running command: /usr/bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.0L9sLv/ceph_fsid.9751.tmp
[2021-11-03 20:52:51,001][osd5][WARNING] command: Running command: /sbin/restorecon -R /var/lib/ceph/tmp/mnt.0L9sLv/fsid.9751.tmp
[2021-11-03 20:52:51,001][osd5][WARNING] command: Running command: /usr/bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.0L9sLv/fsid.9751.tmp
[2021-11-03 20:52:51,017][osd5][WARNING] command: Running command: /sbin/restorecon -R /var/lib/ceph/tmp/mnt.0L9sLv/magic.9751.tmp
[2021-11-03 20:52:51,018][osd5][WARNING] command: Running command: /usr/bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.0L9sLv/magic.9751.tmp
[2021-11-03 20:52:51,050][osd5][WARNING] command: Running command: /sbin/restorecon -R /var/lib/ceph/tmp/mnt.0L9sLv/journal_uuid.9751.tmp
[2021-11-03 20:52:51,050][osd5][WARNING] command: Running command: /usr/bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.0L9sLv/journal_uuid.9751.tmp
[2021-11-03 20:52:51,050][osd5][WARNING] adjust_symlink: Creating symlink /var/lib/ceph/tmp/mnt.0L9sLv/journal -> /dev/disk/by-partuuid/fd454c3d-e38e-4568-a671-d07996eaeb19
[2021-11-03 20:52:51,050][osd5][WARNING] command: Running command: /sbin/restorecon -R /var/lib/ceph/tmp/mnt.0L9sLv
[2021-11-03 20:52:51,050][osd5][WARNING] command: Running command: /usr/bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.0L9sLv
[2021-11-03 20:52:51,050][osd5][WARNING] unmount: Unmounting /var/lib/ceph/tmp/mnt.0L9sLv
[2021-11-03 20:52:51,050][osd5][WARNING] command_check_call: Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.0L9sLv
[2021-11-03 20:52:51,164][osd5][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-11-03 20:52:51,165][osd5][WARNING] command_check_call: Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/sdb
[2021-11-03 20:52:52,231][osd5][DEBUG ] Warning: The kernel is still using the old partition table.
[2021-11-03 20:52:52,232][osd5][DEBUG ] The new table will be used at the next reboot.
[2021-11-03 20:52:52,232][osd5][DEBUG ] The operation has completed successfully.
[2021-11-03 20:52:52,232][osd5][WARNING] update_partition: Calling partprobe on prepared device /dev/sdb
[2021-11-03 20:52:52,232][osd5][WARNING] command_check_call: Running command: /usr/bin/udevadm settle --timeout=600
[2021-11-03 20:52:52,233][osd5][WARNING] command: Running command: /usr/bin/flock -s /dev/sdb /sbin/partprobe /dev/sdb
[2021-11-03 20:52:52,249][osd5][WARNING] command_check_call: Running command: /usr/bin/udevadm settle --timeout=600
[2021-11-03 20:52:52,280][osd5][WARNING] command_check_call: Running command: /usr/bin/udevadm trigger --action=add --sysname-match sdb1
[2021-11-03 20:52:57,293][osd5][INFO  ] checking OSD status...
[2021-11-03 20:52:57,293][osd5][DEBUG ] find the location of an executable
[2021-11-03 20:52:57,296][osd5][INFO  ] Running command: sudo /bin/ceph --cluster=ceph osd stat --format=json
[2021-11-03 20:52:57,411][ceph_deploy.osd][DEBUG ] Host osd5 is now ready for osd use.
[2021-11-03 20:53:19,124][ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephuser/.cephdeploy.conf
[2021-11-03 20:53:19,124][ceph_deploy.cli][INFO  ] Invoked (1.5.39): /bin/ceph-deploy osd activate osd1:/dev/sdb1 osd2:/dev/sdb1 osd3:/dev/sdb1 osd4:/dev/sdb1 osd5:/dev/sdb1
[2021-11-03 20:53:19,124][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2021-11-03 20:53:19,124][ceph_deploy.cli][INFO  ]  username                      : None
[2021-11-03 20:53:19,124][ceph_deploy.cli][INFO  ]  verbose                       : False
[2021-11-03 20:53:19,124][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2021-11-03 20:53:19,125][ceph_deploy.cli][INFO  ]  subcommand                    : activate
[2021-11-03 20:53:19,125][ceph_deploy.cli][INFO  ]  quiet                         : False
[2021-11-03 20:53:19,125][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f7aa34d70e0>
[2021-11-03 20:53:19,125][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2021-11-03 20:53:19,125][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f7aa3524050>
[2021-11-03 20:53:19,125][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2021-11-03 20:53:19,125][ceph_deploy.cli][INFO  ]  default_release               : False
[2021-11-03 20:53:19,125][ceph_deploy.cli][INFO  ]  disk                          : [('osd1', '/dev/sdb1', None), ('osd2', '/dev/sdb1', None), ('osd3', '/dev/sdb1', None), ('osd4', '/dev/sdb1', None), ('osd5', '/dev/sdb1', None)]
[2021-11-03 20:53:19,125][ceph_deploy.osd][DEBUG ] Activating cluster ceph disks osd1:/dev/sdb1: osd2:/dev/sdb1: osd3:/dev/sdb1: osd4:/dev/sdb1: osd5:/dev/sdb1:
[2021-11-03 20:53:19,293][osd1][DEBUG ] connection detected need for sudo
[2021-11-03 20:53:19,461][osd1][DEBUG ] connected to host: osd1 
[2021-11-03 20:53:19,461][osd1][DEBUG ] detect platform information from remote host
[2021-11-03 20:53:19,478][osd1][DEBUG ] detect machine type
[2021-11-03 20:53:19,483][osd1][DEBUG ] find the location of an executable
[2021-11-03 20:53:19,484][ceph_deploy.osd][INFO  ] Distro info: CentOS Linux 7.9.2009 Core
[2021-11-03 20:53:19,484][ceph_deploy.osd][DEBUG ] activating host osd1 disk /dev/sdb1
[2021-11-03 20:53:19,484][ceph_deploy.osd][DEBUG ] will use init type: systemd
[2021-11-03 20:53:19,484][osd1][DEBUG ] find the location of an executable
[2021-11-03 20:53:19,486][osd1][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v activate --mark-init systemd --mount /dev/sdb1
[2021-11-03 20:53:19,555][osd1][WARNING] main_activate: path = /dev/sdb1
[2021-11-03 20:53:19,555][osd1][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb1 uuid path is /sys/dev/block/8:17/dm/uuid
[2021-11-03 20:53:19,555][osd1][WARNING] command: Running command: /sbin/blkid -o udev -p /dev/sdb1
[2021-11-03 20:53:19,559][osd1][WARNING] command: Running command: /sbin/blkid -p -s TYPE -o value -- /dev/sdb1
[2021-11-03 20:53:19,562][osd1][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[2021-11-03 20:53:19,578][osd1][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[2021-11-03 20:53:19,581][osd1][WARNING] mount: Mounting /dev/sdb1 on /var/lib/ceph/tmp/mnt.LfqDXg with options noatime,inode64
[2021-11-03 20:53:19,581][osd1][WARNING] command_check_call: Running command: /usr/bin/mount -t xfs -o noatime,inode64 -- /dev/sdb1 /var/lib/ceph/tmp/mnt.LfqDXg
[2021-11-03 20:53:19,585][osd1][WARNING] command: Running command: /sbin/restorecon /var/lib/ceph/tmp/mnt.LfqDXg
[2021-11-03 20:53:19,589][osd1][WARNING] activate: Cluster uuid is 67d9e224-a6f7-43d3-ae36-e6100c59258e
[2021-11-03 20:53:19,589][osd1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2021-11-03 20:53:19,605][osd1][WARNING] activate: Cluster name is ceph
[2021-11-03 20:53:19,605][osd1][WARNING] activate: OSD uuid is c0949dfc-f62a-44ad-be88-0ddc74834596
[2021-11-03 20:53:19,605][osd1][WARNING] activate: OSD id is 0
[2021-11-03 20:53:19,605][osd1][WARNING] activate: Marking with init system systemd
[2021-11-03 20:53:19,605][osd1][WARNING] command: Running command: /sbin/restorecon -R /var/lib/ceph/tmp/mnt.LfqDXg/systemd
[2021-11-03 20:53:19,609][osd1][WARNING] command: Running command: /usr/bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.LfqDXg/systemd
[2021-11-03 20:53:19,612][osd1][WARNING] activate: ceph osd.0 data dir is ready at /var/lib/ceph/tmp/mnt.LfqDXg
[2021-11-03 20:53:19,612][osd1][WARNING] mount_activate: ceph osd.0 already mounted in position; unmounting ours.
[2021-11-03 20:53:19,612][osd1][WARNING] unmount: Unmounting /var/lib/ceph/tmp/mnt.LfqDXg
[2021-11-03 20:53:19,612][osd1][WARNING] command_check_call: Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.LfqDXg
[2021-11-03 20:53:19,620][osd1][WARNING] start_daemon: Starting ceph osd.0...
[2021-11-03 20:53:19,620][osd1][WARNING] command_check_call: Running command: /usr/bin/systemctl disable ceph-osd@0
[2021-11-03 20:53:19,684][osd1][WARNING] command_check_call: Running command: /usr/bin/systemctl disable ceph-osd@0 --runtime
[2021-11-03 20:53:19,684][osd1][WARNING] Removed symlink /run/systemd/system/ceph-osd.target.wants/ceph-osd@0.service.
[2021-11-03 20:53:19,748][osd1][WARNING] command_check_call: Running command: /usr/bin/systemctl enable ceph-osd@0 --runtime
[2021-11-03 20:53:19,748][osd1][WARNING] Created symlink from /run/systemd/system/ceph-osd.target.wants/ceph-osd@0.service to /usr/lib/systemd/system/ceph-osd@.service.
[2021-11-03 20:53:19,812][osd1][WARNING] command_check_call: Running command: /usr/bin/systemctl start ceph-osd@0
[2021-11-03 20:53:24,817][osd1][INFO  ] checking OSD status...
[2021-11-03 20:53:24,817][osd1][DEBUG ] find the location of an executable
[2021-11-03 20:53:24,821][osd1][INFO  ] Running command: sudo /bin/ceph --cluster=ceph osd stat --format=json
[2021-11-03 20:53:24,939][osd1][INFO  ] Running command: sudo systemctl enable ceph.target
[2021-11-03 20:53:25,162][osd2][DEBUG ] connection detected need for sudo
[2021-11-03 20:53:25,330][osd2][DEBUG ] connected to host: osd2 
[2021-11-03 20:53:25,331][osd2][DEBUG ] detect platform information from remote host
[2021-11-03 20:53:25,349][osd2][DEBUG ] detect machine type
[2021-11-03 20:53:25,353][osd2][DEBUG ] find the location of an executable
[2021-11-03 20:53:25,355][ceph_deploy.osd][INFO  ] Distro info: CentOS Linux 7.9.2009 Core
[2021-11-03 20:53:25,355][ceph_deploy.osd][DEBUG ] activating host osd2 disk /dev/sdb1
[2021-11-03 20:53:25,355][ceph_deploy.osd][DEBUG ] will use init type: systemd
[2021-11-03 20:53:25,355][osd2][DEBUG ] find the location of an executable
[2021-11-03 20:53:25,358][osd2][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v activate --mark-init systemd --mount /dev/sdb1
[2021-11-03 20:53:25,475][osd2][WARNING] main_activate: path = /dev/sdb1
[2021-11-03 20:53:25,475][osd2][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb1 uuid path is /sys/dev/block/8:17/dm/uuid
[2021-11-03 20:53:25,475][osd2][WARNING] command: Running command: /sbin/blkid -o udev -p /dev/sdb1
[2021-11-03 20:53:25,475][osd2][WARNING] command: Running command: /sbin/blkid -p -s TYPE -o value -- /dev/sdb1
[2021-11-03 20:53:25,475][osd2][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[2021-11-03 20:53:25,475][osd2][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[2021-11-03 20:53:25,475][osd2][WARNING] mount: Mounting /dev/sdb1 on /var/lib/ceph/tmp/mnt.h3AjFX with options noatime,inode64
[2021-11-03 20:53:25,475][osd2][WARNING] command_check_call: Running command: /usr/bin/mount -t xfs -o noatime,inode64 -- /dev/sdb1 /var/lib/ceph/tmp/mnt.h3AjFX
[2021-11-03 20:53:25,475][osd2][WARNING] command: Running command: /sbin/restorecon /var/lib/ceph/tmp/mnt.h3AjFX
[2021-11-03 20:53:25,475][osd2][WARNING] activate: Cluster uuid is 67d9e224-a6f7-43d3-ae36-e6100c59258e
[2021-11-03 20:53:25,475][osd2][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2021-11-03 20:53:25,475][osd2][WARNING] activate: Cluster name is ceph
[2021-11-03 20:53:25,475][osd2][WARNING] activate: OSD uuid is dba47fdc-702c-4021-9bd7-c4f19f39ab75
[2021-11-03 20:53:25,475][osd2][WARNING] activate: OSD id is 1
[2021-11-03 20:53:25,476][osd2][WARNING] activate: Marking with init system systemd
[2021-11-03 20:53:25,476][osd2][WARNING] command: Running command: /sbin/restorecon -R /var/lib/ceph/tmp/mnt.h3AjFX/systemd
[2021-11-03 20:53:25,477][osd2][WARNING] command: Running command: /usr/bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.h3AjFX/systemd
[2021-11-03 20:53:25,480][osd2][WARNING] activate: ceph osd.1 data dir is ready at /var/lib/ceph/tmp/mnt.h3AjFX
[2021-11-03 20:53:25,480][osd2][WARNING] mount_activate: ceph osd.1 already mounted in position; unmounting ours.
[2021-11-03 20:53:25,480][osd2][WARNING] unmount: Unmounting /var/lib/ceph/tmp/mnt.h3AjFX
[2021-11-03 20:53:25,480][osd2][WARNING] command_check_call: Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.h3AjFX
[2021-11-03 20:53:25,484][osd2][WARNING] start_daemon: Starting ceph osd.1...
[2021-11-03 20:53:25,484][osd2][WARNING] command_check_call: Running command: /usr/bin/systemctl disable ceph-osd@1
[2021-11-03 20:53:25,548][osd2][WARNING] command_check_call: Running command: /usr/bin/systemctl disable ceph-osd@1 --runtime
[2021-11-03 20:53:25,548][osd2][WARNING] Removed symlink /run/systemd/system/ceph-osd.target.wants/ceph-osd@1.service.
[2021-11-03 20:53:25,612][osd2][WARNING] command_check_call: Running command: /usr/bin/systemctl enable ceph-osd@1 --runtime
[2021-11-03 20:53:25,612][osd2][WARNING] Created symlink from /run/systemd/system/ceph-osd.target.wants/ceph-osd@1.service to /usr/lib/systemd/system/ceph-osd@.service.
[2021-11-03 20:53:25,676][osd2][WARNING] command_check_call: Running command: /usr/bin/systemctl start ceph-osd@1
[2021-11-03 20:53:30,681][osd2][INFO  ] checking OSD status...
[2021-11-03 20:53:30,682][osd2][DEBUG ] find the location of an executable
[2021-11-03 20:53:30,685][osd2][INFO  ] Running command: sudo /bin/ceph --cluster=ceph osd stat --format=json
[2021-11-03 20:53:30,802][osd2][INFO  ] Running command: sudo systemctl enable ceph.target
[2021-11-03 20:53:31,025][osd3][DEBUG ] connection detected need for sudo
[2021-11-03 20:53:31,192][osd3][DEBUG ] connected to host: osd3 
[2021-11-03 20:53:31,193][osd3][DEBUG ] detect platform information from remote host
[2021-11-03 20:53:31,210][osd3][DEBUG ] detect machine type
[2021-11-03 20:53:31,214][osd3][DEBUG ] find the location of an executable
[2021-11-03 20:53:31,215][ceph_deploy.osd][INFO  ] Distro info: CentOS Linux 7.9.2009 Core
[2021-11-03 20:53:31,215][ceph_deploy.osd][DEBUG ] activating host osd3 disk /dev/sdb1
[2021-11-03 20:53:31,215][ceph_deploy.osd][DEBUG ] will use init type: systemd
[2021-11-03 20:53:31,216][osd3][DEBUG ] find the location of an executable
[2021-11-03 20:53:31,218][osd3][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v activate --mark-init systemd --mount /dev/sdb1
[2021-11-03 20:53:31,285][osd3][WARNING] main_activate: path = /dev/sdb1
[2021-11-03 20:53:31,285][osd3][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb1 uuid path is /sys/dev/block/8:17/dm/uuid
[2021-11-03 20:53:31,285][osd3][WARNING] command: Running command: /sbin/blkid -o udev -p /dev/sdb1
[2021-11-03 20:53:31,285][osd3][WARNING] command: Running command: /sbin/blkid -p -s TYPE -o value -- /dev/sdb1
[2021-11-03 20:53:31,286][osd3][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[2021-11-03 20:53:31,294][osd3][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[2021-11-03 20:53:31,301][osd3][WARNING] mount: Mounting /dev/sdb1 on /var/lib/ceph/tmp/mnt.6REy_I with options noatime,inode64
[2021-11-03 20:53:31,302][osd3][WARNING] command_check_call: Running command: /usr/bin/mount -t xfs -o noatime,inode64 -- /dev/sdb1 /var/lib/ceph/tmp/mnt.6REy_I
[2021-11-03 20:53:31,309][osd3][WARNING] command: Running command: /sbin/restorecon /var/lib/ceph/tmp/mnt.6REy_I
[2021-11-03 20:53:31,309][osd3][WARNING] activate: Cluster uuid is 67d9e224-a6f7-43d3-ae36-e6100c59258e
[2021-11-03 20:53:31,309][osd3][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2021-11-03 20:53:31,325][osd3][WARNING] activate: Cluster name is ceph
[2021-11-03 20:53:31,325][osd3][WARNING] activate: OSD uuid is 2f33abcf-3c86-45ad-8729-caa774a24dd7
[2021-11-03 20:53:31,325][osd3][WARNING] activate: OSD id is 2
[2021-11-03 20:53:31,325][osd3][WARNING] activate: Marking with init system systemd
[2021-11-03 20:53:31,325][osd3][WARNING] command: Running command: /sbin/restorecon -R /var/lib/ceph/tmp/mnt.6REy_I/systemd
[2021-11-03 20:53:31,326][osd3][WARNING] command: Running command: /usr/bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.6REy_I/systemd
[2021-11-03 20:53:31,330][osd3][WARNING] activate: ceph osd.2 data dir is ready at /var/lib/ceph/tmp/mnt.6REy_I
[2021-11-03 20:53:31,330][osd3][WARNING] mount_activate: ceph osd.2 already mounted in position; unmounting ours.
[2021-11-03 20:53:31,330][osd3][WARNING] unmount: Unmounting /var/lib/ceph/tmp/mnt.6REy_I
[2021-11-03 20:53:31,330][osd3][WARNING] command_check_call: Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.6REy_I
[2021-11-03 20:53:31,337][osd3][WARNING] start_daemon: Starting ceph osd.2...
[2021-11-03 20:53:31,338][osd3][WARNING] command_check_call: Running command: /usr/bin/systemctl disable ceph-osd@2
[2021-11-03 20:53:31,401][osd3][WARNING] command_check_call: Running command: /usr/bin/systemctl disable ceph-osd@2 --runtime
[2021-11-03 20:53:31,401][osd3][WARNING] Removed symlink /run/systemd/system/ceph-osd.target.wants/ceph-osd@2.service.
[2021-11-03 20:53:31,465][osd3][WARNING] command_check_call: Running command: /usr/bin/systemctl enable ceph-osd@2 --runtime
[2021-11-03 20:53:31,465][osd3][WARNING] Created symlink from /run/systemd/system/ceph-osd.target.wants/ceph-osd@2.service to /usr/lib/systemd/system/ceph-osd@.service.
[2021-11-03 20:53:31,529][osd3][WARNING] command_check_call: Running command: /usr/bin/systemctl start ceph-osd@2
[2021-11-03 20:53:36,534][osd3][INFO  ] checking OSD status...
[2021-11-03 20:53:36,535][osd3][DEBUG ] find the location of an executable
[2021-11-03 20:53:36,538][osd3][INFO  ] Running command: sudo /bin/ceph --cluster=ceph osd stat --format=json
[2021-11-03 20:53:36,655][osd3][INFO  ] Running command: sudo systemctl enable ceph.target
[2021-11-03 20:53:36,873][osd4][DEBUG ] connection detected need for sudo
[2021-11-03 20:53:37,039][osd4][DEBUG ] connected to host: osd4 
[2021-11-03 20:53:37,040][osd4][DEBUG ] detect platform information from remote host
[2021-11-03 20:53:37,059][osd4][DEBUG ] detect machine type
[2021-11-03 20:53:37,063][osd4][DEBUG ] find the location of an executable
[2021-11-03 20:53:37,064][ceph_deploy.osd][INFO  ] Distro info: CentOS Linux 7.9.2009 Core
[2021-11-03 20:53:37,064][ceph_deploy.osd][DEBUG ] activating host osd4 disk /dev/sdb1
[2021-11-03 20:53:37,064][ceph_deploy.osd][DEBUG ] will use init type: systemd
[2021-11-03 20:53:37,064][osd4][DEBUG ] find the location of an executable
[2021-11-03 20:53:37,067][osd4][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v activate --mark-init systemd --mount /dev/sdb1
[2021-11-03 20:53:37,133][osd4][WARNING] main_activate: path = /dev/sdb1
[2021-11-03 20:53:37,133][osd4][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb1 uuid path is /sys/dev/block/8:17/dm/uuid
[2021-11-03 20:53:37,133][osd4][WARNING] command: Running command: /sbin/blkid -o udev -p /dev/sdb1
[2021-11-03 20:53:37,133][osd4][WARNING] command: Running command: /sbin/blkid -p -s TYPE -o value -- /dev/sdb1
[2021-11-03 20:53:37,137][osd4][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[2021-11-03 20:53:37,144][osd4][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[2021-11-03 20:53:37,160][osd4][WARNING] mount: Mounting /dev/sdb1 on /var/lib/ceph/tmp/mnt.zwroy4 with options noatime,inode64
[2021-11-03 20:53:37,160][osd4][WARNING] command_check_call: Running command: /usr/bin/mount -t xfs -o noatime,inode64 -- /dev/sdb1 /var/lib/ceph/tmp/mnt.zwroy4
[2021-11-03 20:53:37,160][osd4][WARNING] command: Running command: /sbin/restorecon /var/lib/ceph/tmp/mnt.zwroy4
[2021-11-03 20:53:37,160][osd4][WARNING] activate: Cluster uuid is 67d9e224-a6f7-43d3-ae36-e6100c59258e
[2021-11-03 20:53:37,160][osd4][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2021-11-03 20:53:37,175][osd4][WARNING] activate: Cluster name is ceph
[2021-11-03 20:53:37,176][osd4][WARNING] activate: OSD uuid is aa17af76-99b7-4682-90ef-bd512efa2dc0
[2021-11-03 20:53:37,176][osd4][WARNING] activate: OSD id is 3
[2021-11-03 20:53:37,176][osd4][WARNING] activate: Marking with init system systemd
[2021-11-03 20:53:37,176][osd4][WARNING] command: Running command: /sbin/restorecon -R /var/lib/ceph/tmp/mnt.zwroy4/systemd
[2021-11-03 20:53:37,177][osd4][WARNING] command: Running command: /usr/bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.zwroy4/systemd
[2021-11-03 20:53:37,180][osd4][WARNING] activate: ceph osd.3 data dir is ready at /var/lib/ceph/tmp/mnt.zwroy4
[2021-11-03 20:53:37,180][osd4][WARNING] mount_activate: ceph osd.3 already mounted in position; unmounting ours.
[2021-11-03 20:53:37,180][osd4][WARNING] unmount: Unmounting /var/lib/ceph/tmp/mnt.zwroy4
[2021-11-03 20:53:37,180][osd4][WARNING] command_check_call: Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.zwroy4
[2021-11-03 20:53:37,184][osd4][WARNING] start_daemon: Starting ceph osd.3...
[2021-11-03 20:53:37,184][osd4][WARNING] command_check_call: Running command: /usr/bin/systemctl disable ceph-osd@3
[2021-11-03 20:53:37,248][osd4][WARNING] command_check_call: Running command: /usr/bin/systemctl disable ceph-osd@3 --runtime
[2021-11-03 20:53:37,248][osd4][WARNING] Removed symlink /run/systemd/system/ceph-osd.target.wants/ceph-osd@3.service.
[2021-11-03 20:53:37,312][osd4][WARNING] command_check_call: Running command: /usr/bin/systemctl enable ceph-osd@3 --runtime
[2021-11-03 20:53:37,312][osd4][WARNING] Created symlink from /run/systemd/system/ceph-osd.target.wants/ceph-osd@3.service to /usr/lib/systemd/system/ceph-osd@.service.
[2021-11-03 20:53:37,344][osd4][WARNING] command_check_call: Running command: /usr/bin/systemctl start ceph-osd@3
[2021-11-03 20:53:42,364][osd4][INFO  ] checking OSD status...
[2021-11-03 20:53:42,364][osd4][DEBUG ] find the location of an executable
[2021-11-03 20:53:42,367][osd4][INFO  ] Running command: sudo /bin/ceph --cluster=ceph osd stat --format=json
[2021-11-03 20:53:42,485][osd4][INFO  ] Running command: sudo systemctl enable ceph.target
[2021-11-03 20:53:42,704][osd5][DEBUG ] connection detected need for sudo
[2021-11-03 20:53:42,871][osd5][DEBUG ] connected to host: osd5 
[2021-11-03 20:53:42,871][osd5][DEBUG ] detect platform information from remote host
[2021-11-03 20:53:42,887][osd5][DEBUG ] detect machine type
[2021-11-03 20:53:42,892][osd5][DEBUG ] find the location of an executable
[2021-11-03 20:53:42,893][ceph_deploy.osd][INFO  ] Distro info: CentOS Linux 7.9.2009 Core
[2021-11-03 20:53:42,893][ceph_deploy.osd][DEBUG ] activating host osd5 disk /dev/sdb1
[2021-11-03 20:53:42,893][ceph_deploy.osd][DEBUG ] will use init type: systemd
[2021-11-03 20:53:42,893][osd5][DEBUG ] find the location of an executable
[2021-11-03 20:53:42,895][osd5][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v activate --mark-init systemd --mount /dev/sdb1
[2021-11-03 20:53:42,962][osd5][WARNING] main_activate: path = /dev/sdb1
[2021-11-03 20:53:42,962][osd5][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb1 uuid path is /sys/dev/block/8:17/dm/uuid
[2021-11-03 20:53:42,962][osd5][WARNING] command: Running command: /sbin/blkid -o udev -p /dev/sdb1
[2021-11-03 20:53:42,966][osd5][WARNING] command: Running command: /sbin/blkid -p -s TYPE -o value -- /dev/sdb1
[2021-11-03 20:53:42,969][osd5][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[2021-11-03 20:53:42,977][osd5][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[2021-11-03 20:53:42,984][osd5][WARNING] mount: Mounting /dev/sdb1 on /var/lib/ceph/tmp/mnt.WivcUz with options noatime,inode64
[2021-11-03 20:53:42,984][osd5][WARNING] command_check_call: Running command: /usr/bin/mount -t xfs -o noatime,inode64 -- /dev/sdb1 /var/lib/ceph/tmp/mnt.WivcUz
[2021-11-03 20:53:42,987][osd5][WARNING] command: Running command: /sbin/restorecon /var/lib/ceph/tmp/mnt.WivcUz
[2021-11-03 20:53:42,991][osd5][WARNING] activate: Cluster uuid is 67d9e224-a6f7-43d3-ae36-e6100c59258e
[2021-11-03 20:53:42,991][osd5][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2021-11-03 20:53:43,006][osd5][WARNING] activate: Cluster name is ceph
[2021-11-03 20:53:43,007][osd5][WARNING] activate: OSD uuid is 2ab0c24a-641f-42af-8952-866fa3506b2a
[2021-11-03 20:53:43,007][osd5][WARNING] activate: OSD id is 4
[2021-11-03 20:53:43,007][osd5][WARNING] activate: Marking with init system systemd
[2021-11-03 20:53:43,007][osd5][WARNING] command: Running command: /sbin/restorecon -R /var/lib/ceph/tmp/mnt.WivcUz/systemd
[2021-11-03 20:53:43,007][osd5][WARNING] command: Running command: /usr/bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.WivcUz/systemd
[2021-11-03 20:53:43,010][osd5][WARNING] activate: ceph osd.4 data dir is ready at /var/lib/ceph/tmp/mnt.WivcUz
[2021-11-03 20:53:43,010][osd5][WARNING] mount_activate: ceph osd.4 already mounted in position; unmounting ours.
[2021-11-03 20:53:43,010][osd5][WARNING] unmount: Unmounting /var/lib/ceph/tmp/mnt.WivcUz
[2021-11-03 20:53:43,010][osd5][WARNING] command_check_call: Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.WivcUz
[2021-11-03 20:53:43,014][osd5][WARNING] start_daemon: Starting ceph osd.4...
[2021-11-03 20:53:43,014][osd5][WARNING] command_check_call: Running command: /usr/bin/systemctl disable ceph-osd@4
[2021-11-03 20:53:43,077][osd5][WARNING] command_check_call: Running command: /usr/bin/systemctl disable ceph-osd@4 --runtime
[2021-11-03 20:53:43,078][osd5][WARNING] Removed symlink /run/systemd/system/ceph-osd.target.wants/ceph-osd@4.service.
[2021-11-03 20:53:43,141][osd5][WARNING] command_check_call: Running command: /usr/bin/systemctl enable ceph-osd@4 --runtime
[2021-11-03 20:53:43,142][osd5][WARNING] Created symlink from /run/systemd/system/ceph-osd.target.wants/ceph-osd@4.service to /usr/lib/systemd/system/ceph-osd@.service.
[2021-11-03 20:53:43,205][osd5][WARNING] command_check_call: Running command: /usr/bin/systemctl start ceph-osd@4
[2021-11-03 20:53:48,211][osd5][INFO  ] checking OSD status...
[2021-11-03 20:53:48,211][osd5][DEBUG ] find the location of an executable
[2021-11-03 20:53:48,214][osd5][INFO  ] Running command: sudo /bin/ceph --cluster=ceph osd stat --format=json
[2021-11-03 20:53:48,332][osd5][INFO  ] Running command: sudo systemctl enable ceph.target
[2021-11-03 20:53:56,763][ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephuser/.cephdeploy.conf
[2021-11-03 20:53:56,764][ceph_deploy.cli][INFO  ] Invoked (1.5.39): /bin/ceph-deploy disk list osd1 osd2 osd3 osd4 osd5
[2021-11-03 20:53:56,764][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2021-11-03 20:53:56,764][ceph_deploy.cli][INFO  ]  username                      : None
[2021-11-03 20:53:56,764][ceph_deploy.cli][INFO  ]  verbose                       : False
[2021-11-03 20:53:56,764][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2021-11-03 20:53:56,764][ceph_deploy.cli][INFO  ]  subcommand                    : list
[2021-11-03 20:53:56,764][ceph_deploy.cli][INFO  ]  quiet                         : False
[2021-11-03 20:53:56,764][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fc334adeea8>
[2021-11-03 20:53:56,764][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2021-11-03 20:53:56,764][ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7fc334ab90c8>
[2021-11-03 20:53:56,764][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2021-11-03 20:53:56,764][ceph_deploy.cli][INFO  ]  default_release               : False
[2021-11-03 20:53:56,764][ceph_deploy.cli][INFO  ]  disk                          : [('osd1', None, None), ('osd2', None, None), ('osd3', None, None), ('osd4', None, None), ('osd5', None, None)]
[2021-11-03 20:53:56,931][osd1][DEBUG ] connection detected need for sudo
[2021-11-03 20:53:57,109][osd1][DEBUG ] connected to host: osd1 
[2021-11-03 20:53:57,109][osd1][DEBUG ] detect platform information from remote host
[2021-11-03 20:53:57,128][osd1][DEBUG ] detect machine type
[2021-11-03 20:53:57,133][osd1][DEBUG ] find the location of an executable
[2021-11-03 20:53:57,134][ceph_deploy.osd][INFO  ] Distro info: CentOS Linux 7.9.2009 Core
[2021-11-03 20:53:57,134][ceph_deploy.osd][DEBUG ] Listing disks on osd1...
[2021-11-03 20:53:57,134][osd1][DEBUG ] find the location of an executable
[2021-11-03 20:53:57,136][osd1][INFO  ] Running command: sudo /usr/sbin/ceph-disk list
[2021-11-03 20:53:57,355][osd1][DEBUG ] /dev/dm-0 other, xfs, mounted on /
[2021-11-03 20:53:57,355][osd1][DEBUG ] /dev/dm-1 swap, swap
[2021-11-03 20:53:57,355][osd1][DEBUG ] /dev/sda :
[2021-11-03 20:53:57,356][osd1][DEBUG ]  /dev/sda2 other, LVM2_member
[2021-11-03 20:53:57,356][osd1][DEBUG ]  /dev/sda1 other, xfs, mounted on /boot
[2021-11-03 20:53:57,356][osd1][DEBUG ] /dev/sdb :
[2021-11-03 20:53:57,356][osd1][DEBUG ]  /dev/sdb2 ceph journal, for /dev/sdb1
[2021-11-03 20:53:57,356][osd1][DEBUG ]  /dev/sdb1 ceph data, active, cluster ceph, osd.0, journal /dev/sdb2
[2021-11-03 20:53:57,356][osd1][DEBUG ] /dev/sr0 other, unknown
[2021-11-03 20:53:57,511][osd2][DEBUG ] connection detected need for sudo
[2021-11-03 20:53:57,683][osd2][DEBUG ] connected to host: osd2 
[2021-11-03 20:53:57,683][osd2][DEBUG ] detect platform information from remote host
[2021-11-03 20:53:57,701][osd2][DEBUG ] detect machine type
[2021-11-03 20:53:57,706][osd2][DEBUG ] find the location of an executable
[2021-11-03 20:53:57,707][ceph_deploy.osd][INFO  ] Distro info: CentOS Linux 7.9.2009 Core
[2021-11-03 20:53:57,707][ceph_deploy.osd][DEBUG ] Listing disks on osd2...
[2021-11-03 20:53:57,708][osd2][DEBUG ] find the location of an executable
[2021-11-03 20:53:57,710][osd2][INFO  ] Running command: sudo /usr/sbin/ceph-disk list
[2021-11-03 20:53:57,927][osd2][DEBUG ] /dev/dm-0 other, xfs, mounted on /
[2021-11-03 20:53:57,927][osd2][DEBUG ] /dev/dm-1 swap, swap
[2021-11-03 20:53:57,927][osd2][DEBUG ] /dev/sda :
[2021-11-03 20:53:57,927][osd2][DEBUG ]  /dev/sda2 other, LVM2_member
[2021-11-03 20:53:57,927][osd2][DEBUG ]  /dev/sda1 other, xfs, mounted on /boot
[2021-11-03 20:53:57,927][osd2][DEBUG ] /dev/sdb :
[2021-11-03 20:53:57,927][osd2][DEBUG ]  /dev/sdb2 ceph journal, for /dev/sdb1
[2021-11-03 20:53:57,927][osd2][DEBUG ]  /dev/sdb1 ceph data, active, cluster ceph, osd.1, journal /dev/sdb2
[2021-11-03 20:53:57,928][osd2][DEBUG ] /dev/sr0 other, unknown
[2021-11-03 20:53:58,081][osd3][DEBUG ] connection detected need for sudo
[2021-11-03 20:53:58,250][osd3][DEBUG ] connected to host: osd3 
[2021-11-03 20:53:58,251][osd3][DEBUG ] detect platform information from remote host
[2021-11-03 20:53:58,267][osd3][DEBUG ] detect machine type
[2021-11-03 20:53:58,272][osd3][DEBUG ] find the location of an executable
[2021-11-03 20:53:58,273][ceph_deploy.osd][INFO  ] Distro info: CentOS Linux 7.9.2009 Core
[2021-11-03 20:53:58,273][ceph_deploy.osd][DEBUG ] Listing disks on osd3...
[2021-11-03 20:53:58,273][osd3][DEBUG ] find the location of an executable
[2021-11-03 20:53:58,276][osd3][INFO  ] Running command: sudo /usr/sbin/ceph-disk list
[2021-11-03 20:53:58,443][osd3][DEBUG ] /dev/dm-0 other, xfs, mounted on /
[2021-11-03 20:53:58,443][osd3][DEBUG ] /dev/dm-1 swap, swap
[2021-11-03 20:53:58,443][osd3][DEBUG ] /dev/sda :
[2021-11-03 20:53:58,443][osd3][DEBUG ]  /dev/sda2 other, LVM2_member
[2021-11-03 20:53:58,443][osd3][DEBUG ]  /dev/sda1 other, xfs, mounted on /boot
[2021-11-03 20:53:58,444][osd3][DEBUG ] /dev/sdb :
[2021-11-03 20:53:58,444][osd3][DEBUG ]  /dev/sdb2 ceph journal, for /dev/sdb1
[2021-11-03 20:53:58,444][osd3][DEBUG ]  /dev/sdb1 ceph data, active, cluster ceph, osd.2, journal /dev/sdb2
[2021-11-03 20:53:58,596][osd4][DEBUG ] connection detected need for sudo
[2021-11-03 20:53:58,761][osd4][DEBUG ] connected to host: osd4 
[2021-11-03 20:53:58,762][osd4][DEBUG ] detect platform information from remote host
[2021-11-03 20:53:58,778][osd4][DEBUG ] detect machine type
[2021-11-03 20:53:58,783][osd4][DEBUG ] find the location of an executable
[2021-11-03 20:53:58,784][ceph_deploy.osd][INFO  ] Distro info: CentOS Linux 7.9.2009 Core
[2021-11-03 20:53:58,784][ceph_deploy.osd][DEBUG ] Listing disks on osd4...
[2021-11-03 20:53:58,784][osd4][DEBUG ] find the location of an executable
[2021-11-03 20:53:58,787][osd4][INFO  ] Running command: sudo /usr/sbin/ceph-disk list
[2021-11-03 20:53:59,003][osd4][DEBUG ] /dev/dm-0 other, xfs, mounted on /
[2021-11-03 20:53:59,004][osd4][DEBUG ] /dev/dm-1 swap, swap
[2021-11-03 20:53:59,004][osd4][DEBUG ] /dev/sda :
[2021-11-03 20:53:59,004][osd4][DEBUG ]  /dev/sda2 other, LVM2_member
[2021-11-03 20:53:59,004][osd4][DEBUG ]  /dev/sda1 other, xfs, mounted on /boot
[2021-11-03 20:53:59,004][osd4][DEBUG ] /dev/sdb :
[2021-11-03 20:53:59,004][osd4][DEBUG ]  /dev/sdb2 ceph journal, for /dev/sdb1
[2021-11-03 20:53:59,004][osd4][DEBUG ]  /dev/sdb1 ceph data, active, cluster ceph, osd.3, journal /dev/sdb2
[2021-11-03 20:53:59,158][osd5][DEBUG ] connection detected need for sudo
[2021-11-03 20:53:59,326][osd5][DEBUG ] connected to host: osd5 
[2021-11-03 20:53:59,326][osd5][DEBUG ] detect platform information from remote host
[2021-11-03 20:53:59,343][osd5][DEBUG ] detect machine type
[2021-11-03 20:53:59,347][osd5][DEBUG ] find the location of an executable
[2021-11-03 20:53:59,349][ceph_deploy.osd][INFO  ] Distro info: CentOS Linux 7.9.2009 Core
[2021-11-03 20:53:59,349][ceph_deploy.osd][DEBUG ] Listing disks on osd5...
[2021-11-03 20:53:59,349][osd5][DEBUG ] find the location of an executable
[2021-11-03 20:53:59,351][osd5][INFO  ] Running command: sudo /usr/sbin/ceph-disk list
[2021-11-03 20:53:59,518][osd5][DEBUG ] /dev/dm-0 other, xfs, mounted on /
[2021-11-03 20:53:59,519][osd5][DEBUG ] /dev/dm-1 swap, swap
[2021-11-03 20:53:59,519][osd5][DEBUG ] /dev/sda :
[2021-11-03 20:53:59,519][osd5][DEBUG ]  /dev/sda2 other, LVM2_member
[2021-11-03 20:53:59,519][osd5][DEBUG ]  /dev/sda1 other, xfs, mounted on /boot
[2021-11-03 20:53:59,519][osd5][DEBUG ] /dev/sdb :
[2021-11-03 20:53:59,519][osd5][DEBUG ]  /dev/sdb2 ceph journal, for /dev/sdb1
[2021-11-03 20:53:59,519][osd5][DEBUG ]  /dev/sdb1 ceph data, active, cluster ceph, osd.4, journal /dev/sdb2
[2021-11-03 20:55:05,349][ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephuser/.cephdeploy.conf
[2021-11-03 20:55:05,349][ceph_deploy.cli][INFO  ] Invoked (1.5.39): /bin/ceph-deploy admin ceph-admin mon1 osd1 osd2 osd3 osd4 osd5
[2021-11-03 20:55:05,349][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2021-11-03 20:55:05,349][ceph_deploy.cli][INFO  ]  username                      : None
[2021-11-03 20:55:05,349][ceph_deploy.cli][INFO  ]  verbose                       : False
[2021-11-03 20:55:05,349][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2021-11-03 20:55:05,349][ceph_deploy.cli][INFO  ]  quiet                         : False
[2021-11-03 20:55:05,350][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f680082db00>
[2021-11-03 20:55:05,350][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2021-11-03 20:55:05,350][ceph_deploy.cli][INFO  ]  client                        : ['ceph-admin', 'mon1', 'osd1', 'osd2', 'osd3', 'osd4', 'osd5']
[2021-11-03 20:55:05,350][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7f6801342a28>
[2021-11-03 20:55:05,350][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2021-11-03 20:55:05,350][ceph_deploy.cli][INFO  ]  default_release               : False
[2021-11-03 20:55:05,350][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to ceph-admin
[2021-11-03 20:55:05,381][ceph-admin][DEBUG ] connection detected need for sudo
[2021-11-03 20:55:05,409][ceph-admin][DEBUG ] connected to host: ceph-admin 
[2021-11-03 20:55:05,410][ceph-admin][DEBUG ] detect platform information from remote host
[2021-11-03 20:55:05,428][ceph-admin][DEBUG ] detect machine type
[2021-11-03 20:55:05,432][ceph-admin][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2021-11-03 20:55:05,433][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to mon1
[2021-11-03 20:55:05,588][mon1][DEBUG ] connection detected need for sudo
[2021-11-03 20:55:05,759][mon1][DEBUG ] connected to host: mon1 
[2021-11-03 20:55:05,760][mon1][DEBUG ] detect platform information from remote host
[2021-11-03 20:55:05,777][mon1][DEBUG ] detect machine type
[2021-11-03 20:55:05,782][mon1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2021-11-03 20:55:05,785][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to osd1
[2021-11-03 20:55:05,940][osd1][DEBUG ] connection detected need for sudo
[2021-11-03 20:55:06,116][osd1][DEBUG ] connected to host: osd1 
[2021-11-03 20:55:06,116][osd1][DEBUG ] detect platform information from remote host
[2021-11-03 20:55:06,135][osd1][DEBUG ] detect machine type
[2021-11-03 20:55:06,140][osd1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2021-11-03 20:55:06,143][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to osd2
[2021-11-03 20:55:06,303][osd2][DEBUG ] connection detected need for sudo
[2021-11-03 20:55:06,476][osd2][DEBUG ] connected to host: osd2 
[2021-11-03 20:55:06,477][osd2][DEBUG ] detect platform information from remote host
[2021-11-03 20:55:06,494][osd2][DEBUG ] detect machine type
[2021-11-03 20:55:06,499][osd2][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2021-11-03 20:55:06,502][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to osd3
[2021-11-03 20:55:06,657][osd3][DEBUG ] connection detected need for sudo
[2021-11-03 20:55:06,825][osd3][DEBUG ] connected to host: osd3 
[2021-11-03 20:55:06,826][osd3][DEBUG ] detect platform information from remote host
[2021-11-03 20:55:06,843][osd3][DEBUG ] detect machine type
[2021-11-03 20:55:06,848][osd3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2021-11-03 20:55:06,851][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to osd4
[2021-11-03 20:55:07,007][osd4][DEBUG ] connection detected need for sudo
[2021-11-03 20:55:07,174][osd4][DEBUG ] connected to host: osd4 
[2021-11-03 20:55:07,174][osd4][DEBUG ] detect platform information from remote host
[2021-11-03 20:55:07,190][osd4][DEBUG ] detect machine type
[2021-11-03 20:55:07,195][osd4][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2021-11-03 20:55:07,198][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to osd5
[2021-11-03 20:55:07,354][osd5][DEBUG ] connection detected need for sudo
[2021-11-03 20:55:07,529][osd5][DEBUG ] connected to host: osd5 
[2021-11-03 20:55:07,530][osd5][DEBUG ] detect platform information from remote host
[2021-11-03 20:55:07,547][osd5][DEBUG ] detect machine type
[2021-11-03 20:55:07,552][osd5][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2021-11-03 21:22:37,792][ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephuser/.cephdeploy.conf
[2021-11-03 21:22:37,793][ceph_deploy.cli][INFO  ] Invoked (1.5.39): /bin/ceph-deploy install client
[2021-11-03 21:22:37,793][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2021-11-03 21:22:37,793][ceph_deploy.cli][INFO  ]  verbose                       : False
[2021-11-03 21:22:37,793][ceph_deploy.cli][INFO  ]  testing                       : None
[2021-11-03 21:22:37,793][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fbc52a851b8>
[2021-11-03 21:22:37,793][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2021-11-03 21:22:37,793][ceph_deploy.cli][INFO  ]  dev_commit                    : None
[2021-11-03 21:22:37,793][ceph_deploy.cli][INFO  ]  install_mds                   : False
[2021-11-03 21:22:37,793][ceph_deploy.cli][INFO  ]  stable                        : None
[2021-11-03 21:22:37,793][ceph_deploy.cli][INFO  ]  default_release               : False
[2021-11-03 21:22:37,794][ceph_deploy.cli][INFO  ]  username                      : None
[2021-11-03 21:22:37,794][ceph_deploy.cli][INFO  ]  adjust_repos                  : True
[2021-11-03 21:22:37,794][ceph_deploy.cli][INFO  ]  func                          : <function install at 0x7fbc53543de8>
[2021-11-03 21:22:37,794][ceph_deploy.cli][INFO  ]  install_mgr                   : False
[2021-11-03 21:22:37,794][ceph_deploy.cli][INFO  ]  install_all                   : False
[2021-11-03 21:22:37,794][ceph_deploy.cli][INFO  ]  repo                          : False
[2021-11-03 21:22:37,794][ceph_deploy.cli][INFO  ]  host                          : ['client']
[2021-11-03 21:22:37,794][ceph_deploy.cli][INFO  ]  install_rgw                   : False
[2021-11-03 21:22:37,794][ceph_deploy.cli][INFO  ]  install_tests                 : False
[2021-11-03 21:22:37,794][ceph_deploy.cli][INFO  ]  repo_url                      : None
[2021-11-03 21:22:37,794][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2021-11-03 21:22:37,794][ceph_deploy.cli][INFO  ]  install_osd                   : False
[2021-11-03 21:22:37,794][ceph_deploy.cli][INFO  ]  version_kind                  : stable
[2021-11-03 21:22:37,794][ceph_deploy.cli][INFO  ]  install_common                : False
[2021-11-03 21:22:37,794][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2021-11-03 21:22:37,794][ceph_deploy.cli][INFO  ]  quiet                         : False
[2021-11-03 21:22:37,794][ceph_deploy.cli][INFO  ]  dev                           : master
[2021-11-03 21:22:37,794][ceph_deploy.cli][INFO  ]  nogpgcheck                    : False
[2021-11-03 21:22:37,794][ceph_deploy.cli][INFO  ]  local_mirror                  : None
[2021-11-03 21:22:37,794][ceph_deploy.cli][INFO  ]  release                       : None
[2021-11-03 21:22:37,794][ceph_deploy.cli][INFO  ]  install_mon                   : False
[2021-11-03 21:22:37,794][ceph_deploy.cli][INFO  ]  gpg_url                       : None
[2021-11-03 21:22:37,795][ceph_deploy.install][DEBUG ] Installing stable version jewel on cluster ceph hosts client
[2021-11-03 21:22:37,795][ceph_deploy.install][DEBUG ] Detecting platform for host client ...
[2021-11-03 21:22:38,006][client][DEBUG ] connection detected need for sudo
[2021-11-03 21:22:38,175][client][DEBUG ] connected to host: client 
[2021-11-03 21:22:38,177][client][DEBUG ] detect platform information from remote host
[2021-11-03 21:22:38,212][client][DEBUG ] detect machine type
[2021-11-03 21:22:38,217][ceph_deploy.install][INFO  ] Distro info: CentOS Linux 7.9.2009 Core
[2021-11-03 21:22:38,217][client][INFO  ] installing Ceph on client
[2021-11-03 21:22:38,219][client][INFO  ] Running command: sudo yum clean all
[2021-11-03 21:22:39,040][client][DEBUG ] Loaded plugins: fastestmirror
[2021-11-03 21:22:39,072][client][DEBUG ] Cleaning repos: base extras updates
[2021-11-03 21:22:39,076][client][DEBUG ] Cleaning up list of fastest mirrors
[2021-11-03 21:22:39,081][client][INFO  ] Running command: sudo yum -y install epel-release
[2021-11-03 21:22:39,197][client][DEBUG ] Loaded plugins: fastestmirror
[2021-11-03 21:22:39,229][client][DEBUG ] Determining fastest mirrors
[2021-11-03 21:22:55,395][client][DEBUG ]  * base: mirror.hackingand.coffee
[2021-11-03 21:22:55,395][client][DEBUG ]  * extras: mirror.grid.uchicago.edu
[2021-11-03 21:22:55,395][client][DEBUG ]  * updates: mirror.dal.nexril.net
[2021-11-03 21:23:21,184][client][DEBUG ] Resolving Dependencies
[2021-11-03 21:23:21,184][client][DEBUG ] --> Running transaction check
[2021-11-03 21:23:21,184][client][DEBUG ] ---> Package epel-release.noarch 0:7-11 will be installed
[2021-11-03 21:23:22,551][client][DEBUG ] --> Finished Dependency Resolution
[2021-11-03 21:23:22,615][client][DEBUG ] 
[2021-11-03 21:23:22,616][client][DEBUG ] Dependencies Resolved
[2021-11-03 21:23:22,616][client][DEBUG ] 
[2021-11-03 21:23:22,616][client][DEBUG ] ================================================================================
[2021-11-03 21:23:22,616][client][DEBUG ]  Package                Arch             Version         Repository        Size
[2021-11-03 21:23:22,616][client][DEBUG ] ================================================================================
[2021-11-03 21:23:22,616][client][DEBUG ] Installing:
[2021-11-03 21:23:22,616][client][DEBUG ]  epel-release           noarch           7-11            extras            15 k
[2021-11-03 21:23:22,616][client][DEBUG ] 
[2021-11-03 21:23:22,616][client][DEBUG ] Transaction Summary
[2021-11-03 21:23:22,616][client][DEBUG ] ================================================================================
[2021-11-03 21:23:22,616][client][DEBUG ] Install  1 Package
[2021-11-03 21:23:22,616][client][DEBUG ] 
[2021-11-03 21:23:22,616][client][DEBUG ] Total download size: 15 k
[2021-11-03 21:23:22,616][client][DEBUG ] Installed size: 24 k
[2021-11-03 21:23:22,616][client][DEBUG ] Downloading packages:
[2021-11-03 21:23:28,246][client][DEBUG ] Running transaction check
[2021-11-03 21:23:28,246][client][DEBUG ] Running transaction test
[2021-11-03 21:23:28,278][client][DEBUG ] Transaction test succeeded
[2021-11-03 21:23:28,278][client][DEBUG ] Running transaction
[2021-11-03 21:23:28,944][client][DEBUG ]   Installing : epel-release-7-11.noarch                                     1/1 
[2021-11-03 21:23:29,358][client][DEBUG ]   Verifying  : epel-release-7-11.noarch                                     1/1 
[2021-11-03 21:23:29,359][client][DEBUG ] 
[2021-11-03 21:23:29,359][client][DEBUG ] Installed:
[2021-11-03 21:23:29,359][client][DEBUG ]   epel-release.noarch 0:7-11                                                    
[2021-11-03 21:23:29,359][client][DEBUG ] 
[2021-11-03 21:23:29,359][client][DEBUG ] Complete!
[2021-11-03 21:23:29,368][client][INFO  ] Running command: sudo yum -y install yum-plugin-priorities
[2021-11-03 21:23:29,484][client][DEBUG ] Loaded plugins: fastestmirror
[2021-11-03 21:23:29,516][client][DEBUG ] Loading mirror speeds from cached hostfile
[2021-11-03 21:23:50,839][client][DEBUG ]  * base: mirror.hackingand.coffee
[2021-11-03 21:23:50,839][client][DEBUG ]  * epel: pubmirror1.math.uh.edu
[2021-11-03 21:23:50,839][client][DEBUG ]  * extras: mirror.grid.uchicago.edu
[2021-11-03 21:23:50,839][client][DEBUG ]  * updates: mirror.dal.nexril.net
[2021-11-03 21:24:05,593][client][DEBUG ] Resolving Dependencies
[2021-11-03 21:24:05,593][client][DEBUG ] --> Running transaction check
[2021-11-03 21:24:05,593][client][DEBUG ] ---> Package yum-plugin-priorities.noarch 0:1.1.31-54.el7_8 will be installed
[2021-11-03 21:24:05,807][client][DEBUG ] --> Finished Dependency Resolution
[2021-11-03 21:24:05,871][client][DEBUG ] 
[2021-11-03 21:24:05,871][client][DEBUG ] Dependencies Resolved
[2021-11-03 21:24:05,871][client][DEBUG ] 
[2021-11-03 21:24:05,871][client][DEBUG ] ================================================================================
[2021-11-03 21:24:05,871][client][DEBUG ]  Package                    Arch        Version                 Repository
[2021-11-03 21:24:05,871][client][DEBUG ]                                                                            Size
[2021-11-03 21:24:05,871][client][DEBUG ] ================================================================================
[2021-11-03 21:24:05,871][client][DEBUG ] Installing:
[2021-11-03 21:24:05,871][client][DEBUG ]  yum-plugin-priorities      noarch      1.1.31-54.el7_8         base       29 k
[2021-11-03 21:24:05,871][client][DEBUG ] 
[2021-11-03 21:24:05,872][client][DEBUG ] Transaction Summary
[2021-11-03 21:24:05,872][client][DEBUG ] ================================================================================
[2021-11-03 21:24:05,872][client][DEBUG ] Install  1 Package
[2021-11-03 21:24:05,872][client][DEBUG ] 
[2021-11-03 21:24:05,872][client][DEBUG ] Total download size: 29 k
[2021-11-03 21:24:05,872][client][DEBUG ] Installed size: 28 k
[2021-11-03 21:24:05,872][client][DEBUG ] Downloading packages:
[2021-11-03 21:24:11,553][client][DEBUG ] Running transaction check
[2021-11-03 21:24:11,554][client][DEBUG ] Running transaction test
[2021-11-03 21:24:11,618][client][DEBUG ] Transaction test succeeded
[2021-11-03 21:24:11,618][client][DEBUG ] Running transaction
[2021-11-03 21:24:12,234][client][DEBUG ]   Installing : yum-plugin-priorities-1.1.31-54.el7_8.noarch                 1/1 
[2021-11-03 21:24:12,599][client][DEBUG ]   Verifying  : yum-plugin-priorities-1.1.31-54.el7_8.noarch                 1/1 
[2021-11-03 21:24:12,599][client][DEBUG ] 
[2021-11-03 21:24:12,599][client][DEBUG ] Installed:
[2021-11-03 21:24:12,599][client][DEBUG ]   yum-plugin-priorities.noarch 0:1.1.31-54.el7_8                                
[2021-11-03 21:24:12,599][client][DEBUG ] 
[2021-11-03 21:24:12,599][client][DEBUG ] Complete!
[2021-11-03 21:24:12,663][client][DEBUG ] Configure Yum priorities to include obsoletes
[2021-11-03 21:24:12,665][client][WARNING] check_obsoletes has been enabled for Yum priorities plugin
[2021-11-03 21:24:12,668][client][INFO  ] Running command: sudo rpm --import https://download.ceph.com/keys/release.asc
[2021-11-03 21:24:18,653][client][INFO  ] Running command: sudo yum remove -y ceph-release
[2021-11-03 21:24:18,770][client][DEBUG ] Loaded plugins: fastestmirror, priorities
[2021-11-03 21:24:18,884][client][WARNING] No Match for argument: ceph-release
[2021-11-03 21:24:18,916][client][DEBUG ] No Packages marked for removal
[2021-11-03 21:24:18,925][client][INFO  ] Running command: sudo yum install -y https://download.ceph.com/rpm-jewel/el7/noarch/ceph-release-1-0.el7.noarch.rpm
[2021-11-03 21:24:19,041][client][DEBUG ] Loaded plugins: fastestmirror, priorities
[2021-11-03 21:24:24,821][client][DEBUG ] Examining /var/tmp/yum-root-ecRQoN/ceph-release-1-0.el7.noarch.rpm: ceph-release-1-1.el7.noarch
[2021-11-03 21:24:24,821][client][DEBUG ] Marking /var/tmp/yum-root-ecRQoN/ceph-release-1-0.el7.noarch.rpm to be installed
[2021-11-03 21:24:24,821][client][DEBUG ] Resolving Dependencies
[2021-11-03 21:24:24,821][client][DEBUG ] --> Running transaction check
[2021-11-03 21:24:24,821][client][DEBUG ] ---> Package ceph-release.noarch 0:1-1.el7 will be installed
[2021-11-03 21:24:25,035][client][DEBUG ] --> Finished Dependency Resolution
[2021-11-03 21:24:25,100][client][DEBUG ] 
[2021-11-03 21:24:25,100][client][DEBUG ] Dependencies Resolved
[2021-11-03 21:24:25,100][client][DEBUG ] 
[2021-11-03 21:24:25,100][client][DEBUG ] ================================================================================
[2021-11-03 21:24:25,100][client][DEBUG ]  Package          Arch       Version     Repository                        Size
[2021-11-03 21:24:25,100][client][DEBUG ] ================================================================================
[2021-11-03 21:24:25,100][client][DEBUG ] Installing:
[2021-11-03 21:24:25,100][client][DEBUG ]  ceph-release     noarch     1-1.el7     /ceph-release-1-0.el7.noarch     535  
[2021-11-03 21:24:25,100][client][DEBUG ] 
[2021-11-03 21:24:25,100][client][DEBUG ] Transaction Summary
[2021-11-03 21:24:25,100][client][DEBUG ] ================================================================================
[2021-11-03 21:24:25,100][client][DEBUG ] Install  1 Package
[2021-11-03 21:24:25,100][client][DEBUG ] 
[2021-11-03 21:24:25,100][client][DEBUG ] Total size: 535  
[2021-11-03 21:24:25,101][client][DEBUG ] Installed size: 535  
[2021-11-03 21:24:25,101][client][DEBUG ] Downloading packages:
[2021-11-03 21:24:25,101][client][DEBUG ] Running transaction check
[2021-11-03 21:24:25,101][client][DEBUG ] Running transaction test
[2021-11-03 21:24:25,101][client][DEBUG ] Transaction test succeeded
[2021-11-03 21:24:25,101][client][DEBUG ] Running transaction
[2021-11-03 21:24:26,419][client][DEBUG ]   Installing : ceph-release-1-1.el7.noarch                                  1/1 
[2021-11-03 21:24:26,834][client][DEBUG ]   Verifying  : ceph-release-1-1.el7.noarch                                  1/1 
[2021-11-03 21:24:26,834][client][DEBUG ] 
[2021-11-03 21:24:26,834][client][DEBUG ] Installed:
[2021-11-03 21:24:26,834][client][DEBUG ]   ceph-release.noarch 0:1-1.el7                                                 
[2021-11-03 21:24:26,834][client][DEBUG ] 
[2021-11-03 21:24:26,834][client][DEBUG ] Complete!
[2021-11-03 21:24:26,850][client][WARNING] ensuring that /etc/yum.repos.d/ceph.repo contains a high priority
[2021-11-03 21:24:26,852][client][WARNING] altered ceph.repo priorities to contain: priority=1
[2021-11-03 21:24:26,854][client][INFO  ] Running command: sudo yum -y install ceph ceph-radosgw
[2021-11-03 21:24:26,971][client][DEBUG ] Loaded plugins: fastestmirror, priorities
[2021-11-03 21:24:27,003][client][DEBUG ] Loading mirror speeds from cached hostfile
[2021-11-03 21:24:42,211][client][DEBUG ]  * base: mirror.hackingand.coffee
[2021-11-03 21:24:42,211][client][DEBUG ]  * epel: pubmirror1.math.uh.edu
[2021-11-03 21:24:42,211][client][DEBUG ]  * extras: mirror.grid.uchicago.edu
[2021-11-03 21:24:42,211][client][DEBUG ]  * updates: mirror.dal.nexril.net
[2021-11-03 21:24:54,614][client][DEBUG ] 12 packages excluded due to repository priority protections
[2021-11-03 21:24:55,430][client][DEBUG ] Resolving Dependencies
[2021-11-03 21:24:55,430][client][DEBUG ] --> Running transaction check
[2021-11-03 21:24:55,430][client][DEBUG ] ---> Package ceph.x86_64 2:10.2.11-0.el7 will be installed
[2021-11-03 21:24:55,430][client][DEBUG ] --> Processing Dependency: ceph-osd = 2:10.2.11-0.el7 for package: 2:ceph-10.2.11-0.el7.x86_64
[2021-11-03 21:24:55,430][client][DEBUG ] --> Processing Dependency: ceph-mds = 2:10.2.11-0.el7 for package: 2:ceph-10.2.11-0.el7.x86_64
[2021-11-03 21:24:55,430][client][DEBUG ] --> Processing Dependency: ceph-mon = 2:10.2.11-0.el7 for package: 2:ceph-10.2.11-0.el7.x86_64
[2021-11-03 21:24:55,430][client][DEBUG ] ---> Package ceph-radosgw.x86_64 2:10.2.11-0.el7 will be installed
[2021-11-03 21:24:55,430][client][DEBUG ] --> Processing Dependency: librgw2 = 2:10.2.11-0.el7 for package: 2:ceph-radosgw-10.2.11-0.el7.x86_64
[2021-11-03 21:24:55,437][client][DEBUG ] --> Processing Dependency: librados2 = 2:10.2.11-0.el7 for package: 2:ceph-radosgw-10.2.11-0.el7.x86_64
[2021-11-03 21:24:55,438][client][DEBUG ] --> Processing Dependency: ceph-selinux = 2:10.2.11-0.el7 for package: 2:ceph-radosgw-10.2.11-0.el7.x86_64
[2021-11-03 21:24:55,438][client][DEBUG ] --> Processing Dependency: ceph-common = 2:10.2.11-0.el7 for package: 2:ceph-radosgw-10.2.11-0.el7.x86_64
[2021-11-03 21:24:55,438][client][DEBUG ] --> Processing Dependency: python-flask for package: 2:ceph-radosgw-10.2.11-0.el7.x86_64
[2021-11-03 21:24:55,438][client][DEBUG ] --> Processing Dependency: mailcap for package: 2:ceph-radosgw-10.2.11-0.el7.x86_64
[2021-11-03 21:24:55,438][client][DEBUG ] --> Processing Dependency: libfcgi.so.0()(64bit) for package: 2:ceph-radosgw-10.2.11-0.el7.x86_64
[2021-11-03 21:24:55,438][client][DEBUG ] --> Processing Dependency: libtcmalloc.so.4()(64bit) for package: 2:ceph-radosgw-10.2.11-0.el7.x86_64
[2021-11-03 21:24:55,454][client][DEBUG ] --> Processing Dependency: librados.so.2()(64bit) for package: 2:ceph-radosgw-10.2.11-0.el7.x86_64
[2021-11-03 21:24:55,454][client][DEBUG ] --> Processing Dependency: libboost_system-mt.so.1.53.0()(64bit) for package: 2:ceph-radosgw-10.2.11-0.el7.x86_64
[2021-11-03 21:24:55,454][client][DEBUG ] --> Processing Dependency: librgw.so.2()(64bit) for package: 2:ceph-radosgw-10.2.11-0.el7.x86_64
[2021-11-03 21:24:55,454][client][DEBUG ] --> Running transaction check
[2021-11-03 21:24:55,454][client][DEBUG ] ---> Package boost-system.x86_64 0:1.53.0-28.el7 will be installed
[2021-11-03 21:24:55,454][client][DEBUG ] ---> Package ceph-common.x86_64 2:10.2.11-0.el7 will be installed
[2021-11-03 21:24:55,462][client][DEBUG ] --> Processing Dependency: python-rbd = 2:10.2.11-0.el7 for package: 2:ceph-common-10.2.11-0.el7.x86_64
[2021-11-03 21:24:55,462][client][DEBUG ] --> Processing Dependency: python-cephfs = 2:10.2.11-0.el7 for package: 2:ceph-common-10.2.11-0.el7.x86_64
[2021-11-03 21:24:55,462][client][DEBUG ] --> Processing Dependency: libcephfs1 = 2:10.2.11-0.el7 for package: 2:ceph-common-10.2.11-0.el7.x86_64
[2021-11-03 21:24:55,462][client][DEBUG ] --> Processing Dependency: librbd1 = 2:10.2.11-0.el7 for package: 2:ceph-common-10.2.11-0.el7.x86_64
[2021-11-03 21:24:55,477][client][DEBUG ] --> Processing Dependency: python-rados = 2:10.2.11-0.el7 for package: 2:ceph-common-10.2.11-0.el7.x86_64
[2021-11-03 21:24:55,477][client][DEBUG ] --> Processing Dependency: python-requests for package: 2:ceph-common-10.2.11-0.el7.x86_64
[2021-11-03 21:24:55,477][client][DEBUG ] --> Processing Dependency: librbd.so.1()(64bit) for package: 2:ceph-common-10.2.11-0.el7.x86_64
[2021-11-03 21:24:55,478][client][DEBUG ] --> Processing Dependency: libboost_thread-mt.so.1.53.0()(64bit) for package: 2:ceph-common-10.2.11-0.el7.x86_64
[2021-11-03 21:24:55,478][client][DEBUG ] --> Processing Dependency: libboost_program_options-mt.so.1.53.0()(64bit) for package: 2:ceph-common-10.2.11-0.el7.x86_64
[2021-11-03 21:24:55,478][client][DEBUG ] --> Processing Dependency: libboost_regex-mt.so.1.53.0()(64bit) for package: 2:ceph-common-10.2.11-0.el7.x86_64
[2021-11-03 21:24:55,478][client][DEBUG ] --> Processing Dependency: libradosstriper.so.1()(64bit) for package: 2:ceph-common-10.2.11-0.el7.x86_64
[2021-11-03 21:24:55,481][client][DEBUG ] --> Processing Dependency: libbabeltrace.so.1()(64bit) for package: 2:ceph-common-10.2.11-0.el7.x86_64
[2021-11-03 21:24:55,481][client][DEBUG ] --> Processing Dependency: libbabeltrace-ctf.so.1()(64bit) for package: 2:ceph-common-10.2.11-0.el7.x86_64
[2021-11-03 21:24:55,481][client][DEBUG ] --> Processing Dependency: libboost_iostreams-mt.so.1.53.0()(64bit) for package: 2:ceph-common-10.2.11-0.el7.x86_64
[2021-11-03 21:24:55,481][client][DEBUG ] ---> Package ceph-mds.x86_64 2:10.2.11-0.el7 will be installed
[2021-11-03 21:24:55,481][client][DEBUG ] --> Processing Dependency: ceph-base = 2:10.2.11-0.el7 for package: 2:ceph-mds-10.2.11-0.el7.x86_64
[2021-11-03 21:24:55,497][client][DEBUG ] ---> Package ceph-mon.x86_64 2:10.2.11-0.el7 will be installed
[2021-11-03 21:24:55,497][client][DEBUG ] --> Processing Dependency: libleveldb.so.1()(64bit) for package: 2:ceph-mon-10.2.11-0.el7.x86_64
[2021-11-03 21:24:55,497][client][DEBUG ] --> Processing Dependency: libboost_random-mt.so.1.53.0()(64bit) for package: 2:ceph-mon-10.2.11-0.el7.x86_64
[2021-11-03 21:24:55,497][client][DEBUG ] ---> Package ceph-osd.x86_64 2:10.2.11-0.el7 will be installed
[2021-11-03 21:24:55,497][client][DEBUG ] --> Processing Dependency: libfuse.so.2(FUSE_2.6)(64bit) for package: 2:ceph-osd-10.2.11-0.el7.x86_64
[2021-11-03 21:24:55,497][client][DEBUG ] --> Processing Dependency: gdisk for package: 2:ceph-osd-10.2.11-0.el7.x86_64
[2021-11-03 21:24:55,497][client][DEBUG ] --> Processing Dependency: libfuse.so.2(FUSE_2.8)(64bit) for package: 2:ceph-osd-10.2.11-0.el7.x86_64
[2021-11-03 21:24:55,497][client][DEBUG ] --> Processing Dependency: libfuse.so.2(FUSE_2.2)(64bit) for package: 2:ceph-osd-10.2.11-0.el7.x86_64
[2021-11-03 21:24:55,497][client][DEBUG ] --> Processing Dependency: libfuse.so.2(FUSE_2.5)(64bit) for package: 2:ceph-osd-10.2.11-0.el7.x86_64
[2021-11-03 21:24:55,497][client][DEBUG ] --> Processing Dependency: libfuse.so.2()(64bit) for package: 2:ceph-osd-10.2.11-0.el7.x86_64
[2021-11-03 21:24:55,497][client][DEBUG ] ---> Package ceph-selinux.x86_64 2:10.2.11-0.el7 will be installed
[2021-11-03 21:24:55,505][client][DEBUG ] ---> Package fcgi.x86_64 0:2.4.0-25.el7 will be installed
[2021-11-03 21:24:55,505][client][DEBUG ] ---> Package gperftools-libs.x86_64 0:2.6.1-1.el7 will be installed
[2021-11-03 21:24:55,505][client][DEBUG ] ---> Package librados2.x86_64 2:10.2.11-0.el7 will be installed
[2021-11-03 21:24:55,505][client][DEBUG ] --> Processing Dependency: liblttng-ust.so.0()(64bit) for package: 2:librados2-10.2.11-0.el7.x86_64
[2021-11-03 21:24:55,505][client][DEBUG ] ---> Package librgw2.x86_64 2:10.2.11-0.el7 will be installed
[2021-11-03 21:24:55,521][client][DEBUG ] ---> Package mailcap.noarch 0:2.1.41-2.el7 will be installed
[2021-11-03 21:24:55,521][client][DEBUG ] ---> Package python-flask.noarch 1:0.10.1-5.el7_7 will be installed
[2021-11-03 21:24:55,521][client][DEBUG ] --> Processing Dependency: python-werkzeug for package: 1:python-flask-0.10.1-5.el7_7.noarch
[2021-11-03 21:24:55,521][client][DEBUG ] --> Processing Dependency: python-jinja2 for package: 1:python-flask-0.10.1-5.el7_7.noarch
[2021-11-03 21:24:55,521][client][DEBUG ] --> Processing Dependency: python-itsdangerous for package: 1:python-flask-0.10.1-5.el7_7.noarch
[2021-11-03 21:24:55,521][client][DEBUG ] --> Running transaction check
[2021-11-03 21:24:55,521][client][DEBUG ] ---> Package boost-iostreams.x86_64 0:1.53.0-28.el7 will be installed
[2021-11-03 21:24:55,521][client][DEBUG ] ---> Package boost-program-options.x86_64 0:1.53.0-28.el7 will be installed
[2021-11-03 21:24:55,521][client][DEBUG ] ---> Package boost-random.x86_64 0:1.53.0-28.el7 will be installed
[2021-11-03 21:24:55,521][client][DEBUG ] ---> Package boost-regex.x86_64 0:1.53.0-28.el7 will be installed
[2021-11-03 21:24:55,521][client][DEBUG ] --> Processing Dependency: libicuuc.so.50()(64bit) for package: boost-regex-1.53.0-28.el7.x86_64
[2021-11-03 21:24:55,521][client][DEBUG ] --> Processing Dependency: libicui18n.so.50()(64bit) for package: boost-regex-1.53.0-28.el7.x86_64
[2021-11-03 21:24:55,521][client][DEBUG ] --> Processing Dependency: libicudata.so.50()(64bit) for package: boost-regex-1.53.0-28.el7.x86_64
[2021-11-03 21:24:55,521][client][DEBUG ] ---> Package boost-thread.x86_64 0:1.53.0-28.el7 will be installed
[2021-11-03 21:24:55,521][client][DEBUG ] ---> Package ceph-base.x86_64 2:10.2.11-0.el7 will be installed
[2021-11-03 21:24:55,529][client][DEBUG ] --> Processing Dependency: python-setuptools for package: 2:ceph-base-10.2.11-0.el7.x86_64
[2021-11-03 21:24:55,529][client][DEBUG ] --> Processing Dependency: psmisc for package: 2:ceph-base-10.2.11-0.el7.x86_64
[2021-11-03 21:24:55,529][client][DEBUG ] --> Processing Dependency: cryptsetup for package: 2:ceph-base-10.2.11-0.el7.x86_64
[2021-11-03 21:24:55,529][client][DEBUG ] --> Processing Dependency: hdparm for package: 2:ceph-base-10.2.11-0.el7.x86_64
[2021-11-03 21:24:55,529][client][DEBUG ] ---> Package fuse-libs.x86_64 0:2.9.2-11.el7 will be installed
[2021-11-03 21:24:55,529][client][DEBUG ] ---> Package gdisk.x86_64 0:0.8.10-3.el7 will be installed
[2021-11-03 21:24:55,529][client][DEBUG ] ---> Package leveldb.x86_64 0:1.12.0-11.el7 will be installed
[2021-11-03 21:24:55,529][client][DEBUG ] ---> Package libbabeltrace.x86_64 0:1.2.4-3.el7 will be installed
[2021-11-03 21:24:55,529][client][DEBUG ] ---> Package libcephfs1.x86_64 2:10.2.11-0.el7 will be installed
[2021-11-03 21:24:55,529][client][DEBUG ] ---> Package libradosstriper1.x86_64 2:10.2.11-0.el7 will be installed
[2021-11-03 21:24:55,529][client][DEBUG ] ---> Package librbd1.x86_64 2:10.2.11-0.el7 will be installed
[2021-11-03 21:24:55,545][client][DEBUG ] ---> Package lttng-ust.x86_64 0:2.4.1-4.el7 will be installed
[2021-11-03 21:24:55,545][client][DEBUG ] --> Processing Dependency: liburcu-bp.so.1()(64bit) for package: lttng-ust-2.4.1-4.el7.x86_64
[2021-11-03 21:24:55,545][client][DEBUG ] --> Processing Dependency: liburcu-cds.so.1()(64bit) for package: lttng-ust-2.4.1-4.el7.x86_64
[2021-11-03 21:24:55,545][client][DEBUG ] ---> Package python-cephfs.x86_64 2:10.2.11-0.el7 will be installed
[2021-11-03 21:24:55,545][client][DEBUG ] ---> Package python-itsdangerous.noarch 0:0.23-2.el7 will be installed
[2021-11-03 21:24:55,545][client][DEBUG ] ---> Package python-jinja2.noarch 0:2.7.2-4.el7 will be installed
[2021-11-03 21:24:55,545][client][DEBUG ] --> Processing Dependency: python-babel >= 0.8 for package: python-jinja2-2.7.2-4.el7.noarch
[2021-11-03 21:24:55,545][client][DEBUG ] --> Processing Dependency: python-markupsafe for package: python-jinja2-2.7.2-4.el7.noarch
[2021-11-03 21:24:55,545][client][DEBUG ] ---> Package python-rados.x86_64 2:10.2.11-0.el7 will be installed
[2021-11-03 21:24:55,545][client][DEBUG ] ---> Package python-rbd.x86_64 2:10.2.11-0.el7 will be installed
[2021-11-03 21:24:55,545][client][DEBUG ] ---> Package python-requests.noarch 0:2.6.0-10.el7 will be installed
[2021-11-03 21:24:55,545][client][DEBUG ] --> Processing Dependency: python-urllib3 >= 1.10.2-1 for package: python-requests-2.6.0-10.el7.noarch
[2021-11-03 21:24:55,546][client][DEBUG ] --> Processing Dependency: python-chardet >= 2.2.1-1 for package: python-requests-2.6.0-10.el7.noarch
[2021-11-03 21:24:55,546][client][DEBUG ] ---> Package python-werkzeug.noarch 0:0.9.1-2.el7 will be installed
[2021-11-03 21:24:55,546][client][DEBUG ] --> Running transaction check
[2021-11-03 21:24:55,546][client][DEBUG ] ---> Package cryptsetup.x86_64 0:2.0.3-6.el7 will be installed
[2021-11-03 21:24:55,546][client][DEBUG ] ---> Package hdparm.x86_64 0:9.43-5.el7 will be installed
[2021-11-03 21:24:55,546][client][DEBUG ] ---> Package libicu.x86_64 0:50.2-4.el7_7 will be installed
[2021-11-03 21:24:55,546][client][DEBUG ] ---> Package psmisc.x86_64 0:22.20-17.el7 will be installed
[2021-11-03 21:24:55,553][client][DEBUG ] ---> Package python-babel.noarch 0:0.9.6-8.el7 will be installed
[2021-11-03 21:24:55,553][client][DEBUG ] ---> Package python-chardet.noarch 0:2.2.1-3.el7 will be installed
[2021-11-03 21:24:55,553][client][DEBUG ] ---> Package python-markupsafe.x86_64 0:0.11-10.el7 will be installed
[2021-11-03 21:24:55,553][client][DEBUG ] ---> Package python-setuptools.noarch 0:0.9.8-7.el7 will be installed
[2021-11-03 21:24:55,554][client][DEBUG ] --> Processing Dependency: python-backports-ssl_match_hostname for package: python-setuptools-0.9.8-7.el7.noarch
[2021-11-03 21:24:55,554][client][DEBUG ] ---> Package python-urllib3.noarch 0:1.10.2-7.el7 will be installed
[2021-11-03 21:24:55,554][client][DEBUG ] --> Processing Dependency: python-six for package: python-urllib3-1.10.2-7.el7.noarch
[2021-11-03 21:24:55,554][client][DEBUG ] --> Processing Dependency: python-ipaddress for package: python-urllib3-1.10.2-7.el7.noarch
[2021-11-03 21:24:55,554][client][DEBUG ] ---> Package userspace-rcu.x86_64 0:0.7.16-1.el7 will be installed
[2021-11-03 21:24:55,554][client][DEBUG ] --> Running transaction check
[2021-11-03 21:24:55,554][client][DEBUG ] ---> Package python-backports-ssl_match_hostname.noarch 0:3.5.0.1-1.el7 will be installed
[2021-11-03 21:24:55,554][client][DEBUG ] --> Processing Dependency: python-backports for package: python-backports-ssl_match_hostname-3.5.0.1-1.el7.noarch
[2021-11-03 21:24:55,554][client][DEBUG ] ---> Package python-ipaddress.noarch 0:1.0.16-2.el7 will be installed
[2021-11-03 21:24:55,554][client][DEBUG ] ---> Package python-six.noarch 0:1.9.0-2.el7 will be installed
[2021-11-03 21:24:55,554][client][DEBUG ] --> Running transaction check
[2021-11-03 21:24:55,554][client][DEBUG ] ---> Package python-backports.x86_64 0:1.0-8.el7 will be installed
[2021-11-03 21:24:55,768][client][DEBUG ] --> Finished Dependency Resolution
[2021-11-03 21:24:55,882][client][DEBUG ] 
[2021-11-03 21:24:55,882][client][DEBUG ] Dependencies Resolved
[2021-11-03 21:24:55,882][client][DEBUG ] 
[2021-11-03 21:24:55,882][client][DEBUG ] ================================================================================
[2021-11-03 21:24:55,882][client][DEBUG ]  Package                              Arch    Version             Repository
[2021-11-03 21:24:55,882][client][DEBUG ]                                                                            Size
[2021-11-03 21:24:55,883][client][DEBUG ] ================================================================================
[2021-11-03 21:24:55,883][client][DEBUG ] Installing:
[2021-11-03 21:24:55,883][client][DEBUG ]  ceph                                 x86_64  2:10.2.11-0.el7     Ceph    3.0 k
[2021-11-03 21:24:55,883][client][DEBUG ]  ceph-radosgw                         x86_64  2:10.2.11-0.el7     Ceph    267 k
[2021-11-03 21:24:55,883][client][DEBUG ] Installing for dependencies:
[2021-11-03 21:24:55,883][client][DEBUG ]  boost-iostreams                      x86_64  1.53.0-28.el7       base     61 k
[2021-11-03 21:24:55,883][client][DEBUG ]  boost-program-options                x86_64  1.53.0-28.el7       base    156 k
[2021-11-03 21:24:55,883][client][DEBUG ]  boost-random                         x86_64  1.53.0-28.el7       base     39 k
[2021-11-03 21:24:55,883][client][DEBUG ]  boost-regex                          x86_64  1.53.0-28.el7       base    296 k
[2021-11-03 21:24:55,883][client][DEBUG ]  boost-system                         x86_64  1.53.0-28.el7       base     40 k
[2021-11-03 21:24:55,883][client][DEBUG ]  boost-thread                         x86_64  1.53.0-28.el7       base     58 k
[2021-11-03 21:24:55,883][client][DEBUG ]  ceph-base                            x86_64  2:10.2.11-0.el7     Ceph    4.2 M
[2021-11-03 21:24:55,883][client][DEBUG ]  ceph-common                          x86_64  2:10.2.11-0.el7     Ceph     17 M
[2021-11-03 21:24:55,883][client][DEBUG ]  ceph-mds                             x86_64  2:10.2.11-0.el7     Ceph    2.8 M
[2021-11-03 21:24:55,883][client][DEBUG ]  ceph-mon                             x86_64  2:10.2.11-0.el7     Ceph    3.1 M
[2021-11-03 21:24:55,883][client][DEBUG ]  ceph-osd                             x86_64  2:10.2.11-0.el7     Ceph    9.5 M
[2021-11-03 21:24:55,883][client][DEBUG ]  ceph-selinux                         x86_64  2:10.2.11-0.el7     Ceph     20 k
[2021-11-03 21:24:55,883][client][DEBUG ]  cryptsetup                           x86_64  2.0.3-6.el7         base    154 k
[2021-11-03 21:24:55,883][client][DEBUG ]  fcgi                                 x86_64  2.4.0-25.el7        epel     47 k
[2021-11-03 21:24:55,884][client][DEBUG ]  fuse-libs                            x86_64  2.9.2-11.el7        base     93 k
[2021-11-03 21:24:55,884][client][DEBUG ]  gdisk                                x86_64  0.8.10-3.el7        base    190 k
[2021-11-03 21:24:55,884][client][DEBUG ]  gperftools-libs                      x86_64  2.6.1-1.el7         base    272 k
[2021-11-03 21:24:55,884][client][DEBUG ]  hdparm                               x86_64  9.43-5.el7          base     83 k
[2021-11-03 21:24:55,884][client][DEBUG ]  leveldb                              x86_64  1.12.0-11.el7       epel    161 k
[2021-11-03 21:24:55,884][client][DEBUG ]  libbabeltrace                        x86_64  1.2.4-3.el7         epel    147 k
[2021-11-03 21:24:55,884][client][DEBUG ]  libcephfs1                           x86_64  2:10.2.11-0.el7     Ceph    1.9 M
[2021-11-03 21:24:55,884][client][DEBUG ]  libicu                               x86_64  50.2-4.el7_7        base    6.9 M
[2021-11-03 21:24:55,884][client][DEBUG ]  librados2                            x86_64  2:10.2.11-0.el7     Ceph    1.9 M
[2021-11-03 21:24:55,884][client][DEBUG ]  libradosstriper1                     x86_64  2:10.2.11-0.el7     Ceph    1.8 M
[2021-11-03 21:24:55,884][client][DEBUG ]  librbd1                              x86_64  2:10.2.11-0.el7     Ceph    2.5 M
[2021-11-03 21:24:55,884][client][DEBUG ]  librgw2                              x86_64  2:10.2.11-0.el7     Ceph    3.0 M
[2021-11-03 21:24:55,884][client][DEBUG ]  lttng-ust                            x86_64  2.4.1-4.el7         epel    176 k
[2021-11-03 21:24:55,884][client][DEBUG ]  mailcap                              noarch  2.1.41-2.el7        base     31 k
[2021-11-03 21:24:55,884][client][DEBUG ]  psmisc                               x86_64  22.20-17.el7        base    141 k
[2021-11-03 21:24:55,884][client][DEBUG ]  python-babel                         noarch  0.9.6-8.el7         base    1.4 M
[2021-11-03 21:24:55,884][client][DEBUG ]  python-backports                     x86_64  1.0-8.el7           base    5.8 k
[2021-11-03 21:24:55,884][client][DEBUG ]  python-backports-ssl_match_hostname  noarch  3.5.0.1-1.el7       base     13 k
[2021-11-03 21:24:55,885][client][DEBUG ]  python-cephfs                        x86_64  2:10.2.11-0.el7     Ceph     78 k
[2021-11-03 21:24:55,885][client][DEBUG ]  python-chardet                       noarch  2.2.1-3.el7         base    227 k
[2021-11-03 21:24:55,885][client][DEBUG ]  python-flask                         noarch  1:0.10.1-5.el7_7    extras  205 k
[2021-11-03 21:24:55,885][client][DEBUG ]  python-ipaddress                     noarch  1.0.16-2.el7        base     34 k
[2021-11-03 21:24:55,885][client][DEBUG ]  python-itsdangerous                  noarch  0.23-2.el7          extras   24 k
[2021-11-03 21:24:55,885][client][DEBUG ]  python-jinja2                        noarch  2.7.2-4.el7         base    519 k
[2021-11-03 21:24:55,885][client][DEBUG ]  python-markupsafe                    x86_64  0.11-10.el7         base     25 k
[2021-11-03 21:24:55,885][client][DEBUG ]  python-rados                         x86_64  2:10.2.11-0.el7     Ceph    149 k
[2021-11-03 21:24:55,885][client][DEBUG ]  python-rbd                           x86_64  2:10.2.11-0.el7     Ceph     79 k
[2021-11-03 21:24:55,885][client][DEBUG ]  python-requests                      noarch  2.6.0-10.el7        base     95 k
[2021-11-03 21:24:55,885][client][DEBUG ]  python-setuptools                    noarch  0.9.8-7.el7         base    397 k
[2021-11-03 21:24:55,885][client][DEBUG ]  python-six                           noarch  1.9.0-2.el7         base     29 k
[2021-11-03 21:24:55,885][client][DEBUG ]  python-urllib3                       noarch  1.10.2-7.el7        base    103 k
[2021-11-03 21:24:55,885][client][DEBUG ]  python-werkzeug                      noarch  0.9.1-2.el7         extras  562 k
[2021-11-03 21:24:55,885][client][DEBUG ]  userspace-rcu                        x86_64  0.7.16-1.el7        epel     73 k
[2021-11-03 21:24:55,885][client][DEBUG ] 
[2021-11-03 21:24:55,885][client][DEBUG ] Transaction Summary
[2021-11-03 21:24:55,885][client][DEBUG ] ================================================================================
[2021-11-03 21:24:55,885][client][DEBUG ] Install  2 Packages (+47 Dependent packages)
[2021-11-03 21:24:55,886][client][DEBUG ] 
[2021-11-03 21:24:55,886][client][DEBUG ] Total download size: 60 M
[2021-11-03 21:24:55,886][client][DEBUG ] Installed size: 225 M
[2021-11-03 21:24:55,886][client][DEBUG ] Downloading packages:
[2021-11-03 21:25:14,904][client][DEBUG ] Public key for fcgi-2.4.0-25.el7.x86_64.rpm is not installed
[2021-11-03 21:25:14,905][client][WARNING] warning: /var/cache/yum/x86_64/7/epel/packages/fcgi-2.4.0-25.el7.x86_64.rpm: Header V3 RSA/SHA256 Signature, key ID 352c64e5: NOKEY
[2021-11-03 21:25:23,842][client][DEBUG ] --------------------------------------------------------------------------------
[2021-11-03 21:25:23,843][client][WARNING] Importing GPG key 0x352C64E5:
[2021-11-03 21:25:23,843][client][DEBUG ] Total                                              2.2 MB/s |  60 MB  00:27     
[2021-11-03 21:25:23,843][client][WARNING]  Userid     : "Fedora EPEL (7) <epel@fedoraproject.org>"
[2021-11-03 21:25:23,843][client][DEBUG ] Retrieving key from file:///etc/pki/rpm-gpg/RPM-GPG-KEY-EPEL-7
[2021-11-03 21:25:23,843][client][WARNING]  Fingerprint: 91e9 7d7c 4a5e 96f1 7f3e 888f 6a2f aea2 352c 64e5
[2021-11-03 21:25:23,843][client][WARNING]  Package    : epel-release-7-11.noarch (@extras)
[2021-11-03 21:25:23,843][client][WARNING]  From       : /etc/pki/rpm-gpg/RPM-GPG-KEY-EPEL-7
[2021-11-03 21:25:24,709][client][DEBUG ] Running transaction check
[2021-11-03 21:25:24,741][client][DEBUG ] Running transaction test
[2021-11-03 21:25:25,006][client][DEBUG ] Transaction test succeeded
[2021-11-03 21:25:25,006][client][DEBUG ] Running transaction
[2021-11-03 21:25:25,722][client][DEBUG ]   Installing : boost-system-1.53.0-28.el7.x86_64                           1/49 
[2021-11-03 21:25:26,137][client][DEBUG ]   Installing : boost-thread-1.53.0-28.el7.x86_64                           2/49 
[2021-11-03 21:25:26,551][client][DEBUG ]   Installing : boost-iostreams-1.53.0-28.el7.x86_64                        3/49 
[2021-11-03 21:25:27,017][client][DEBUG ]   Installing : boost-random-1.53.0-28.el7.x86_64                           4/49 
[2021-11-03 21:25:27,582][client][DEBUG ]   Installing : gperftools-libs-2.6.1-1.el7.x86_64                          5/49 
[2021-11-03 21:25:27,947][client][DEBUG ]   Installing : 2:libcephfs1-10.2.11-0.el7.x86_64                           6/49 
[2021-11-03 21:25:28,261][client][DEBUG ]   Installing : python-ipaddress-1.0.16-2.el7.noarch                        7/49 
[2021-11-03 21:25:28,676][client][DEBUG ]   Installing : leveldb-1.12.0-11.el7.x86_64                                8/49 
[2021-11-03 21:25:29,191][client][DEBUG ]   Installing : fcgi-2.4.0-25.el7.x86_64                                    9/49 
[2021-11-03 21:25:29,606][client][DEBUG ]   Installing : boost-program-options-1.53.0-28.el7.x86_64                 10/49 
[2021-11-03 21:25:30,272][client][DEBUG ]   Installing : gdisk-0.8.10-3.el7.x86_64                                  11/49 
[2021-11-03 21:25:30,937][client][DEBUG ]   Installing : python-babel-0.9.6-8.el7.noarch                            12/49 
[2021-11-03 21:25:31,302][client][DEBUG ]   Installing : python-werkzeug-0.9.1-2.el7.noarch                         13/49 
[2021-11-03 21:25:31,617][client][DEBUG ]   Installing : python-backports-1.0-8.el7.x86_64                          14/49 
[2021-11-03 21:25:32,333][client][DEBUG ]   Installing : python-backports-ssl_match_hostname-3.5.0.1-1.el7.noarch   15/49 
[2021-11-03 21:25:32,698][client][DEBUG ]   Installing : python-setuptools-0.9.8-7.el7.noarch                       16/49 
[2021-11-03 21:25:33,113][client][DEBUG ]   Installing : userspace-rcu-0.7.16-1.el7.x86_64                          17/49 
[2021-11-03 21:25:34,079][client][DEBUG ]   Installing : lttng-ust-2.4.1-4.el7.x86_64                               18/49 
[2021-11-03 21:25:34,745][client][DEBUG ]   Installing : 2:librados2-10.2.11-0.el7.x86_64                           19/49 
[2021-11-03 21:25:35,211][client][DEBUG ]   Installing : 2:librbd1-10.2.11-0.el7.x86_64                             20/49 
[2021-11-03 21:25:35,880][client][DEBUG ]   Installing : 2:python-rados-10.2.11-0.el7.x86_64                        21/49 
[2021-11-03 21:25:36,445][client][DEBUG ]   Installing : 2:librgw2-10.2.11-0.el7.x86_64                             22/49 
[2021-11-03 21:25:36,862][client][DEBUG ]   Installing : 2:python-rbd-10.2.11-0.el7.x86_64                          23/49 
[2021-11-03 21:25:37,478][client][DEBUG ]   Installing : 2:python-cephfs-10.2.11-0.el7.x86_64                       24/49 
[2021-11-03 21:25:38,946][client][DEBUG ]   Installing : 2:libradosstriper1-10.2.11-0.el7.x86_64                    25/49 
[2021-11-03 21:25:39,612][client][DEBUG ]   Installing : python-chardet-2.2.1-3.el7.noarch                          26/49 
[2021-11-03 21:25:39,977][client][DEBUG ]   Installing : cryptsetup-2.0.3-6.el7.x86_64                              27/49 
[2021-11-03 21:25:40,291][client][DEBUG ]   Installing : python-six-1.9.0-2.el7.noarch                              28/49 
[2021-11-03 21:25:40,656][client][DEBUG ]   Installing : python-urllib3-1.10.2-7.el7.noarch                         29/49 
[2021-11-03 21:25:41,673][client][DEBUG ]   Installing : python-requests-2.6.0-10.el7.noarch                        30/49 
[2021-11-03 21:25:42,188][client][DEBUG ]   Installing : libicu-50.2-4.el7_7.x86_64                                 31/49 
[2021-11-03 21:25:42,553][client][DEBUG ]   Installing : boost-regex-1.53.0-28.el7.x86_64                           32/49 
[2021-11-03 21:25:42,868][client][DEBUG ]   Installing : python-itsdangerous-0.23-2.el7.noarch                      33/49 
[2021-11-03 21:25:43,283][client][DEBUG ]   Installing : hdparm-9.43-5.el7.x86_64                                   34/49 
[2021-11-03 21:25:45,755][client][DEBUG ]   Installing : libbabeltrace-1.2.4-3.el7.x86_64                           35/49 
[2021-11-03 21:25:47,474][client][DEBUG ]   Installing : 2:ceph-common-10.2.11-0.el7.x86_64                         36/49 
[2021-11-03 21:25:49,443][client][DEBUG ]   Installing : psmisc-22.20-17.el7.x86_64                                 37/49 
[2021-11-03 21:25:50,009][client][DEBUG ]   Installing : 2:ceph-base-10.2.11-0.el7.x86_64                           38/49 
[2021-11-03 21:25:58,748][client][DEBUG ]   Installing : 2:ceph-selinux-10.2.11-0.el7.x86_64                        39/49 
[2021-11-03 21:25:59,364][client][DEBUG ]   Installing : 2:ceph-mds-10.2.11-0.el7.x86_64                            40/49 
[2021-11-03 21:25:59,679][client][DEBUG ]   Installing : mailcap-2.1.41-2.el7.noarch                                41/49 
[2021-11-03 21:26:00,244][client][DEBUG ]   Installing : python-markupsafe-0.11-10.el7.x86_64                       42/49 
[2021-11-03 21:26:00,709][client][DEBUG ]   Installing : python-jinja2-2.7.2-4.el7.noarch                           43/49 
[2021-11-03 21:26:01,426][client][DEBUG ]   Installing : 1:python-flask-0.10.1-5.el7_7.noarch                       44/49 
[2021-11-03 21:26:01,841][client][DEBUG ]   Installing : 2:ceph-mon-10.2.11-0.el7.x86_64                            45/49 
[2021-11-03 21:26:03,209][client][DEBUG ]   Installing : fuse-libs-2.9.2-11.el7.x86_64                              46/49 
[2021-11-03 21:26:05,379][client][DEBUG ]   Installing : 2:ceph-osd-10.2.11-0.el7.x86_64                            47/49 
[2021-11-03 21:26:05,645][client][DEBUG ]   Installing : 2:ceph-10.2.11-0.el7.x86_64                                48/49 
[2021-11-03 21:26:06,110][client][DEBUG ]   Installing : 2:ceph-radosgw-10.2.11-0.el7.x86_64                        49/49 
[2021-11-03 21:26:06,274][client][DEBUG ]   Verifying  : fuse-libs-2.9.2-11.el7.x86_64                               1/49 
[2021-11-03 21:26:06,389][client][DEBUG ]   Verifying  : python-backports-ssl_match_hostname-3.5.0.1-1.el7.noarch    2/49 
[2021-11-03 21:26:06,503][client][DEBUG ]   Verifying  : boost-thread-1.53.0-28.el7.x86_64                           3/49 
[2021-11-03 21:26:06,667][client][DEBUG ]   Verifying  : python-markupsafe-0.11-10.el7.x86_64                        4/49 
[2021-11-03 21:26:06,781][client][DEBUG ]   Verifying  : 2:ceph-common-10.2.11-0.el7.x86_64                          5/49 
[2021-11-03 21:26:06,895][client][DEBUG ]   Verifying  : mailcap-2.1.41-2.el7.noarch                                 6/49 
[2021-11-03 21:26:07,060][client][DEBUG ]   Verifying  : boost-random-1.53.0-28.el7.x86_64                           7/49 
[2021-11-03 21:26:07,174][client][DEBUG ]   Verifying  : psmisc-22.20-17.el7.x86_64                                  8/49 
[2021-11-03 21:26:07,288][client][DEBUG ]   Verifying  : 2:ceph-osd-10.2.11-0.el7.x86_64                             9/49 
[2021-11-03 21:26:07,453][client][DEBUG ]   Verifying  : python-setuptools-0.9.8-7.el7.noarch                       10/49 
[2021-11-03 21:26:07,567][client][DEBUG ]   Verifying  : python-urllib3-1.10.2-7.el7.noarch                         11/49 
[2021-11-03 21:26:07,732][client][DEBUG ]   Verifying  : libbabeltrace-1.2.4-3.el7.x86_64                           12/49 
[2021-11-03 21:26:07,846][client][DEBUG ]   Verifying  : hdparm-9.43-5.el7.x86_64                                   13/49 
[2021-11-03 21:26:07,961][client][DEBUG ]   Verifying  : boost-iostreams-1.53.0-28.el7.x86_64                       14/49 
[2021-11-03 21:26:08,125][client][DEBUG ]   Verifying  : 2:libcephfs1-10.2.11-0.el7.x86_64                          15/49 
[2021-11-03 21:26:08,339][client][DEBUG ]   Verifying  : 2:ceph-10.2.11-0.el7.x86_64                                16/49 
[2021-11-03 21:26:08,454][client][DEBUG ]   Verifying  : python-itsdangerous-0.23-2.el7.noarch                      17/49 
[2021-11-03 21:26:08,568][client][DEBUG ]   Verifying  : 2:libradosstriper1-10.2.11-0.el7.x86_64                    18/49 
[2021-11-03 21:26:08,682][client][DEBUG ]   Verifying  : python-jinja2-2.7.2-4.el7.noarch                           19/49 
[2021-11-03 21:26:08,796][client][DEBUG ]   Verifying  : libicu-50.2-4.el7_7.x86_64                                 20/49 
[2021-11-03 21:26:08,960][client][DEBUG ]   Verifying  : boost-regex-1.53.0-28.el7.x86_64                           21/49 
[2021-11-03 21:26:09,074][client][DEBUG ]   Verifying  : python-six-1.9.0-2.el7.noarch                              22/49 
[2021-11-03 21:26:09,188][client][DEBUG ]   Verifying  : cryptsetup-2.0.3-6.el7.x86_64                              23/49 
[2021-11-03 21:26:09,603][client][DEBUG ]   Verifying  : boost-program-options-1.53.0-28.el7.x86_64                 24/49 
[2021-11-03 21:26:09,867][client][DEBUG ]   Verifying  : 2:librados2-10.2.11-0.el7.x86_64                           25/49 
[2021-11-03 21:26:09,982][client][DEBUG ]   Verifying  : boost-system-1.53.0-28.el7.x86_64                          26/49 
[2021-11-03 21:26:10,096][client][DEBUG ]   Verifying  : fcgi-2.4.0-25.el7.x86_64                                   27/49 
[2021-11-03 21:26:10,210][client][DEBUG ]   Verifying  : python-chardet-2.2.1-3.el7.noarch                          28/49 
[2021-11-03 21:26:10,324][client][DEBUG ]   Verifying  : leveldb-1.12.0-11.el7.x86_64                               29/49 
[2021-11-03 21:26:10,438][client][DEBUG ]   Verifying  : lttng-ust-2.4.1-4.el7.x86_64                               30/49 
[2021-11-03 21:26:10,552][client][DEBUG ]   Verifying  : 2:ceph-radosgw-10.2.11-0.el7.x86_64                        31/49 
[2021-11-03 21:26:10,666][client][DEBUG ]   Verifying  : 2:librbd1-10.2.11-0.el7.x86_64                             32/49 
[2021-11-03 21:26:10,780][client][DEBUG ]   Verifying  : 2:python-rados-10.2.11-0.el7.x86_64                        33/49 
[2021-11-03 21:26:10,894][client][DEBUG ]   Verifying  : userspace-rcu-0.7.16-1.el7.x86_64                          34/49 
[2021-11-03 21:26:11,008][client][DEBUG ]   Verifying  : python-backports-1.0-8.el7.x86_64                          35/49 
[2021-11-03 21:26:11,122][client][DEBUG ]   Verifying  : 1:python-flask-0.10.1-5.el7_7.noarch                       36/49 
[2021-11-03 21:26:11,237][client][DEBUG ]   Verifying  : python-werkzeug-0.9.1-2.el7.noarch                         37/49 
[2021-11-03 21:26:11,351][client][DEBUG ]   Verifying  : python-babel-0.9.6-8.el7.noarch                            38/49 
[2021-11-03 21:26:11,465][client][DEBUG ]   Verifying  : 2:librgw2-10.2.11-0.el7.x86_64                             39/49 
[2021-11-03 21:26:11,629][client][DEBUG ]   Verifying  : 2:ceph-mds-10.2.11-0.el7.x86_64                            40/49 
[2021-11-03 21:26:11,693][client][DEBUG ]   Verifying  : python-requests-2.6.0-10.el7.noarch                        41/49 
[2021-11-03 21:26:11,857][client][DEBUG ]   Verifying  : 2:python-rbd-10.2.11-0.el7.x86_64                          42/49 
[2021-11-03 21:26:11,971][client][DEBUG ]   Verifying  : 2:ceph-selinux-10.2.11-0.el7.x86_64                        43/49 
[2021-11-03 21:26:12,085][client][DEBUG ]   Verifying  : gperftools-libs-2.6.1-1.el7.x86_64                         44/49 
[2021-11-03 21:26:12,199][client][DEBUG ]   Verifying  : 2:python-cephfs-10.2.11-0.el7.x86_64                       45/49 
[2021-11-03 21:26:12,313][client][DEBUG ]   Verifying  : python-ipaddress-1.0.16-2.el7.noarch                       46/49 
[2021-11-03 21:26:12,427][client][DEBUG ]   Verifying  : 2:ceph-mon-10.2.11-0.el7.x86_64                            47/49 
[2021-11-03 21:26:12,541][client][DEBUG ]   Verifying  : gdisk-0.8.10-3.el7.x86_64                                  48/49 
[2021-11-03 21:26:13,508][client][DEBUG ]   Verifying  : 2:ceph-base-10.2.11-0.el7.x86_64                           49/49 
[2021-11-03 21:26:13,508][client][DEBUG ] 
[2021-11-03 21:26:13,508][client][DEBUG ] Installed:
[2021-11-03 21:26:13,508][client][DEBUG ]   ceph.x86_64 2:10.2.11-0.el7        ceph-radosgw.x86_64 2:10.2.11-0.el7       
[2021-11-03 21:26:13,508][client][DEBUG ] 
[2021-11-03 21:26:13,508][client][DEBUG ] Dependency Installed:
[2021-11-03 21:26:13,508][client][DEBUG ]   boost-iostreams.x86_64 0:1.53.0-28.el7                                        
[2021-11-03 21:26:13,508][client][DEBUG ]   boost-program-options.x86_64 0:1.53.0-28.el7                                  
[2021-11-03 21:26:13,508][client][DEBUG ]   boost-random.x86_64 0:1.53.0-28.el7                                           
[2021-11-03 21:26:13,508][client][DEBUG ]   boost-regex.x86_64 0:1.53.0-28.el7                                            
[2021-11-03 21:26:13,508][client][DEBUG ]   boost-system.x86_64 0:1.53.0-28.el7                                           
[2021-11-03 21:26:13,508][client][DEBUG ]   boost-thread.x86_64 0:1.53.0-28.el7                                           
[2021-11-03 21:26:13,509][client][DEBUG ]   ceph-base.x86_64 2:10.2.11-0.el7                                              
[2021-11-03 21:26:13,509][client][DEBUG ]   ceph-common.x86_64 2:10.2.11-0.el7                                            
[2021-11-03 21:26:13,509][client][DEBUG ]   ceph-mds.x86_64 2:10.2.11-0.el7                                               
[2021-11-03 21:26:13,509][client][DEBUG ]   ceph-mon.x86_64 2:10.2.11-0.el7                                               
[2021-11-03 21:26:13,509][client][DEBUG ]   ceph-osd.x86_64 2:10.2.11-0.el7                                               
[2021-11-03 21:26:13,509][client][DEBUG ]   ceph-selinux.x86_64 2:10.2.11-0.el7                                           
[2021-11-03 21:26:13,509][client][DEBUG ]   cryptsetup.x86_64 0:2.0.3-6.el7                                               
[2021-11-03 21:26:13,509][client][DEBUG ]   fcgi.x86_64 0:2.4.0-25.el7                                                    
[2021-11-03 21:26:13,509][client][DEBUG ]   fuse-libs.x86_64 0:2.9.2-11.el7                                               
[2021-11-03 21:26:13,509][client][DEBUG ]   gdisk.x86_64 0:0.8.10-3.el7                                                   
[2021-11-03 21:26:13,509][client][DEBUG ]   gperftools-libs.x86_64 0:2.6.1-1.el7                                          
[2021-11-03 21:26:13,509][client][DEBUG ]   hdparm.x86_64 0:9.43-5.el7                                                    
[2021-11-03 21:26:13,509][client][DEBUG ]   leveldb.x86_64 0:1.12.0-11.el7                                                
[2021-11-03 21:26:13,509][client][DEBUG ]   libbabeltrace.x86_64 0:1.2.4-3.el7                                            
[2021-11-03 21:26:13,509][client][DEBUG ]   libcephfs1.x86_64 2:10.2.11-0.el7                                             
[2021-11-03 21:26:13,509][client][DEBUG ]   libicu.x86_64 0:50.2-4.el7_7                                                  
[2021-11-03 21:26:13,509][client][DEBUG ]   librados2.x86_64 2:10.2.11-0.el7                                              
[2021-11-03 21:26:13,509][client][DEBUG ]   libradosstriper1.x86_64 2:10.2.11-0.el7                                       
[2021-11-03 21:26:13,509][client][DEBUG ]   librbd1.x86_64 2:10.2.11-0.el7                                                
[2021-11-03 21:26:13,510][client][DEBUG ]   librgw2.x86_64 2:10.2.11-0.el7                                                
[2021-11-03 21:26:13,510][client][DEBUG ]   lttng-ust.x86_64 0:2.4.1-4.el7                                                
[2021-11-03 21:26:13,510][client][DEBUG ]   mailcap.noarch 0:2.1.41-2.el7                                                 
[2021-11-03 21:26:13,510][client][DEBUG ]   psmisc.x86_64 0:22.20-17.el7                                                  
[2021-11-03 21:26:13,510][client][DEBUG ]   python-babel.noarch 0:0.9.6-8.el7                                             
[2021-11-03 21:26:13,510][client][DEBUG ]   python-backports.x86_64 0:1.0-8.el7                                           
[2021-11-03 21:26:13,510][client][DEBUG ]   python-backports-ssl_match_hostname.noarch 0:3.5.0.1-1.el7                    
[2021-11-03 21:26:13,510][client][DEBUG ]   python-cephfs.x86_64 2:10.2.11-0.el7                                          
[2021-11-03 21:26:13,510][client][DEBUG ]   python-chardet.noarch 0:2.2.1-3.el7                                           
[2021-11-03 21:26:13,510][client][DEBUG ]   python-flask.noarch 1:0.10.1-5.el7_7                                          
[2021-11-03 21:26:13,510][client][DEBUG ]   python-ipaddress.noarch 0:1.0.16-2.el7                                        
[2021-11-03 21:26:13,510][client][DEBUG ]   python-itsdangerous.noarch 0:0.23-2.el7                                       
[2021-11-03 21:26:13,510][client][DEBUG ]   python-jinja2.noarch 0:2.7.2-4.el7                                            
[2021-11-03 21:26:13,510][client][DEBUG ]   python-markupsafe.x86_64 0:0.11-10.el7                                        
[2021-11-03 21:26:13,510][client][DEBUG ]   python-rados.x86_64 2:10.2.11-0.el7                                           
[2021-11-03 21:26:13,510][client][DEBUG ]   python-rbd.x86_64 2:10.2.11-0.el7                                             
[2021-11-03 21:26:13,510][client][DEBUG ]   python-requests.noarch 0:2.6.0-10.el7                                         
[2021-11-03 21:26:13,510][client][DEBUG ]   python-setuptools.noarch 0:0.9.8-7.el7                                        
[2021-11-03 21:26:13,510][client][DEBUG ]   python-six.noarch 0:1.9.0-2.el7                                               
[2021-11-03 21:26:13,511][client][DEBUG ]   python-urllib3.noarch 0:1.10.2-7.el7                                          
[2021-11-03 21:26:13,511][client][DEBUG ]   python-werkzeug.noarch 0:0.9.1-2.el7                                          
[2021-11-03 21:26:13,511][client][DEBUG ]   userspace-rcu.x86_64 0:0.7.16-1.el7                                           
[2021-11-03 21:26:13,511][client][DEBUG ] 
[2021-11-03 21:26:13,511][client][DEBUG ] Complete!
[2021-11-03 21:26:13,627][client][INFO  ] Running command: sudo ceph --version
[2021-11-03 21:26:13,844][client][DEBUG ] ceph version 10.2.11 (e4b061b47f07f583c92a050d9e84b1813a35671e)
[2021-11-03 21:29:45,685][ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephuser/.cephdeploy.conf
[2021-11-03 21:29:45,686][ceph_deploy.cli][INFO  ] Invoked (1.5.39): /bin/ceph-deploy admin client
[2021-11-03 21:29:45,686][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2021-11-03 21:29:45,686][ceph_deploy.cli][INFO  ]  username                      : None
[2021-11-03 21:29:45,686][ceph_deploy.cli][INFO  ]  verbose                       : False
[2021-11-03 21:29:45,686][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2021-11-03 21:29:45,686][ceph_deploy.cli][INFO  ]  quiet                         : False
[2021-11-03 21:29:45,687][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7ff71636cb00>
[2021-11-03 21:29:45,687][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2021-11-03 21:29:45,687][ceph_deploy.cli][INFO  ]  client                        : ['client']
[2021-11-03 21:29:45,687][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7ff716e81a28>
[2021-11-03 21:29:45,687][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2021-11-03 21:29:45,687][ceph_deploy.cli][INFO  ]  default_release               : False
[2021-11-03 21:29:45,688][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to client
[2021-11-03 21:29:45,887][client][DEBUG ] connection detected need for sudo
[2021-11-03 21:29:46,054][client][DEBUG ] connected to host: client 
[2021-11-03 21:29:46,055][client][DEBUG ] detect platform information from remote host
[2021-11-03 21:29:46,071][client][DEBUG ] detect machine type
[2021-11-03 21:29:46,077][client][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2021-12-10 21:32:24,709][ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephuser/.cephdeploy.conf
[2021-12-10 21:32:24,709][ceph_deploy.cli][INFO  ] Invoked (1.5.39): /usr/bin/ceph-deploy mon create-initial
[2021-12-10 21:32:24,710][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2021-12-10 21:32:24,710][ceph_deploy.cli][INFO  ]  username                      : None
[2021-12-10 21:32:24,710][ceph_deploy.cli][INFO  ]  verbose                       : False
[2021-12-10 21:32:24,710][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2021-12-10 21:32:24,710][ceph_deploy.cli][INFO  ]  subcommand                    : create-initial
[2021-12-10 21:32:24,710][ceph_deploy.cli][INFO  ]  quiet                         : False
[2021-12-10 21:32:24,710][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fb2ef620f38>
[2021-12-10 21:32:24,710][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2021-12-10 21:32:24,710][ceph_deploy.cli][INFO  ]  func                          : <function mon at 0x7fb2ef607758>
[2021-12-10 21:32:24,710][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2021-12-10 21:32:24,710][ceph_deploy.cli][INFO  ]  default_release               : False
[2021-12-10 21:32:24,710][ceph_deploy.cli][INFO  ]  keyrings                      : None
[2021-12-10 21:32:24,711][ceph_deploy.mon][DEBUG ] Deploying mon, cluster ceph hosts mon1
[2021-12-10 21:32:24,712][ceph_deploy.mon][DEBUG ] detecting platform for host mon1 ...
[2021-12-10 21:32:29,943][mon1][DEBUG ] connection detected need for sudo
[2021-12-10 21:32:34,106][mon1][DEBUG ] connected to host: mon1 
[2021-12-10 21:32:34,107][mon1][DEBUG ] detect platform information from remote host
[2021-12-10 21:32:34,188][mon1][DEBUG ] detect machine type
[2021-12-10 21:32:34,193][mon1][DEBUG ] find the location of an executable
[2021-12-10 21:32:34,194][ceph_deploy.mon][INFO  ] distro info: CentOS Linux 7.9.2009 Core
[2021-12-10 21:32:34,194][mon1][DEBUG ] determining if provided host has same hostname in remote
[2021-12-10 21:32:34,194][mon1][DEBUG ] get remote short hostname
[2021-12-10 21:32:34,195][mon1][DEBUG ] deploying mon to mon1
[2021-12-10 21:32:34,195][mon1][DEBUG ] get remote short hostname
[2021-12-10 21:32:34,196][mon1][DEBUG ] remote hostname: mon1
[2021-12-10 21:32:34,198][mon1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2021-12-10 21:32:34,201][ceph_deploy.mon][ERROR ] RuntimeError: config file /etc/ceph/ceph.conf exists with different content; use --overwrite-conf to overwrite
[2021-12-10 21:32:34,201][ceph_deploy][ERROR ] GenericError: Failed to create 1 monitors

[2021-12-10 21:34:25,491][ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephuser/.cephdeploy.conf
[2021-12-10 21:34:25,492][ceph_deploy.cli][INFO  ] Invoked (1.5.39): /usr/bin/ceph-deploy mon create-initial
[2021-12-10 21:34:25,492][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2021-12-10 21:34:25,492][ceph_deploy.cli][INFO  ]  username                      : None
[2021-12-10 21:34:25,492][ceph_deploy.cli][INFO  ]  verbose                       : False
[2021-12-10 21:34:25,492][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2021-12-10 21:34:25,492][ceph_deploy.cli][INFO  ]  subcommand                    : create-initial
[2021-12-10 21:34:25,492][ceph_deploy.cli][INFO  ]  quiet                         : False
[2021-12-10 21:34:25,492][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f413704ff38>
[2021-12-10 21:34:25,492][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2021-12-10 21:34:25,492][ceph_deploy.cli][INFO  ]  func                          : <function mon at 0x7f4137036758>
[2021-12-10 21:34:25,492][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2021-12-10 21:34:25,492][ceph_deploy.cli][INFO  ]  default_release               : False
[2021-12-10 21:34:25,492][ceph_deploy.cli][INFO  ]  keyrings                      : None
[2021-12-10 21:34:25,493][ceph_deploy.mon][DEBUG ] Deploying mon, cluster ceph hosts mon1
[2021-12-10 21:34:25,493][ceph_deploy.mon][DEBUG ] detecting platform for host mon1 ...
[2021-12-10 21:34:27,681][mon1][DEBUG ] connection detected need for sudo
[2021-12-10 21:34:30,261][mon1][DEBUG ] connected to host: mon1 
[2021-12-10 21:34:30,262][mon1][DEBUG ] detect platform information from remote host
[2021-12-10 21:34:30,279][mon1][DEBUG ] detect machine type
[2021-12-10 21:34:30,284][mon1][DEBUG ] find the location of an executable
[2021-12-10 21:34:30,286][ceph_deploy.mon][INFO  ] distro info: CentOS Linux 7.9.2009 Core
[2021-12-10 21:34:30,286][mon1][DEBUG ] determining if provided host has same hostname in remote
[2021-12-10 21:34:30,286][mon1][DEBUG ] get remote short hostname
[2021-12-10 21:34:30,287][mon1][DEBUG ] deploying mon to mon1
[2021-12-10 21:34:30,287][mon1][DEBUG ] get remote short hostname
[2021-12-10 21:34:30,288][mon1][DEBUG ] remote hostname: mon1
[2021-12-10 21:34:30,290][mon1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2021-12-10 21:34:30,292][ceph_deploy.mon][ERROR ] RuntimeError: config file /etc/ceph/ceph.conf exists with different content; use --overwrite-conf to overwrite
[2021-12-10 21:34:30,292][ceph_deploy][ERROR ] GenericError: Failed to create 1 monitors

[2021-12-10 21:35:19,769][ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephuser/.cephdeploy.conf
[2021-12-10 21:35:19,769][ceph_deploy.cli][INFO  ] Invoked (1.5.39): /usr/bin/ceph-deploy --overwrite-conf mon create-initial
[2021-12-10 21:35:19,769][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2021-12-10 21:35:19,770][ceph_deploy.cli][INFO  ]  username                      : None
[2021-12-10 21:35:19,770][ceph_deploy.cli][INFO  ]  verbose                       : False
[2021-12-10 21:35:19,770][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2021-12-10 21:35:19,770][ceph_deploy.cli][INFO  ]  subcommand                    : create-initial
[2021-12-10 21:35:19,770][ceph_deploy.cli][INFO  ]  quiet                         : False
[2021-12-10 21:35:19,770][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f1da6955f38>
[2021-12-10 21:35:19,770][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2021-12-10 21:35:19,770][ceph_deploy.cli][INFO  ]  func                          : <function mon at 0x7f1da693c758>
[2021-12-10 21:35:19,770][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2021-12-10 21:35:19,770][ceph_deploy.cli][INFO  ]  default_release               : False
[2021-12-10 21:35:19,770][ceph_deploy.cli][INFO  ]  keyrings                      : None
[2021-12-10 21:35:19,770][ceph_deploy.mon][DEBUG ] Deploying mon, cluster ceph hosts mon1
[2021-12-10 21:35:19,770][ceph_deploy.mon][DEBUG ] detecting platform for host mon1 ...
[2021-12-10 21:35:23,225][mon1][DEBUG ] connection detected need for sudo
[2021-12-10 21:35:26,152][mon1][DEBUG ] connected to host: mon1 
[2021-12-10 21:35:26,153][mon1][DEBUG ] detect platform information from remote host
[2021-12-10 21:35:26,170][mon1][DEBUG ] detect machine type
[2021-12-10 21:35:26,175][mon1][DEBUG ] find the location of an executable
[2021-12-10 21:35:26,176][ceph_deploy.mon][INFO  ] distro info: CentOS Linux 7.9.2009 Core
[2021-12-10 21:35:26,176][mon1][DEBUG ] determining if provided host has same hostname in remote
[2021-12-10 21:35:26,176][mon1][DEBUG ] get remote short hostname
[2021-12-10 21:35:26,177][mon1][DEBUG ] deploying mon to mon1
[2021-12-10 21:35:26,177][mon1][DEBUG ] get remote short hostname
[2021-12-10 21:35:26,178][mon1][DEBUG ] remote hostname: mon1
[2021-12-10 21:35:26,179][mon1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2021-12-10 21:35:26,193][mon1][DEBUG ] create the mon path if it does not exist
[2021-12-10 21:35:26,194][mon1][DEBUG ] checking for done path: /var/lib/ceph/mon/ceph-mon1/done
[2021-12-10 21:35:26,195][mon1][DEBUG ] done path does not exist: /var/lib/ceph/mon/ceph-mon1/done
[2021-12-10 21:35:26,204][mon1][INFO  ] creating keyring file: /var/lib/ceph/tmp/ceph-mon1.mon.keyring
[2021-12-10 21:35:26,204][mon1][DEBUG ] create the monitor keyring file
[2021-12-10 21:35:26,206][mon1][INFO  ] Running command: sudo ceph-mon --cluster ceph --mkfs -i mon1 --keyring /var/lib/ceph/tmp/ceph-mon1.mon.keyring --setuser 167 --setgroup 167
[2021-12-10 21:35:26,325][mon1][DEBUG ] ceph-mon: renaming mon.noname-a 192.168.1.93:6789/0 to mon.mon1
[2021-12-10 21:35:26,326][mon1][DEBUG ] ceph-mon: set fsid to 67d9e224-a6f7-43d3-ae36-e6100c59258e
[2021-12-10 21:35:26,490][mon1][DEBUG ] ceph-mon: created monfs at /var/lib/ceph/mon/ceph-mon1 for mon.mon1
[2021-12-10 21:35:26,490][mon1][INFO  ] unlinking keyring file /var/lib/ceph/tmp/ceph-mon1.mon.keyring
[2021-12-10 21:35:26,491][mon1][DEBUG ] create a done file to avoid re-doing the mon deployment
[2021-12-10 21:35:26,492][mon1][DEBUG ] create the init path if it does not exist
[2021-12-10 21:35:26,495][mon1][INFO  ] Running command: sudo systemctl enable ceph.target
[2021-12-10 21:35:26,613][mon1][INFO  ] Running command: sudo systemctl enable ceph-mon@mon1
[2021-12-10 21:35:26,679][mon1][WARNING] Created symlink from /etc/systemd/system/ceph-mon.target.wants/ceph-mon@mon1.service to /usr/lib/systemd/system/ceph-mon@.service.
[2021-12-10 21:35:26,713][mon1][INFO  ] Running command: sudo systemctl start ceph-mon@mon1
[2021-12-10 21:35:28,783][mon1][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.mon1.asok mon_status
[2021-12-10 21:35:28,848][mon1][DEBUG ] ********************************************************************************
[2021-12-10 21:35:28,848][mon1][DEBUG ] status for monitor: mon.mon1
[2021-12-10 21:35:28,848][mon1][DEBUG ] {
[2021-12-10 21:35:28,848][mon1][DEBUG ]   "election_epoch": 3, 
[2021-12-10 21:35:28,848][mon1][DEBUG ]   "extra_probe_peers": [], 
[2021-12-10 21:35:28,848][mon1][DEBUG ]   "monmap": {
[2021-12-10 21:35:28,848][mon1][DEBUG ]     "created": "2021-12-10 21:35:26.299692", 
[2021-12-10 21:35:28,848][mon1][DEBUG ]     "epoch": 1, 
[2021-12-10 21:35:28,848][mon1][DEBUG ]     "fsid": "67d9e224-a6f7-43d3-ae36-e6100c59258e", 
[2021-12-10 21:35:28,848][mon1][DEBUG ]     "modified": "2021-12-10 21:35:26.299692", 
[2021-12-10 21:35:28,848][mon1][DEBUG ]     "mons": [
[2021-12-10 21:35:28,848][mon1][DEBUG ]       {
[2021-12-10 21:35:28,848][mon1][DEBUG ]         "addr": "192.168.1.93:6789/0", 
[2021-12-10 21:35:28,848][mon1][DEBUG ]         "name": "mon1", 
[2021-12-10 21:35:28,849][mon1][DEBUG ]         "rank": 0
[2021-12-10 21:35:28,849][mon1][DEBUG ]       }
[2021-12-10 21:35:28,849][mon1][DEBUG ]     ]
[2021-12-10 21:35:28,849][mon1][DEBUG ]   }, 
[2021-12-10 21:35:28,849][mon1][DEBUG ]   "name": "mon1", 
[2021-12-10 21:35:28,849][mon1][DEBUG ]   "outside_quorum": [], 
[2021-12-10 21:35:28,849][mon1][DEBUG ]   "quorum": [
[2021-12-10 21:35:28,849][mon1][DEBUG ]     0
[2021-12-10 21:35:28,849][mon1][DEBUG ]   ], 
[2021-12-10 21:35:28,849][mon1][DEBUG ]   "rank": 0, 
[2021-12-10 21:35:28,849][mon1][DEBUG ]   "state": "leader", 
[2021-12-10 21:35:28,849][mon1][DEBUG ]   "sync_provider": []
[2021-12-10 21:35:28,849][mon1][DEBUG ] }
[2021-12-10 21:35:28,849][mon1][DEBUG ] ********************************************************************************
[2021-12-10 21:35:28,849][mon1][INFO  ] monitor: mon.mon1 is running
[2021-12-10 21:35:28,851][mon1][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.mon1.asok mon_status
[2021-12-10 21:35:28,915][ceph_deploy.mon][INFO  ] processing monitor mon.mon1
[2021-12-10 21:35:32,545][mon1][DEBUG ] connection detected need for sudo
[2021-12-10 21:35:35,735][mon1][DEBUG ] connected to host: mon1 
[2021-12-10 21:35:35,736][mon1][DEBUG ] detect platform information from remote host
[2021-12-10 21:35:35,754][mon1][DEBUG ] detect machine type
[2021-12-10 21:35:35,760][mon1][DEBUG ] find the location of an executable
[2021-12-10 21:35:35,763][mon1][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.mon1.asok mon_status
[2021-12-10 21:35:35,827][ceph_deploy.mon][INFO  ] mon.mon1 monitor has reached quorum!
[2021-12-10 21:35:35,828][ceph_deploy.mon][INFO  ] all initial monitors are running and have formed quorum
[2021-12-10 21:35:35,828][ceph_deploy.mon][INFO  ] Running gatherkeys...
[2021-12-10 21:35:35,828][ceph_deploy.gatherkeys][INFO  ] Storing keys in temp directory /tmp/tmp1ST_tG
[2021-12-10 21:35:38,641][mon1][DEBUG ] connection detected need for sudo
[2021-12-10 21:35:40,985][mon1][DEBUG ] connected to host: mon1 
[2021-12-10 21:35:40,985][mon1][DEBUG ] detect platform information from remote host
[2021-12-10 21:35:41,004][mon1][DEBUG ] detect machine type
[2021-12-10 21:35:41,010][mon1][DEBUG ] get remote short hostname
[2021-12-10 21:35:41,011][mon1][DEBUG ] fetch remote file
[2021-12-10 21:35:41,015][mon1][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --admin-daemon=/var/run/ceph/ceph-mon.mon1.asok mon_status
[2021-12-10 21:35:41,082][mon1][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-mon1/keyring auth get client.admin
[2021-12-10 21:35:41,298][mon1][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-mon1/keyring auth get-or-create client.admin osd allow * mds allow * mon allow * mgr allow *
[2021-12-10 21:35:41,463][mon1][ERROR ] "ceph auth get-or-create for keytype admin returned 22
[2021-12-10 21:35:41,464][mon1][DEBUG ] Error EINVAL: unknown cap type 'mgr'
[2021-12-10 21:35:41,464][mon1][ERROR ] Failed to return 'admin' key from host mon1
[2021-12-10 21:35:41,464][ceph_deploy.gatherkeys][ERROR ] Failed to connect to host:mon1
[2021-12-10 21:35:41,464][ceph_deploy.gatherkeys][INFO  ] Destroy temp directory /tmp/tmp1ST_tG
[2021-12-10 21:35:41,464][ceph_deploy][ERROR ] RuntimeError: Failed to connect any mon

[2021-12-10 21:40:55,644][ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephuser/.cephdeploy.conf
[2021-12-10 21:40:55,644][ceph_deploy.cli][INFO  ] Invoked (1.5.39): /usr/bin/ceph-deploy --overwrite-conf mon create-initial
[2021-12-10 21:40:55,644][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2021-12-10 21:40:55,644][ceph_deploy.cli][INFO  ]  username                      : None
[2021-12-10 21:40:55,644][ceph_deploy.cli][INFO  ]  verbose                       : False
[2021-12-10 21:40:55,644][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2021-12-10 21:40:55,644][ceph_deploy.cli][INFO  ]  subcommand                    : create-initial
[2021-12-10 21:40:55,644][ceph_deploy.cli][INFO  ]  quiet                         : False
[2021-12-10 21:40:55,644][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fdaaf857f38>
[2021-12-10 21:40:55,644][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2021-12-10 21:40:55,645][ceph_deploy.cli][INFO  ]  func                          : <function mon at 0x7fdaaf83e758>
[2021-12-10 21:40:55,645][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2021-12-10 21:40:55,645][ceph_deploy.cli][INFO  ]  default_release               : False
[2021-12-10 21:40:55,645][ceph_deploy.cli][INFO  ]  keyrings                      : None
[2021-12-10 21:40:55,645][ceph_deploy.mon][DEBUG ] Deploying mon, cluster ceph hosts mon1
[2021-12-10 21:40:55,645][ceph_deploy.mon][DEBUG ] detecting platform for host mon1 ...
[2021-12-10 21:40:55,807][mon1][DEBUG ] connection detected need for sudo
[2021-12-10 21:40:55,971][mon1][DEBUG ] connected to host: mon1 
[2021-12-10 21:40:55,971][mon1][DEBUG ] detect platform information from remote host
[2021-12-10 21:40:55,989][mon1][DEBUG ] detect machine type
[2021-12-10 21:40:55,993][mon1][DEBUG ] find the location of an executable
[2021-12-10 21:40:55,994][ceph_deploy.mon][INFO  ] distro info: CentOS Linux 7.9.2009 Core
[2021-12-10 21:40:55,994][mon1][DEBUG ] determining if provided host has same hostname in remote
[2021-12-10 21:40:55,994][mon1][DEBUG ] get remote short hostname
[2021-12-10 21:40:55,995][mon1][DEBUG ] deploying mon to mon1
[2021-12-10 21:40:55,995][mon1][DEBUG ] get remote short hostname
[2021-12-10 21:40:55,996][mon1][DEBUG ] remote hostname: mon1
[2021-12-10 21:40:55,999][mon1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2021-12-10 21:40:56,001][mon1][DEBUG ] create the mon path if it does not exist
[2021-12-10 21:40:56,002][mon1][DEBUG ] checking for done path: /var/lib/ceph/mon/ceph-mon1/done
[2021-12-10 21:40:56,003][mon1][DEBUG ] create a done file to avoid re-doing the mon deployment
[2021-12-10 21:40:56,004][mon1][DEBUG ] create the init path if it does not exist
[2021-12-10 21:40:56,006][mon1][INFO  ] Running command: sudo systemctl enable ceph.target
[2021-12-10 21:40:56,077][mon1][INFO  ] Running command: sudo systemctl enable ceph-mon@mon1
[2021-12-10 21:40:56,145][mon1][INFO  ] Running command: sudo systemctl start ceph-mon@mon1
[2021-12-10 21:40:58,215][mon1][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.mon1.asok mon_status
[2021-12-10 21:40:58,280][mon1][DEBUG ] ********************************************************************************
[2021-12-10 21:40:58,280][mon1][DEBUG ] status for monitor: mon.mon1
[2021-12-10 21:40:58,281][mon1][DEBUG ] {
[2021-12-10 21:40:58,281][mon1][DEBUG ]   "election_epoch": 3, 
[2021-12-10 21:40:58,281][mon1][DEBUG ]   "extra_probe_peers": [], 
[2021-12-10 21:40:58,281][mon1][DEBUG ]   "monmap": {
[2021-12-10 21:40:58,281][mon1][DEBUG ]     "created": "2021-12-10 21:35:26.299692", 
[2021-12-10 21:40:58,281][mon1][DEBUG ]     "epoch": 1, 
[2021-12-10 21:40:58,281][mon1][DEBUG ]     "fsid": "67d9e224-a6f7-43d3-ae36-e6100c59258e", 
[2021-12-10 21:40:58,281][mon1][DEBUG ]     "modified": "2021-12-10 21:35:26.299692", 
[2021-12-10 21:40:58,281][mon1][DEBUG ]     "mons": [
[2021-12-10 21:40:58,281][mon1][DEBUG ]       {
[2021-12-10 21:40:58,281][mon1][DEBUG ]         "addr": "192.168.1.93:6789/0", 
[2021-12-10 21:40:58,281][mon1][DEBUG ]         "name": "mon1", 
[2021-12-10 21:40:58,281][mon1][DEBUG ]         "rank": 0
[2021-12-10 21:40:58,281][mon1][DEBUG ]       }
[2021-12-10 21:40:58,281][mon1][DEBUG ]     ]
[2021-12-10 21:40:58,281][mon1][DEBUG ]   }, 
[2021-12-10 21:40:58,281][mon1][DEBUG ]   "name": "mon1", 
[2021-12-10 21:40:58,281][mon1][DEBUG ]   "outside_quorum": [], 
[2021-12-10 21:40:58,281][mon1][DEBUG ]   "quorum": [
[2021-12-10 21:40:58,282][mon1][DEBUG ]     0
[2021-12-10 21:40:58,282][mon1][DEBUG ]   ], 
[2021-12-10 21:40:58,282][mon1][DEBUG ]   "rank": 0, 
[2021-12-10 21:40:58,282][mon1][DEBUG ]   "state": "leader", 
[2021-12-10 21:40:58,282][mon1][DEBUG ]   "sync_provider": []
[2021-12-10 21:40:58,282][mon1][DEBUG ] }
[2021-12-10 21:40:58,282][mon1][DEBUG ] ********************************************************************************
[2021-12-10 21:40:58,282][mon1][INFO  ] monitor: mon.mon1 is running
[2021-12-10 21:40:58,284][mon1][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.mon1.asok mon_status
[2021-12-10 21:40:58,449][ceph_deploy.mon][INFO  ] processing monitor mon.mon1
[2021-12-10 21:40:58,612][mon1][DEBUG ] connection detected need for sudo
[2021-12-10 21:40:58,779][mon1][DEBUG ] connected to host: mon1 
[2021-12-10 21:40:58,779][mon1][DEBUG ] detect platform information from remote host
[2021-12-10 21:40:58,798][mon1][DEBUG ] detect machine type
[2021-12-10 21:40:58,802][mon1][DEBUG ] find the location of an executable
[2021-12-10 21:40:58,805][mon1][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.mon1.asok mon_status
[2021-12-10 21:40:58,870][ceph_deploy.mon][INFO  ] mon.mon1 monitor has reached quorum!
[2021-12-10 21:40:58,870][ceph_deploy.mon][INFO  ] all initial monitors are running and have formed quorum
[2021-12-10 21:40:58,870][ceph_deploy.mon][INFO  ] Running gatherkeys...
[2021-12-10 21:40:58,870][ceph_deploy.gatherkeys][INFO  ] Storing keys in temp directory /tmp/tmplcchaN
[2021-12-10 21:40:59,030][mon1][DEBUG ] connection detected need for sudo
[2021-12-10 21:40:59,193][mon1][DEBUG ] connected to host: mon1 
[2021-12-10 21:40:59,193][mon1][DEBUG ] detect platform information from remote host
[2021-12-10 21:40:59,210][mon1][DEBUG ] detect machine type
[2021-12-10 21:40:59,216][mon1][DEBUG ] get remote short hostname
[2021-12-10 21:40:59,217][mon1][DEBUG ] fetch remote file
[2021-12-10 21:40:59,219][mon1][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --admin-daemon=/var/run/ceph/ceph-mon.mon1.asok mon_status
[2021-12-10 21:40:59,285][mon1][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-mon1/keyring auth get client.admin
[2021-12-10 21:40:59,452][mon1][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-mon1/keyring auth get-or-create client.admin osd allow * mds allow * mon allow * mgr allow *
[2021-12-10 21:40:59,617][mon1][ERROR ] "ceph auth get-or-create for keytype admin returned 22
[2021-12-10 21:40:59,617][mon1][DEBUG ] Error EINVAL: unknown cap type 'mgr'
[2021-12-10 21:40:59,617][mon1][ERROR ] Failed to return 'admin' key from host mon1
[2021-12-10 21:40:59,618][ceph_deploy.gatherkeys][ERROR ] Failed to connect to host:mon1
[2021-12-10 21:40:59,618][ceph_deploy.gatherkeys][INFO  ] Destroy temp directory /tmp/tmplcchaN
[2021-12-10 21:40:59,618][ceph_deploy][ERROR ] RuntimeError: Failed to connect any mon

[2021-12-10 21:42:51,245][ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephuser/.cephdeploy.conf
[2021-12-10 21:42:51,245][ceph_deploy.cli][INFO  ] Invoked (1.5.39): /usr/bin/ceph-deploy --overwrite-conf mon create-initial
[2021-12-10 21:42:51,245][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2021-12-10 21:42:51,245][ceph_deploy.cli][INFO  ]  username                      : None
[2021-12-10 21:42:51,246][ceph_deploy.cli][INFO  ]  verbose                       : False
[2021-12-10 21:42:51,246][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2021-12-10 21:42:51,246][ceph_deploy.cli][INFO  ]  subcommand                    : create-initial
[2021-12-10 21:42:51,246][ceph_deploy.cli][INFO  ]  quiet                         : False
[2021-12-10 21:42:51,246][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f0e217b4f38>
[2021-12-10 21:42:51,246][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2021-12-10 21:42:51,246][ceph_deploy.cli][INFO  ]  func                          : <function mon at 0x7f0e2179b758>
[2021-12-10 21:42:51,246][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2021-12-10 21:42:51,246][ceph_deploy.cli][INFO  ]  default_release               : False
[2021-12-10 21:42:51,246][ceph_deploy.cli][INFO  ]  keyrings                      : None
[2021-12-10 21:42:51,246][ceph_deploy.mon][DEBUG ] Deploying mon, cluster ceph hosts mon1
[2021-12-10 21:42:51,247][ceph_deploy.mon][DEBUG ] detecting platform for host mon1 ...
[2021-12-10 21:42:51,411][mon1][DEBUG ] connection detected need for sudo
[2021-12-10 21:42:51,575][mon1][DEBUG ] connected to host: mon1 
[2021-12-10 21:42:51,576][mon1][DEBUG ] detect platform information from remote host
[2021-12-10 21:42:51,593][mon1][DEBUG ] detect machine type
[2021-12-10 21:42:51,597][mon1][DEBUG ] find the location of an executable
[2021-12-10 21:42:51,598][ceph_deploy.mon][INFO  ] distro info: CentOS Linux 7.9.2009 Core
[2021-12-10 21:42:51,598][mon1][DEBUG ] determining if provided host has same hostname in remote
[2021-12-10 21:42:51,598][mon1][DEBUG ] get remote short hostname
[2021-12-10 21:42:51,599][mon1][DEBUG ] deploying mon to mon1
[2021-12-10 21:42:51,600][mon1][DEBUG ] get remote short hostname
[2021-12-10 21:42:51,601][mon1][DEBUG ] remote hostname: mon1
[2021-12-10 21:42:51,606][mon1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2021-12-10 21:42:51,607][mon1][DEBUG ] create the mon path if it does not exist
[2021-12-10 21:42:51,608][mon1][DEBUG ] checking for done path: /var/lib/ceph/mon/ceph-mon1/done
[2021-12-10 21:42:51,609][mon1][DEBUG ] create a done file to avoid re-doing the mon deployment
[2021-12-10 21:42:51,610][mon1][DEBUG ] create the init path if it does not exist
[2021-12-10 21:42:51,612][mon1][INFO  ] Running command: sudo systemctl enable ceph.target
[2021-12-10 21:42:51,682][mon1][INFO  ] Running command: sudo systemctl enable ceph-mon@mon1
[2021-12-10 21:42:51,750][mon1][INFO  ] Running command: sudo systemctl start ceph-mon@mon1
[2021-12-10 21:42:53,821][mon1][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.mon1.asok mon_status
[2021-12-10 21:42:53,886][mon1][DEBUG ] ********************************************************************************
[2021-12-10 21:42:53,886][mon1][DEBUG ] status for monitor: mon.mon1
[2021-12-10 21:42:53,886][mon1][DEBUG ] {
[2021-12-10 21:42:53,886][mon1][DEBUG ]   "election_epoch": 3, 
[2021-12-10 21:42:53,886][mon1][DEBUG ]   "extra_probe_peers": [], 
[2021-12-10 21:42:53,886][mon1][DEBUG ]   "monmap": {
[2021-12-10 21:42:53,886][mon1][DEBUG ]     "created": "2021-12-10 21:35:26.299692", 
[2021-12-10 21:42:53,886][mon1][DEBUG ]     "epoch": 1, 
[2021-12-10 21:42:53,887][mon1][DEBUG ]     "fsid": "67d9e224-a6f7-43d3-ae36-e6100c59258e", 
[2021-12-10 21:42:53,887][mon1][DEBUG ]     "modified": "2021-12-10 21:35:26.299692", 
[2021-12-10 21:42:53,887][mon1][DEBUG ]     "mons": [
[2021-12-10 21:42:53,887][mon1][DEBUG ]       {
[2021-12-10 21:42:53,887][mon1][DEBUG ]         "addr": "192.168.1.93:6789/0", 
[2021-12-10 21:42:53,887][mon1][DEBUG ]         "name": "mon1", 
[2021-12-10 21:42:53,887][mon1][DEBUG ]         "rank": 0
[2021-12-10 21:42:53,887][mon1][DEBUG ]       }
[2021-12-10 21:42:53,887][mon1][DEBUG ]     ]
[2021-12-10 21:42:53,887][mon1][DEBUG ]   }, 
[2021-12-10 21:42:53,887][mon1][DEBUG ]   "name": "mon1", 
[2021-12-10 21:42:53,887][mon1][DEBUG ]   "outside_quorum": [], 
[2021-12-10 21:42:53,887][mon1][DEBUG ]   "quorum": [
[2021-12-10 21:42:53,887][mon1][DEBUG ]     0
[2021-12-10 21:42:53,887][mon1][DEBUG ]   ], 
[2021-12-10 21:42:53,887][mon1][DEBUG ]   "rank": 0, 
[2021-12-10 21:42:53,887][mon1][DEBUG ]   "state": "leader", 
[2021-12-10 21:42:53,887][mon1][DEBUG ]   "sync_provider": []
[2021-12-10 21:42:53,887][mon1][DEBUG ] }
[2021-12-10 21:42:53,887][mon1][DEBUG ] ********************************************************************************
[2021-12-10 21:42:53,887][mon1][INFO  ] monitor: mon.mon1 is running
[2021-12-10 21:42:53,889][mon1][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.mon1.asok mon_status
[2021-12-10 21:42:54,305][ceph_deploy.mon][INFO  ] processing monitor mon.mon1
[2021-12-10 21:42:54,464][mon1][DEBUG ] connection detected need for sudo
[2021-12-10 21:42:54,634][mon1][DEBUG ] connected to host: mon1 
[2021-12-10 21:42:54,634][mon1][DEBUG ] detect platform information from remote host
[2021-12-10 21:42:54,652][mon1][DEBUG ] detect machine type
[2021-12-10 21:42:54,656][mon1][DEBUG ] find the location of an executable
[2021-12-10 21:42:54,659][mon1][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.mon1.asok mon_status
[2021-12-10 21:42:54,725][ceph_deploy.mon][INFO  ] mon.mon1 monitor has reached quorum!
[2021-12-10 21:42:54,726][ceph_deploy.mon][INFO  ] all initial monitors are running and have formed quorum
[2021-12-10 21:42:54,726][ceph_deploy.mon][INFO  ] Running gatherkeys...
[2021-12-10 21:42:54,726][ceph_deploy.gatherkeys][INFO  ] Storing keys in temp directory /tmp/tmpAXEHej
[2021-12-10 21:42:54,886][mon1][DEBUG ] connection detected need for sudo
[2021-12-10 21:42:55,046][mon1][DEBUG ] connected to host: mon1 
[2021-12-10 21:42:55,047][mon1][DEBUG ] detect platform information from remote host
[2021-12-10 21:42:55,064][mon1][DEBUG ] detect machine type
[2021-12-10 21:42:55,069][mon1][DEBUG ] get remote short hostname
[2021-12-10 21:42:55,071][mon1][DEBUG ] fetch remote file
[2021-12-10 21:42:55,073][mon1][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --admin-daemon=/var/run/ceph/ceph-mon.mon1.asok mon_status
[2021-12-10 21:42:55,140][mon1][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-mon1/keyring auth get client.admin
[2021-12-10 21:42:55,307][mon1][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-mon1/keyring auth get-or-create client.admin osd allow * mds allow * mon allow * mgr allow *
[2021-12-10 21:42:55,472][mon1][ERROR ] "ceph auth get-or-create for keytype admin returned 22
[2021-12-10 21:42:55,472][mon1][DEBUG ] Error EINVAL: unknown cap type 'mgr'
[2021-12-10 21:42:55,472][mon1][ERROR ] Failed to return 'admin' key from host mon1
[2021-12-10 21:42:55,473][ceph_deploy.gatherkeys][ERROR ] Failed to connect to host:mon1
[2021-12-10 21:42:55,473][ceph_deploy.gatherkeys][INFO  ] Destroy temp directory /tmp/tmpAXEHej
[2021-12-10 21:42:55,473][ceph_deploy][ERROR ] RuntimeError: Failed to connect any mon

[2021-12-10 21:43:04,709][ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephuser/.cephdeploy.conf
[2021-12-10 21:43:04,709][ceph_deploy.cli][INFO  ] Invoked (1.5.39): /usr/bin/ceph-deploy mon create-initial
[2021-12-10 21:43:04,709][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2021-12-10 21:43:04,709][ceph_deploy.cli][INFO  ]  username                      : None
[2021-12-10 21:43:04,709][ceph_deploy.cli][INFO  ]  verbose                       : False
[2021-12-10 21:43:04,709][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2021-12-10 21:43:04,709][ceph_deploy.cli][INFO  ]  subcommand                    : create-initial
[2021-12-10 21:43:04,709][ceph_deploy.cli][INFO  ]  quiet                         : False
[2021-12-10 21:43:04,709][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fedf0f2bf38>
[2021-12-10 21:43:04,709][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2021-12-10 21:43:04,709][ceph_deploy.cli][INFO  ]  func                          : <function mon at 0x7fedf0f12758>
[2021-12-10 21:43:04,709][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2021-12-10 21:43:04,709][ceph_deploy.cli][INFO  ]  default_release               : False
[2021-12-10 21:43:04,710][ceph_deploy.cli][INFO  ]  keyrings                      : None
[2021-12-10 21:43:04,710][ceph_deploy.mon][DEBUG ] Deploying mon, cluster ceph hosts mon1
[2021-12-10 21:43:04,710][ceph_deploy.mon][DEBUG ] detecting platform for host mon1 ...
[2021-12-10 21:43:04,874][mon1][DEBUG ] connection detected need for sudo
[2021-12-10 21:43:05,038][mon1][DEBUG ] connected to host: mon1 
[2021-12-10 21:43:05,038][mon1][DEBUG ] detect platform information from remote host
[2021-12-10 21:43:05,056][mon1][DEBUG ] detect machine type
[2021-12-10 21:43:05,060][mon1][DEBUG ] find the location of an executable
[2021-12-10 21:43:05,061][ceph_deploy.mon][INFO  ] distro info: CentOS Linux 7.9.2009 Core
[2021-12-10 21:43:05,062][mon1][DEBUG ] determining if provided host has same hostname in remote
[2021-12-10 21:43:05,062][mon1][DEBUG ] get remote short hostname
[2021-12-10 21:43:05,062][mon1][DEBUG ] deploying mon to mon1
[2021-12-10 21:43:05,062][mon1][DEBUG ] get remote short hostname
[2021-12-10 21:43:05,063][mon1][DEBUG ] remote hostname: mon1
[2021-12-10 21:43:05,065][mon1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2021-12-10 21:43:05,066][mon1][DEBUG ] create the mon path if it does not exist
[2021-12-10 21:43:05,067][mon1][DEBUG ] checking for done path: /var/lib/ceph/mon/ceph-mon1/done
[2021-12-10 21:43:05,068][mon1][DEBUG ] create a done file to avoid re-doing the mon deployment
[2021-12-10 21:43:05,069][mon1][DEBUG ] create the init path if it does not exist
[2021-12-10 21:43:05,072][mon1][INFO  ] Running command: sudo systemctl enable ceph.target
[2021-12-10 21:43:05,142][mon1][INFO  ] Running command: sudo systemctl enable ceph-mon@mon1
[2021-12-10 21:43:05,210][mon1][INFO  ] Running command: sudo systemctl start ceph-mon@mon1
[2021-12-10 21:43:07,280][mon1][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.mon1.asok mon_status
[2021-12-10 21:43:07,345][mon1][DEBUG ] ********************************************************************************
[2021-12-10 21:43:07,345][mon1][DEBUG ] status for monitor: mon.mon1
[2021-12-10 21:43:07,345][mon1][DEBUG ] {
[2021-12-10 21:43:07,345][mon1][DEBUG ]   "election_epoch": 3, 
[2021-12-10 21:43:07,345][mon1][DEBUG ]   "extra_probe_peers": [], 
[2021-12-10 21:43:07,345][mon1][DEBUG ]   "monmap": {
[2021-12-10 21:43:07,345][mon1][DEBUG ]     "created": "2021-12-10 21:35:26.299692", 
[2021-12-10 21:43:07,345][mon1][DEBUG ]     "epoch": 1, 
[2021-12-10 21:43:07,345][mon1][DEBUG ]     "fsid": "67d9e224-a6f7-43d3-ae36-e6100c59258e", 
[2021-12-10 21:43:07,345][mon1][DEBUG ]     "modified": "2021-12-10 21:35:26.299692", 
[2021-12-10 21:43:07,345][mon1][DEBUG ]     "mons": [
[2021-12-10 21:43:07,345][mon1][DEBUG ]       {
[2021-12-10 21:43:07,345][mon1][DEBUG ]         "addr": "192.168.1.93:6789/0", 
[2021-12-10 21:43:07,345][mon1][DEBUG ]         "name": "mon1", 
[2021-12-10 21:43:07,345][mon1][DEBUG ]         "rank": 0
[2021-12-10 21:43:07,346][mon1][DEBUG ]       }
[2021-12-10 21:43:07,346][mon1][DEBUG ]     ]
[2021-12-10 21:43:07,346][mon1][DEBUG ]   }, 
[2021-12-10 21:43:07,346][mon1][DEBUG ]   "name": "mon1", 
[2021-12-10 21:43:07,346][mon1][DEBUG ]   "outside_quorum": [], 
[2021-12-10 21:43:07,346][mon1][DEBUG ]   "quorum": [
[2021-12-10 21:43:07,346][mon1][DEBUG ]     0
[2021-12-10 21:43:07,346][mon1][DEBUG ]   ], 
[2021-12-10 21:43:07,346][mon1][DEBUG ]   "rank": 0, 
[2021-12-10 21:43:07,346][mon1][DEBUG ]   "state": "leader", 
[2021-12-10 21:43:07,346][mon1][DEBUG ]   "sync_provider": []
[2021-12-10 21:43:07,346][mon1][DEBUG ] }
[2021-12-10 21:43:07,346][mon1][DEBUG ] ********************************************************************************
[2021-12-10 21:43:07,346][mon1][INFO  ] monitor: mon.mon1 is running
[2021-12-10 21:43:07,348][mon1][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.mon1.asok mon_status
[2021-12-10 21:43:07,513][ceph_deploy.mon][INFO  ] processing monitor mon.mon1
[2021-12-10 21:43:07,672][mon1][DEBUG ] connection detected need for sudo
[2021-12-10 21:43:07,834][mon1][DEBUG ] connected to host: mon1 
[2021-12-10 21:43:07,835][mon1][DEBUG ] detect platform information from remote host
[2021-12-10 21:43:07,852][mon1][DEBUG ] detect machine type
[2021-12-10 21:43:07,857][mon1][DEBUG ] find the location of an executable
[2021-12-10 21:43:07,859][mon1][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.mon1.asok mon_status
[2021-12-10 21:43:07,924][ceph_deploy.mon][INFO  ] mon.mon1 monitor has reached quorum!
[2021-12-10 21:43:07,924][ceph_deploy.mon][INFO  ] all initial monitors are running and have formed quorum
[2021-12-10 21:43:07,924][ceph_deploy.mon][INFO  ] Running gatherkeys...
[2021-12-10 21:43:07,925][ceph_deploy.gatherkeys][INFO  ] Storing keys in temp directory /tmp/tmpQG4Gyc
[2021-12-10 21:43:08,084][mon1][DEBUG ] connection detected need for sudo
[2021-12-10 21:43:08,247][mon1][DEBUG ] connected to host: mon1 
[2021-12-10 21:43:08,247][mon1][DEBUG ] detect platform information from remote host
[2021-12-10 21:43:08,264][mon1][DEBUG ] detect machine type
[2021-12-10 21:43:08,269][mon1][DEBUG ] get remote short hostname
[2021-12-10 21:43:08,270][mon1][DEBUG ] fetch remote file
[2021-12-10 21:43:08,273][mon1][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --admin-daemon=/var/run/ceph/ceph-mon.mon1.asok mon_status
[2021-12-10 21:43:08,340][mon1][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-mon1/keyring auth get client.admin
[2021-12-10 21:43:08,507][mon1][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-mon1/keyring auth get-or-create client.admin osd allow * mds allow * mon allow * mgr allow *
[2021-12-10 21:43:08,672][mon1][ERROR ] "ceph auth get-or-create for keytype admin returned 22
[2021-12-10 21:43:08,672][mon1][DEBUG ] Error EINVAL: unknown cap type 'mgr'
[2021-12-10 21:43:08,672][mon1][ERROR ] Failed to return 'admin' key from host mon1
[2021-12-10 21:43:08,672][ceph_deploy.gatherkeys][ERROR ] Failed to connect to host:mon1
[2021-12-10 21:43:08,673][ceph_deploy.gatherkeys][INFO  ] Destroy temp directory /tmp/tmpQG4Gyc
[2021-12-10 21:43:08,673][ceph_deploy][ERROR ] RuntimeError: Failed to connect any mon

[2021-12-10 21:44:54,975][ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephuser/.cephdeploy.conf
[2021-12-10 21:44:54,976][ceph_deploy.cli][INFO  ] Invoked (1.5.39): /usr/bin/ceph-deploy gatherkeys mon1
[2021-12-10 21:44:54,976][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2021-12-10 21:44:54,976][ceph_deploy.cli][INFO  ]  username                      : None
[2021-12-10 21:44:54,976][ceph_deploy.cli][INFO  ]  verbose                       : False
[2021-12-10 21:44:54,976][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2021-12-10 21:44:54,976][ceph_deploy.cli][INFO  ]  quiet                         : False
[2021-12-10 21:44:54,976][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fe70ba34878>
[2021-12-10 21:44:54,976][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2021-12-10 21:44:54,976][ceph_deploy.cli][INFO  ]  mon                           : ['mon1']
[2021-12-10 21:44:54,976][ceph_deploy.cli][INFO  ]  func                          : <function gatherkeys at 0x7fe70b9f8230>
[2021-12-10 21:44:54,977][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2021-12-10 21:44:54,977][ceph_deploy.cli][INFO  ]  default_release               : False
[2021-12-10 21:44:54,977][ceph_deploy.gatherkeys][INFO  ] Storing keys in temp directory /tmp/tmpuClPQE
[2021-12-10 21:44:55,141][mon1][DEBUG ] connection detected need for sudo
[2021-12-10 21:44:55,304][mon1][DEBUG ] connected to host: mon1 
[2021-12-10 21:44:55,305][mon1][DEBUG ] detect platform information from remote host
[2021-12-10 21:44:55,321][mon1][DEBUG ] detect machine type
[2021-12-10 21:44:55,327][mon1][DEBUG ] get remote short hostname
[2021-12-10 21:44:55,328][mon1][DEBUG ] fetch remote file
[2021-12-10 21:44:55,332][mon1][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --admin-daemon=/var/run/ceph/ceph-mon.mon1.asok mon_status
[2021-12-10 21:44:55,400][mon1][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-mon1/keyring auth get client.admin
[2021-12-10 21:44:55,567][mon1][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-mon1/keyring auth get-or-create client.admin osd allow * mds allow * mon allow * mgr allow *
[2021-12-10 21:44:55,732][mon1][ERROR ] "ceph auth get-or-create for keytype admin returned 22
[2021-12-10 21:44:55,732][mon1][DEBUG ] Error EINVAL: unknown cap type 'mgr'
[2021-12-10 21:44:55,732][mon1][ERROR ] Failed to return 'admin' key from host mon1
[2021-12-10 21:44:55,732][ceph_deploy.gatherkeys][ERROR ] Failed to connect to host:mon1
[2021-12-10 21:44:55,732][ceph_deploy.gatherkeys][INFO  ] Destroy temp directory /tmp/tmpuClPQE
[2021-12-10 21:44:55,732][ceph_deploy][ERROR ] RuntimeError: Failed to connect any mon

[2021-12-10 21:45:11,365][ceph_deploy.conf][DEBUG ] found configuration file at: /root/.cephdeploy.conf
[2021-12-10 21:45:11,365][ceph_deploy.cli][INFO  ] Invoked (1.5.39): /bin/ceph-deploy --overwrite-conf mon create-initial
[2021-12-10 21:45:11,366][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2021-12-10 21:45:11,366][ceph_deploy.cli][INFO  ]  username                      : None
[2021-12-10 21:45:11,366][ceph_deploy.cli][INFO  ]  verbose                       : False
[2021-12-10 21:45:11,366][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2021-12-10 21:45:11,366][ceph_deploy.cli][INFO  ]  subcommand                    : create-initial
[2021-12-10 21:45:11,366][ceph_deploy.cli][INFO  ]  quiet                         : False
[2021-12-10 21:45:11,366][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fa388be3f38>
[2021-12-10 21:45:11,366][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2021-12-10 21:45:11,366][ceph_deploy.cli][INFO  ]  func                          : <function mon at 0x7fa388bca758>
[2021-12-10 21:45:11,366][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2021-12-10 21:45:11,366][ceph_deploy.cli][INFO  ]  default_release               : False
[2021-12-10 21:45:11,366][ceph_deploy.cli][INFO  ]  keyrings                      : None
[2021-12-10 21:45:11,367][ceph_deploy.mon][DEBUG ] Deploying mon, cluster ceph hosts mon1
[2021-12-10 21:45:11,367][ceph_deploy.mon][DEBUG ] detecting platform for host mon1 ...
[2021-12-10 21:45:14,044][mon1][DEBUG ] connection detected need for sudo
[2021-12-10 21:45:16,667][mon1][DEBUG ] connected to host: mon1 
[2021-12-10 21:45:16,668][mon1][DEBUG ] detect platform information from remote host
[2021-12-10 21:45:16,685][mon1][DEBUG ] detect machine type
[2021-12-10 21:45:16,690][mon1][DEBUG ] find the location of an executable
[2021-12-10 21:45:16,691][ceph_deploy.mon][INFO  ] distro info: CentOS Linux 7.9.2009 Core
[2021-12-10 21:45:16,691][mon1][DEBUG ] determining if provided host has same hostname in remote
[2021-12-10 21:45:16,691][mon1][DEBUG ] get remote short hostname
[2021-12-10 21:45:16,692][mon1][DEBUG ] deploying mon to mon1
[2021-12-10 21:45:16,692][mon1][DEBUG ] get remote short hostname
[2021-12-10 21:45:16,693][mon1][DEBUG ] remote hostname: mon1
[2021-12-10 21:45:16,695][mon1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2021-12-10 21:45:16,696][mon1][DEBUG ] create the mon path if it does not exist
[2021-12-10 21:45:16,697][mon1][DEBUG ] checking for done path: /var/lib/ceph/mon/ceph-mon1/done
[2021-12-10 21:45:16,699][mon1][DEBUG ] create a done file to avoid re-doing the mon deployment
[2021-12-10 21:45:16,700][mon1][DEBUG ] create the init path if it does not exist
[2021-12-10 21:45:16,703][mon1][INFO  ] Running command: sudo systemctl enable ceph.target
[2021-12-10 21:45:16,772][mon1][INFO  ] Running command: sudo systemctl enable ceph-mon@mon1
[2021-12-10 21:45:16,841][mon1][INFO  ] Running command: sudo systemctl start ceph-mon@mon1
[2021-12-10 21:45:18,912][mon1][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.mon1.asok mon_status
[2021-12-10 21:45:18,977][mon1][DEBUG ] ********************************************************************************
[2021-12-10 21:45:18,977][mon1][DEBUG ] status for monitor: mon.mon1
[2021-12-10 21:45:18,977][mon1][DEBUG ] {
[2021-12-10 21:45:18,977][mon1][DEBUG ]   "election_epoch": 3, 
[2021-12-10 21:45:18,977][mon1][DEBUG ]   "extra_probe_peers": [], 
[2021-12-10 21:45:18,977][mon1][DEBUG ]   "monmap": {
[2021-12-10 21:45:18,977][mon1][DEBUG ]     "created": "2021-12-10 21:35:26.299692", 
[2021-12-10 21:45:18,977][mon1][DEBUG ]     "epoch": 1, 
[2021-12-10 21:45:18,977][mon1][DEBUG ]     "fsid": "67d9e224-a6f7-43d3-ae36-e6100c59258e", 
[2021-12-10 21:45:18,977][mon1][DEBUG ]     "modified": "2021-12-10 21:35:26.299692", 
[2021-12-10 21:45:18,977][mon1][DEBUG ]     "mons": [
[2021-12-10 21:45:18,977][mon1][DEBUG ]       {
[2021-12-10 21:45:18,977][mon1][DEBUG ]         "addr": "192.168.1.93:6789/0", 
[2021-12-10 21:45:18,977][mon1][DEBUG ]         "name": "mon1", 
[2021-12-10 21:45:18,978][mon1][DEBUG ]         "rank": 0
[2021-12-10 21:45:18,978][mon1][DEBUG ]       }
[2021-12-10 21:45:18,978][mon1][DEBUG ]     ]
[2021-12-10 21:45:18,978][mon1][DEBUG ]   }, 
[2021-12-10 21:45:18,978][mon1][DEBUG ]   "name": "mon1", 
[2021-12-10 21:45:18,978][mon1][DEBUG ]   "outside_quorum": [], 
[2021-12-10 21:45:18,978][mon1][DEBUG ]   "quorum": [
[2021-12-10 21:45:18,978][mon1][DEBUG ]     0
[2021-12-10 21:45:18,978][mon1][DEBUG ]   ], 
[2021-12-10 21:45:18,978][mon1][DEBUG ]   "rank": 0, 
[2021-12-10 21:45:18,978][mon1][DEBUG ]   "state": "leader", 
[2021-12-10 21:45:18,978][mon1][DEBUG ]   "sync_provider": []
[2021-12-10 21:45:18,978][mon1][DEBUG ] }
[2021-12-10 21:45:18,978][mon1][DEBUG ] ********************************************************************************
[2021-12-10 21:45:18,978][mon1][INFO  ] monitor: mon.mon1 is running
[2021-12-10 21:45:18,980][mon1][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.mon1.asok mon_status
[2021-12-10 21:45:19,145][ceph_deploy.mon][INFO  ] processing monitor mon.mon1
[2021-12-10 21:45:21,465][mon1][DEBUG ] connection detected need for sudo
[2021-12-10 21:45:23,262][mon1][DEBUG ] connected to host: mon1 
[2021-12-10 21:45:23,262][mon1][DEBUG ] detect platform information from remote host
[2021-12-10 21:45:23,280][mon1][DEBUG ] detect machine type
[2021-12-10 21:45:23,285][mon1][DEBUG ] find the location of an executable
[2021-12-10 21:45:23,288][mon1][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.mon1.asok mon_status
[2021-12-10 21:45:23,352][ceph_deploy.mon][INFO  ] mon.mon1 monitor has reached quorum!
[2021-12-10 21:45:23,353][ceph_deploy.mon][INFO  ] all initial monitors are running and have formed quorum
[2021-12-10 21:45:23,353][ceph_deploy.mon][INFO  ] Running gatherkeys...
[2021-12-10 21:45:23,353][ceph_deploy.gatherkeys][INFO  ] Storing keys in temp directory /tmp/tmpAPIeE2
[2021-12-10 21:45:25,454][mon1][DEBUG ] connection detected need for sudo
[2021-12-10 21:45:27,507][mon1][DEBUG ] connected to host: mon1 
[2021-12-10 21:45:27,508][mon1][DEBUG ] detect platform information from remote host
[2021-12-10 21:45:27,526][mon1][DEBUG ] detect machine type
[2021-12-10 21:45:27,530][mon1][DEBUG ] get remote short hostname
[2021-12-10 21:45:27,531][mon1][DEBUG ] fetch remote file
[2021-12-10 21:45:27,534][mon1][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --admin-daemon=/var/run/ceph/ceph-mon.mon1.asok mon_status
[2021-12-10 21:45:27,600][mon1][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-mon1/keyring auth get client.admin
[2021-12-10 21:45:27,767][mon1][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-mon1/keyring auth get-or-create client.admin osd allow * mds allow * mon allow * mgr allow *
[2021-12-10 21:45:27,933][mon1][ERROR ] "ceph auth get-or-create for keytype admin returned 22
[2021-12-10 21:45:27,933][mon1][DEBUG ] Error EINVAL: unknown cap type 'mgr'
[2021-12-10 21:45:27,933][mon1][ERROR ] Failed to return 'admin' key from host mon1
[2021-12-10 21:45:27,933][ceph_deploy.gatherkeys][ERROR ] Failed to connect to host:mon1
[2021-12-10 21:45:27,933][ceph_deploy.gatherkeys][INFO  ] Destroy temp directory /tmp/tmpAPIeE2
[2021-12-10 21:45:27,933][ceph_deploy][ERROR ] RuntimeError: Failed to connect any mon

[2021-12-10 21:45:41,486][ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephuser/.cephdeploy.conf
[2021-12-10 21:45:41,486][ceph_deploy.cli][INFO  ] Invoked (1.5.39): /usr/bin/ceph-deploy install mon1
[2021-12-10 21:45:41,486][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2021-12-10 21:45:41,486][ceph_deploy.cli][INFO  ]  verbose                       : False
[2021-12-10 21:45:41,486][ceph_deploy.cli][INFO  ]  testing                       : None
[2021-12-10 21:45:41,487][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f8d59a54200>
[2021-12-10 21:45:41,487][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2021-12-10 21:45:41,487][ceph_deploy.cli][INFO  ]  dev_commit                    : None
[2021-12-10 21:45:41,487][ceph_deploy.cli][INFO  ]  install_mds                   : False
[2021-12-10 21:45:41,487][ceph_deploy.cli][INFO  ]  stable                        : None
[2021-12-10 21:45:41,487][ceph_deploy.cli][INFO  ]  default_release               : False
[2021-12-10 21:45:41,487][ceph_deploy.cli][INFO  ]  username                      : None
[2021-12-10 21:45:41,487][ceph_deploy.cli][INFO  ]  adjust_repos                  : True
[2021-12-10 21:45:41,487][ceph_deploy.cli][INFO  ]  func                          : <function install at 0x7f8d5a718de8>
[2021-12-10 21:45:41,487][ceph_deploy.cli][INFO  ]  install_mgr                   : False
[2021-12-10 21:45:41,487][ceph_deploy.cli][INFO  ]  install_all                   : False
[2021-12-10 21:45:41,487][ceph_deploy.cli][INFO  ]  repo                          : False
[2021-12-10 21:45:41,487][ceph_deploy.cli][INFO  ]  host                          : ['mon1']
[2021-12-10 21:45:41,487][ceph_deploy.cli][INFO  ]  install_rgw                   : False
[2021-12-10 21:45:41,487][ceph_deploy.cli][INFO  ]  install_tests                 : False
[2021-12-10 21:45:41,487][ceph_deploy.cli][INFO  ]  repo_url                      : None
[2021-12-10 21:45:41,487][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2021-12-10 21:45:41,487][ceph_deploy.cli][INFO  ]  install_osd                   : False
[2021-12-10 21:45:41,487][ceph_deploy.cli][INFO  ]  version_kind                  : stable
[2021-12-10 21:45:41,487][ceph_deploy.cli][INFO  ]  install_common                : False
[2021-12-10 21:45:41,487][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2021-12-10 21:45:41,488][ceph_deploy.cli][INFO  ]  quiet                         : False
[2021-12-10 21:45:41,488][ceph_deploy.cli][INFO  ]  dev                           : master
[2021-12-10 21:45:41,488][ceph_deploy.cli][INFO  ]  nogpgcheck                    : False
[2021-12-10 21:45:41,488][ceph_deploy.cli][INFO  ]  local_mirror                  : None
[2021-12-10 21:45:41,488][ceph_deploy.cli][INFO  ]  release                       : None
[2021-12-10 21:45:41,488][ceph_deploy.cli][INFO  ]  install_mon                   : False
[2021-12-10 21:45:41,489][ceph_deploy.cli][INFO  ]  gpg_url                       : None
[2021-12-10 21:45:41,489][ceph_deploy.install][DEBUG ] Installing stable version jewel on cluster ceph hosts mon1
[2021-12-10 21:45:41,489][ceph_deploy.install][DEBUG ] Detecting platform for host mon1 ...
[2021-12-10 21:45:41,656][mon1][DEBUG ] connection detected need for sudo
[2021-12-10 21:45:41,823][mon1][DEBUG ] connected to host: mon1 
[2021-12-10 21:45:41,823][mon1][DEBUG ] detect platform information from remote host
[2021-12-10 21:45:41,841][mon1][DEBUG ] detect machine type
[2021-12-10 21:45:41,846][ceph_deploy.install][INFO  ] Distro info: CentOS Linux 7.9.2009 Core
[2021-12-10 21:45:41,846][mon1][INFO  ] installing Ceph on mon1
[2021-12-10 21:45:41,848][mon1][INFO  ] Running command: sudo yum clean all
[2021-12-10 21:45:43,070][mon1][DEBUG ] Loaded plugins: fastestmirror, priorities
[2021-12-10 21:45:43,134][mon1][DEBUG ] Cleaning repos: Ceph Ceph-noarch base ceph-source epel extras updates
[2021-12-10 21:45:43,141][mon1][DEBUG ] Cleaning up list of fastest mirrors
[2021-12-10 21:45:43,175][mon1][INFO  ] Running command: sudo yum -y install epel-release
[2021-12-10 21:45:43,291][mon1][DEBUG ] Loaded plugins: fastestmirror, priorities
[2021-12-10 21:45:43,356][mon1][DEBUG ] Determining fastest mirrors
[2021-12-10 21:46:20,534][mon1][DEBUG ]  * base: mirror.arizona.edu
[2021-12-10 21:46:20,534][mon1][DEBUG ]  * epel: pubmirror1.math.uh.edu
[2021-12-10 21:46:20,534][mon1][DEBUG ]  * extras: centos.mirror.lstn.net
[2021-12-10 21:46:20,534][mon1][DEBUG ]  * updates: bay.uchicago.edu
[2021-12-10 21:47:13,299][mon1][DEBUG ] 12 packages excluded due to repository priority protections
[2021-12-10 21:47:15,150][mon1][DEBUG ] Resolving Dependencies
[2021-12-10 21:47:15,150][mon1][DEBUG ] --> Running transaction check
[2021-12-10 21:47:15,150][mon1][DEBUG ] ---> Package epel-release.noarch 0:7-11 will be updated
[2021-12-10 21:47:15,214][mon1][DEBUG ] ---> Package epel-release.noarch 0:7-14 will be an update
[2021-12-10 21:47:16,682][mon1][DEBUG ] --> Finished Dependency Resolution
[2021-12-10 21:47:16,682][mon1][DEBUG ] 
[2021-12-10 21:47:16,682][mon1][DEBUG ] Dependencies Resolved
[2021-12-10 21:47:16,682][mon1][DEBUG ] 
[2021-12-10 21:47:16,682][mon1][DEBUG ] ================================================================================
[2021-12-10 21:47:16,682][mon1][DEBUG ]  Package                Arch             Version           Repository      Size
[2021-12-10 21:47:16,683][mon1][DEBUG ] ================================================================================
[2021-12-10 21:47:16,683][mon1][DEBUG ] Updating:
[2021-12-10 21:47:16,683][mon1][DEBUG ]  epel-release           noarch           7-14              epel            15 k
[2021-12-10 21:47:16,683][mon1][DEBUG ] 
[2021-12-10 21:47:16,683][mon1][DEBUG ] Transaction Summary
[2021-12-10 21:47:16,683][mon1][DEBUG ] ================================================================================
[2021-12-10 21:47:16,683][mon1][DEBUG ] Upgrade  1 Package
[2021-12-10 21:47:16,683][mon1][DEBUG ] 
[2021-12-10 21:47:16,683][mon1][DEBUG ] Total download size: 15 k
[2021-12-10 21:47:16,683][mon1][DEBUG ] Downloading packages:
[2021-12-10 21:47:16,683][mon1][DEBUG ] Delta RPMs disabled because /usr/bin/applydeltarpm not installed.
[2021-12-10 21:47:22,413][mon1][DEBUG ] Running transaction check
[2021-12-10 21:47:22,414][mon1][DEBUG ] Running transaction test
[2021-12-10 21:47:22,477][mon1][DEBUG ] Transaction test succeeded
[2021-12-10 21:47:22,478][mon1][DEBUG ] Running transaction
[2021-12-10 21:47:23,895][mon1][DEBUG ]   Updating   : epel-release-7-14.noarch                                     1/2 
[2021-12-10 21:47:24,210][mon1][DEBUG ]   Cleanup    : epel-release-7-11.noarch                                     2/2 
[2021-12-10 21:47:24,375][mon1][DEBUG ]   Verifying  : epel-release-7-14.noarch                                     1/2 
[2021-12-10 21:47:24,689][mon1][DEBUG ]   Verifying  : epel-release-7-11.noarch                                     2/2 
[2021-12-10 21:47:24,690][mon1][DEBUG ] 
[2021-12-10 21:47:24,690][mon1][DEBUG ] Updated:
[2021-12-10 21:47:24,690][mon1][DEBUG ]   epel-release.noarch 0:7-14                                                    
[2021-12-10 21:47:24,690][mon1][DEBUG ] 
[2021-12-10 21:47:24,690][mon1][DEBUG ] Complete!
[2021-12-10 21:47:25,207][mon1][INFO  ] Running command: sudo yum -y install yum-plugin-priorities
[2021-12-10 21:47:25,324][mon1][DEBUG ] Loaded plugins: fastestmirror, priorities
[2021-12-10 21:47:25,388][mon1][DEBUG ] Loading mirror speeds from cached hostfile
[2021-12-10 21:47:25,388][mon1][DEBUG ]  * base: mirror.arizona.edu
[2021-12-10 21:47:25,388][mon1][DEBUG ]  * epel: pubmirror1.math.uh.edu
[2021-12-10 21:47:25,388][mon1][DEBUG ]  * extras: centos.mirror.lstn.net
[2021-12-10 21:47:25,388][mon1][DEBUG ]  * updates: bay.uchicago.edu
[2021-12-10 21:47:26,104][mon1][DEBUG ] 12 packages excluded due to repository priority protections
[2021-12-10 21:47:26,569][mon1][DEBUG ] Package yum-plugin-priorities-1.1.31-54.el7_8.noarch already installed and latest version
[2021-12-10 21:47:26,569][mon1][DEBUG ] Nothing to do
[2021-12-10 21:47:26,684][mon1][DEBUG ] Configure Yum priorities to include obsoletes
[2021-12-10 21:47:26,686][mon1][WARNING] check_obsoletes has been enabled for Yum priorities plugin
[2021-12-10 21:47:26,687][mon1][INFO  ] Running command: sudo rpm --import https://download.ceph.com/keys/release.asc
[2021-12-10 21:47:32,672][mon1][INFO  ] Running command: sudo yum remove -y ceph-release
[2021-12-10 21:47:32,789][mon1][DEBUG ] Loaded plugins: fastestmirror, priorities
[2021-12-10 21:47:32,903][mon1][DEBUG ] Resolving Dependencies
[2021-12-10 21:47:32,903][mon1][DEBUG ] --> Running transaction check
[2021-12-10 21:47:32,903][mon1][DEBUG ] ---> Package ceph-release.noarch 0:1-1.el7 will be erased
[2021-12-10 21:47:33,168][mon1][DEBUG ] --> Finished Dependency Resolution
[2021-12-10 21:47:33,232][mon1][DEBUG ] 
[2021-12-10 21:47:33,232][mon1][DEBUG ] Dependencies Resolved
[2021-12-10 21:47:33,232][mon1][DEBUG ] 
[2021-12-10 21:47:33,232][mon1][DEBUG ] ================================================================================
[2021-12-10 21:47:33,232][mon1][DEBUG ]  Package         Arch      Version       Repository                        Size
[2021-12-10 21:47:33,232][mon1][DEBUG ] ================================================================================
[2021-12-10 21:47:33,232][mon1][DEBUG ] Removing:
[2021-12-10 21:47:33,232][mon1][DEBUG ]  ceph-release    noarch    1-1.el7       @/ceph-release-1-0.el7.noarch    535  
[2021-12-10 21:47:33,232][mon1][DEBUG ] 
[2021-12-10 21:47:33,232][mon1][DEBUG ] Transaction Summary
[2021-12-10 21:47:33,232][mon1][DEBUG ] ================================================================================
[2021-12-10 21:47:33,232][mon1][DEBUG ] Remove  1 Package
[2021-12-10 21:47:33,232][mon1][DEBUG ] 
[2021-12-10 21:47:33,232][mon1][DEBUG ] Installed size: 535  
[2021-12-10 21:47:33,232][mon1][DEBUG ] Downloading packages:
[2021-12-10 21:47:33,232][mon1][DEBUG ] Running transaction check
[2021-12-10 21:47:33,232][mon1][DEBUG ] Running transaction test
[2021-12-10 21:47:33,233][mon1][DEBUG ] Transaction test succeeded
[2021-12-10 21:47:33,233][mon1][DEBUG ] Running transaction
[2021-12-10 21:47:33,547][mon1][DEBUG ]   Erasing    : ceph-release-1-1.el7.noarch                                  1/1 
[2021-12-10 21:47:33,547][mon1][DEBUG ] warning: /etc/yum.repos.d/ceph.repo saved as /etc/yum.repos.d/ceph.repo.rpmsave
[2021-12-10 21:47:34,263][mon1][DEBUG ]   Verifying  : ceph-release-1-1.el7.noarch                                  1/1 
[2021-12-10 21:47:34,263][mon1][DEBUG ] 
[2021-12-10 21:47:34,263][mon1][DEBUG ] Removed:
[2021-12-10 21:47:34,263][mon1][DEBUG ]   ceph-release.noarch 0:1-1.el7                                                 
[2021-12-10 21:47:34,264][mon1][DEBUG ] 
[2021-12-10 21:47:34,264][mon1][DEBUG ] Complete!
[2021-12-10 21:47:34,297][mon1][INFO  ] Running command: sudo yum install -y https://download.ceph.com/rpm-jewel/el7/noarch/ceph-release-1-0.el7.noarch.rpm
[2021-12-10 21:47:34,414][mon1][DEBUG ] Loaded plugins: fastestmirror, priorities
[2021-12-10 21:47:40,297][mon1][DEBUG ] Examining /var/tmp/yum-root-frbOM9/ceph-release-1-0.el7.noarch.rpm: ceph-release-1-1.el7.noarch
[2021-12-10 21:47:40,297][mon1][DEBUG ] Marking /var/tmp/yum-root-frbOM9/ceph-release-1-0.el7.noarch.rpm to be installed
[2021-12-10 21:47:40,297][mon1][DEBUG ] Resolving Dependencies
[2021-12-10 21:47:40,297][mon1][DEBUG ] --> Running transaction check
[2021-12-10 21:47:40,297][mon1][DEBUG ] ---> Package ceph-release.noarch 0:1-1.el7 will be installed
[2021-12-10 21:47:40,561][mon1][DEBUG ] --> Finished Dependency Resolution
[2021-12-10 21:47:40,676][mon1][DEBUG ] 
[2021-12-10 21:47:40,676][mon1][DEBUG ] Dependencies Resolved
[2021-12-10 21:47:40,676][mon1][DEBUG ] 
[2021-12-10 21:47:40,676][mon1][DEBUG ] ================================================================================
[2021-12-10 21:47:40,676][mon1][DEBUG ]  Package          Arch       Version     Repository                        Size
[2021-12-10 21:47:40,677][mon1][DEBUG ] ================================================================================
[2021-12-10 21:47:40,677][mon1][DEBUG ] Installing:
[2021-12-10 21:47:40,677][mon1][DEBUG ]  ceph-release     noarch     1-1.el7     /ceph-release-1-0.el7.noarch     535  
[2021-12-10 21:47:40,677][mon1][DEBUG ] 
[2021-12-10 21:47:40,677][mon1][DEBUG ] Transaction Summary
[2021-12-10 21:47:40,677][mon1][DEBUG ] ================================================================================
[2021-12-10 21:47:40,677][mon1][DEBUG ] Install  1 Package
[2021-12-10 21:47:40,677][mon1][DEBUG ] 
[2021-12-10 21:47:40,677][mon1][DEBUG ] Total size: 535  
[2021-12-10 21:47:40,677][mon1][DEBUG ] Installed size: 535  
[2021-12-10 21:47:40,677][mon1][DEBUG ] Downloading packages:
[2021-12-10 21:47:40,677][mon1][DEBUG ] Running transaction check
[2021-12-10 21:47:40,677][mon1][DEBUG ] Running transaction test
[2021-12-10 21:47:40,678][mon1][DEBUG ] Transaction test succeeded
[2021-12-10 21:47:40,678][mon1][DEBUG ] Running transaction
[2021-12-10 21:47:41,243][mon1][DEBUG ]   Installing : ceph-release-1-1.el7.noarch                                  1/1 
[2021-12-10 21:47:41,659][mon1][DEBUG ]   Verifying  : ceph-release-1-1.el7.noarch                                  1/1 
[2021-12-10 21:47:41,659][mon1][DEBUG ] 
[2021-12-10 21:47:41,659][mon1][DEBUG ] Installed:
[2021-12-10 21:47:41,660][mon1][DEBUG ]   ceph-release.noarch 0:1-1.el7                                                 
[2021-12-10 21:47:41,660][mon1][DEBUG ] 
[2021-12-10 21:47:41,660][mon1][DEBUG ] Complete!
[2021-12-10 21:47:41,660][mon1][WARNING] ensuring that /etc/yum.repos.d/ceph.repo contains a high priority
[2021-12-10 21:47:41,662][mon1][WARNING] altered ceph.repo priorities to contain: priority=1
[2021-12-10 21:47:41,664][mon1][INFO  ] Running command: sudo yum -y install ceph ceph-radosgw
[2021-12-10 21:47:41,780][mon1][DEBUG ] Loaded plugins: fastestmirror, priorities
[2021-12-10 21:47:41,844][mon1][DEBUG ] Loading mirror speeds from cached hostfile
[2021-12-10 21:47:41,844][mon1][DEBUG ]  * base: mirror.arizona.edu
[2021-12-10 21:47:41,844][mon1][DEBUG ]  * epel: pubmirror1.math.uh.edu
[2021-12-10 21:47:41,844][mon1][DEBUG ]  * extras: centos.mirror.lstn.net
[2021-12-10 21:47:41,844][mon1][DEBUG ]  * updates: bay.uchicago.edu
[2021-12-10 21:47:48,327][mon1][DEBUG ] 12 packages excluded due to repository priority protections
[2021-12-10 21:47:48,793][mon1][DEBUG ] Package 2:ceph-10.2.11-0.el7.x86_64 already installed and latest version
[2021-12-10 21:47:49,108][mon1][DEBUG ] Package 2:ceph-radosgw-10.2.11-0.el7.x86_64 already installed and latest version
[2021-12-10 21:47:49,108][mon1][DEBUG ] Nothing to do
[2021-12-10 21:47:49,141][mon1][INFO  ] Running command: sudo ceph --version
[2021-12-10 21:47:49,208][mon1][DEBUG ] ceph version 10.2.11 (e4b061b47f07f583c92a050d9e84b1813a35671e)
[2021-12-10 21:47:57,523][ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephuser/.cephdeploy.conf
[2021-12-10 21:47:57,523][ceph_deploy.cli][INFO  ] Invoked (1.5.39): /usr/bin/ceph-deploy mon create-initial
[2021-12-10 21:47:57,524][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2021-12-10 21:47:57,524][ceph_deploy.cli][INFO  ]  username                      : None
[2021-12-10 21:47:57,524][ceph_deploy.cli][INFO  ]  verbose                       : False
[2021-12-10 21:47:57,524][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2021-12-10 21:47:57,524][ceph_deploy.cli][INFO  ]  subcommand                    : create-initial
[2021-12-10 21:47:57,524][ceph_deploy.cli][INFO  ]  quiet                         : False
[2021-12-10 21:47:57,524][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7ff91a581f38>
[2021-12-10 21:47:57,524][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2021-12-10 21:47:57,524][ceph_deploy.cli][INFO  ]  func                          : <function mon at 0x7ff91a568758>
[2021-12-10 21:47:57,524][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2021-12-10 21:47:57,524][ceph_deploy.cli][INFO  ]  default_release               : False
[2021-12-10 21:47:57,524][ceph_deploy.cli][INFO  ]  keyrings                      : None
[2021-12-10 21:47:57,525][ceph_deploy.mon][DEBUG ] Deploying mon, cluster ceph hosts mon1
[2021-12-10 21:47:57,525][ceph_deploy.mon][DEBUG ] detecting platform for host mon1 ...
[2021-12-10 21:47:57,696][mon1][DEBUG ] connection detected need for sudo
[2021-12-10 21:47:57,869][mon1][DEBUG ] connected to host: mon1 
[2021-12-10 21:47:57,870][mon1][DEBUG ] detect platform information from remote host
[2021-12-10 21:47:57,889][mon1][DEBUG ] detect machine type
[2021-12-10 21:47:57,895][mon1][DEBUG ] find the location of an executable
[2021-12-10 21:47:57,896][ceph_deploy.mon][INFO  ] distro info: CentOS Linux 7.9.2009 Core
[2021-12-10 21:47:57,896][mon1][DEBUG ] determining if provided host has same hostname in remote
[2021-12-10 21:47:57,896][mon1][DEBUG ] get remote short hostname
[2021-12-10 21:47:57,897][mon1][DEBUG ] deploying mon to mon1
[2021-12-10 21:47:57,897][mon1][DEBUG ] get remote short hostname
[2021-12-10 21:47:57,898][mon1][DEBUG ] remote hostname: mon1
[2021-12-10 21:47:57,900][mon1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2021-12-10 21:47:57,902][mon1][DEBUG ] create the mon path if it does not exist
[2021-12-10 21:47:57,903][mon1][DEBUG ] checking for done path: /var/lib/ceph/mon/ceph-mon1/done
[2021-12-10 21:47:57,904][mon1][DEBUG ] create a done file to avoid re-doing the mon deployment
[2021-12-10 21:47:57,905][mon1][DEBUG ] create the init path if it does not exist
[2021-12-10 21:47:57,907][mon1][INFO  ] Running command: sudo systemctl enable ceph.target
[2021-12-10 21:47:57,978][mon1][INFO  ] Running command: sudo systemctl enable ceph-mon@mon1
[2021-12-10 21:47:58,046][mon1][INFO  ] Running command: sudo systemctl start ceph-mon@mon1
[2021-12-10 21:48:00,116][mon1][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.mon1.asok mon_status
[2021-12-10 21:48:00,181][mon1][DEBUG ] ********************************************************************************
[2021-12-10 21:48:00,181][mon1][DEBUG ] status for monitor: mon.mon1
[2021-12-10 21:48:00,181][mon1][DEBUG ] {
[2021-12-10 21:48:00,182][mon1][DEBUG ]   "election_epoch": 3, 
[2021-12-10 21:48:00,182][mon1][DEBUG ]   "extra_probe_peers": [], 
[2021-12-10 21:48:00,182][mon1][DEBUG ]   "monmap": {
[2021-12-10 21:48:00,182][mon1][DEBUG ]     "created": "2021-12-10 21:35:26.299692", 
[2021-12-10 21:48:00,182][mon1][DEBUG ]     "epoch": 1, 
[2021-12-10 21:48:00,182][mon1][DEBUG ]     "fsid": "67d9e224-a6f7-43d3-ae36-e6100c59258e", 
[2021-12-10 21:48:00,182][mon1][DEBUG ]     "modified": "2021-12-10 21:35:26.299692", 
[2021-12-10 21:48:00,182][mon1][DEBUG ]     "mons": [
[2021-12-10 21:48:00,182][mon1][DEBUG ]       {
[2021-12-10 21:48:00,182][mon1][DEBUG ]         "addr": "192.168.1.93:6789/0", 
[2021-12-10 21:48:00,182][mon1][DEBUG ]         "name": "mon1", 
[2021-12-10 21:48:00,182][mon1][DEBUG ]         "rank": 0
[2021-12-10 21:48:00,182][mon1][DEBUG ]       }
[2021-12-10 21:48:00,182][mon1][DEBUG ]     ]
[2021-12-10 21:48:00,182][mon1][DEBUG ]   }, 
[2021-12-10 21:48:00,182][mon1][DEBUG ]   "name": "mon1", 
[2021-12-10 21:48:00,182][mon1][DEBUG ]   "outside_quorum": [], 
[2021-12-10 21:48:00,182][mon1][DEBUG ]   "quorum": [
[2021-12-10 21:48:00,182][mon1][DEBUG ]     0
[2021-12-10 21:48:00,182][mon1][DEBUG ]   ], 
[2021-12-10 21:48:00,182][mon1][DEBUG ]   "rank": 0, 
[2021-12-10 21:48:00,183][mon1][DEBUG ]   "state": "leader", 
[2021-12-10 21:48:00,183][mon1][DEBUG ]   "sync_provider": []
[2021-12-10 21:48:00,183][mon1][DEBUG ] }
[2021-12-10 21:48:00,183][mon1][DEBUG ] ********************************************************************************
[2021-12-10 21:48:00,183][mon1][INFO  ] monitor: mon.mon1 is running
[2021-12-10 21:48:00,184][mon1][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.mon1.asok mon_status
[2021-12-10 21:48:00,400][ceph_deploy.mon][INFO  ] processing monitor mon.mon1
[2021-12-10 21:48:00,563][mon1][DEBUG ] connection detected need for sudo
[2021-12-10 21:48:00,730][mon1][DEBUG ] connected to host: mon1 
[2021-12-10 21:48:00,731][mon1][DEBUG ] detect platform information from remote host
[2021-12-10 21:48:00,748][mon1][DEBUG ] detect machine type
[2021-12-10 21:48:00,754][mon1][DEBUG ] find the location of an executable
[2021-12-10 21:48:00,757][mon1][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.mon1.asok mon_status
[2021-12-10 21:48:00,872][ceph_deploy.mon][INFO  ] mon.mon1 monitor has reached quorum!
[2021-12-10 21:48:00,873][ceph_deploy.mon][INFO  ] all initial monitors are running and have formed quorum
[2021-12-10 21:48:00,873][ceph_deploy.mon][INFO  ] Running gatherkeys...
[2021-12-10 21:48:00,873][ceph_deploy.gatherkeys][INFO  ] Storing keys in temp directory /tmp/tmpWsj5vT
[2021-12-10 21:48:01,034][mon1][DEBUG ] connection detected need for sudo
[2021-12-10 21:48:01,199][mon1][DEBUG ] connected to host: mon1 
[2021-12-10 21:48:01,200][mon1][DEBUG ] detect platform information from remote host
[2021-12-10 21:48:01,217][mon1][DEBUG ] detect machine type
[2021-12-10 21:48:01,222][mon1][DEBUG ] get remote short hostname
[2021-12-10 21:48:01,223][mon1][DEBUG ] fetch remote file
[2021-12-10 21:48:01,225][mon1][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --admin-daemon=/var/run/ceph/ceph-mon.mon1.asok mon_status
[2021-12-10 21:48:01,291][mon1][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-mon1/keyring auth get client.admin
[2021-12-10 21:48:01,458][mon1][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-mon1/keyring auth get-or-create client.admin osd allow * mds allow * mon allow * mgr allow *
[2021-12-10 21:48:01,623][mon1][ERROR ] "ceph auth get-or-create for keytype admin returned 22
[2021-12-10 21:48:01,623][mon1][DEBUG ] Error EINVAL: unknown cap type 'mgr'
[2021-12-10 21:48:01,623][mon1][ERROR ] Failed to return 'admin' key from host mon1
[2021-12-10 21:48:01,623][ceph_deploy.gatherkeys][ERROR ] Failed to connect to host:mon1
[2021-12-10 21:48:01,623][ceph_deploy.gatherkeys][INFO  ] Destroy temp directory /tmp/tmpWsj5vT
[2021-12-10 21:48:01,623][ceph_deploy][ERROR ] RuntimeError: Failed to connect any mon

[2021-12-10 22:06:20,828][ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephuser/.cephdeploy.conf
[2021-12-10 22:06:20,881][ceph_deploy.cli][INFO  ] Invoked (1.5.39): /usr/bin/ceph-deploy new mon1
[2021-12-10 22:06:20,881][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2021-12-10 22:06:20,881][ceph_deploy.cli][INFO  ]  username                      : None
[2021-12-10 22:06:20,881][ceph_deploy.cli][INFO  ]  func                          : <function new at 0x7fc8bd58e668>
[2021-12-10 22:06:20,881][ceph_deploy.cli][INFO  ]  verbose                       : False
[2021-12-10 22:06:20,881][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2021-12-10 22:06:20,881][ceph_deploy.cli][INFO  ]  quiet                         : False
[2021-12-10 22:06:20,881][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fc8bcd00f38>
[2021-12-10 22:06:20,881][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2021-12-10 22:06:20,881][ceph_deploy.cli][INFO  ]  ssh_copykey                   : True
[2021-12-10 22:06:20,881][ceph_deploy.cli][INFO  ]  mon                           : ['mon1']
[2021-12-10 22:06:20,881][ceph_deploy.cli][INFO  ]  public_network                : None
[2021-12-10 22:06:20,881][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2021-12-10 22:06:20,882][ceph_deploy.cli][INFO  ]  cluster_network               : None
[2021-12-10 22:06:20,882][ceph_deploy.cli][INFO  ]  default_release               : False
[2021-12-10 22:06:20,882][ceph_deploy.cli][INFO  ]  fsid                          : None
[2021-12-10 22:06:20,882][ceph_deploy.new][DEBUG ] Creating new cluster named ceph
[2021-12-10 22:06:20,882][ceph_deploy.new][INFO  ] making sure passwordless SSH succeeds
[2021-12-10 22:06:20,908][mon1][DEBUG ] connected to host: ceph-admin 
[2021-12-10 22:06:20,914][mon1][INFO  ] Running command: ssh -CT -o BatchMode=yes mon1
[2021-12-10 22:06:21,240][mon1][DEBUG ] connection detected need for sudo
[2021-12-10 22:06:21,406][mon1][DEBUG ] connected to host: mon1 
[2021-12-10 22:06:21,407][mon1][DEBUG ] detect platform information from remote host
[2021-12-10 22:06:21,424][mon1][DEBUG ] detect machine type
[2021-12-10 22:06:21,429][mon1][DEBUG ] find the location of an executable
[2021-12-10 22:06:21,432][mon1][INFO  ] Running command: sudo /usr/sbin/ip link show
[2021-12-10 22:06:21,442][mon1][INFO  ] Running command: sudo /usr/sbin/ip addr show
[2021-12-10 22:06:21,451][mon1][DEBUG ] IP addresses found: [u'192.168.1.93']
[2021-12-10 22:06:21,451][ceph_deploy.new][DEBUG ] Resolving host mon1
[2021-12-10 22:06:21,451][ceph_deploy.new][DEBUG ] Monitor mon1 at 192.168.1.93
[2021-12-10 22:06:21,451][ceph_deploy.new][DEBUG ] Monitor initial members are ['mon1']
[2021-12-10 22:06:21,451][ceph_deploy.new][DEBUG ] Monitor addrs are ['192.168.1.93']
[2021-12-10 22:06:21,451][ceph_deploy.new][DEBUG ] Creating a random mon key...
[2021-12-10 22:06:21,451][ceph_deploy.new][DEBUG ] Writing monitor keyring to ceph.mon.keyring...
[2021-12-10 22:06:21,451][ceph_deploy.new][DEBUG ] Writing initial config to ceph.conf...
[2021-12-10 22:07:15,995][ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephuser/.cephdeploy.conf
[2021-12-10 22:07:15,995][ceph_deploy.cli][INFO  ] Invoked (1.5.39): /usr/bin/ceph-deploy mon create-initial
[2021-12-10 22:07:15,995][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2021-12-10 22:07:15,995][ceph_deploy.cli][INFO  ]  username                      : None
[2021-12-10 22:07:15,995][ceph_deploy.cli][INFO  ]  verbose                       : False
[2021-12-10 22:07:15,996][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2021-12-10 22:07:15,996][ceph_deploy.cli][INFO  ]  subcommand                    : create-initial
[2021-12-10 22:07:15,996][ceph_deploy.cli][INFO  ]  quiet                         : False
[2021-12-10 22:07:15,996][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7ffa81afef38>
[2021-12-10 22:07:15,996][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2021-12-10 22:07:15,996][ceph_deploy.cli][INFO  ]  func                          : <function mon at 0x7ffa81ae5758>
[2021-12-10 22:07:15,996][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2021-12-10 22:07:15,996][ceph_deploy.cli][INFO  ]  default_release               : False
[2021-12-10 22:07:15,996][ceph_deploy.cli][INFO  ]  keyrings                      : None
[2021-12-10 22:07:15,996][ceph_deploy.mon][DEBUG ] Deploying mon, cluster ceph hosts mon1
[2021-12-10 22:07:15,996][ceph_deploy.mon][DEBUG ] detecting platform for host mon1 ...
[2021-12-10 22:07:16,162][mon1][DEBUG ] connection detected need for sudo
[2021-12-10 22:07:16,328][mon1][DEBUG ] connected to host: mon1 
[2021-12-10 22:07:16,329][mon1][DEBUG ] detect platform information from remote host
[2021-12-10 22:07:16,347][mon1][DEBUG ] detect machine type
[2021-12-10 22:07:16,352][mon1][DEBUG ] find the location of an executable
[2021-12-10 22:07:16,353][ceph_deploy.mon][INFO  ] distro info: CentOS Linux 7.9.2009 Core
[2021-12-10 22:07:16,354][mon1][DEBUG ] determining if provided host has same hostname in remote
[2021-12-10 22:07:16,354][mon1][DEBUG ] get remote short hostname
[2021-12-10 22:07:16,355][mon1][DEBUG ] deploying mon to mon1
[2021-12-10 22:07:16,355][mon1][DEBUG ] get remote short hostname
[2021-12-10 22:07:16,356][mon1][DEBUG ] remote hostname: mon1
[2021-12-10 22:07:16,358][mon1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2021-12-10 22:07:16,360][ceph_deploy.mon][ERROR ] RuntimeError: config file /etc/ceph/ceph.conf exists with different content; use --overwrite-conf to overwrite
[2021-12-10 22:07:16,360][ceph_deploy][ERROR ] GenericError: Failed to create 1 monitors

[2021-12-10 22:07:18,271][ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephuser/.cephdeploy.conf
[2021-12-10 22:07:18,271][ceph_deploy.cli][INFO  ] Invoked (1.5.39): /usr/bin/ceph-deploy mon create-initial
[2021-12-10 22:07:18,271][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2021-12-10 22:07:18,271][ceph_deploy.cli][INFO  ]  username                      : None
[2021-12-10 22:07:18,271][ceph_deploy.cli][INFO  ]  verbose                       : False
[2021-12-10 22:07:18,271][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2021-12-10 22:07:18,271][ceph_deploy.cli][INFO  ]  subcommand                    : create-initial
[2021-12-10 22:07:18,271][ceph_deploy.cli][INFO  ]  quiet                         : False
[2021-12-10 22:07:18,271][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f76c54c6f38>
[2021-12-10 22:07:18,272][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2021-12-10 22:07:18,272][ceph_deploy.cli][INFO  ]  func                          : <function mon at 0x7f76c54ad758>
[2021-12-10 22:07:18,272][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2021-12-10 22:07:18,272][ceph_deploy.cli][INFO  ]  default_release               : False
[2021-12-10 22:07:18,272][ceph_deploy.cli][INFO  ]  keyrings                      : None
[2021-12-10 22:07:18,272][ceph_deploy.mon][DEBUG ] Deploying mon, cluster ceph hosts mon1
[2021-12-10 22:07:18,272][ceph_deploy.mon][DEBUG ] detecting platform for host mon1 ...
[2021-12-10 22:07:18,436][mon1][DEBUG ] connection detected need for sudo
[2021-12-10 22:07:18,600][mon1][DEBUG ] connected to host: mon1 
[2021-12-10 22:07:18,600][mon1][DEBUG ] detect platform information from remote host
[2021-12-10 22:07:18,618][mon1][DEBUG ] detect machine type
[2021-12-10 22:07:18,623][mon1][DEBUG ] find the location of an executable
[2021-12-10 22:07:18,624][ceph_deploy.mon][INFO  ] distro info: CentOS Linux 7.9.2009 Core
[2021-12-10 22:07:18,624][mon1][DEBUG ] determining if provided host has same hostname in remote
[2021-12-10 22:07:18,624][mon1][DEBUG ] get remote short hostname
[2021-12-10 22:07:18,625][mon1][DEBUG ] deploying mon to mon1
[2021-12-10 22:07:18,625][mon1][DEBUG ] get remote short hostname
[2021-12-10 22:07:18,626][mon1][DEBUG ] remote hostname: mon1
[2021-12-10 22:07:18,628][mon1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2021-12-10 22:07:18,630][ceph_deploy.mon][ERROR ] RuntimeError: config file /etc/ceph/ceph.conf exists with different content; use --overwrite-conf to overwrite
[2021-12-10 22:07:18,630][ceph_deploy][ERROR ] GenericError: Failed to create 1 monitors

[2021-12-10 22:07:19,409][ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephuser/.cephdeploy.conf
[2021-12-10 22:07:19,409][ceph_deploy.cli][INFO  ] Invoked (1.5.39): /usr/bin/ceph-deploy mon create-initial
[2021-12-10 22:07:19,409][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2021-12-10 22:07:19,409][ceph_deploy.cli][INFO  ]  username                      : None
[2021-12-10 22:07:19,409][ceph_deploy.cli][INFO  ]  verbose                       : False
[2021-12-10 22:07:19,409][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2021-12-10 22:07:19,409][ceph_deploy.cli][INFO  ]  subcommand                    : create-initial
[2021-12-10 22:07:19,409][ceph_deploy.cli][INFO  ]  quiet                         : False
[2021-12-10 22:07:19,409][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f213c7c7f38>
[2021-12-10 22:07:19,409][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2021-12-10 22:07:19,409][ceph_deploy.cli][INFO  ]  func                          : <function mon at 0x7f213c7ae758>
[2021-12-10 22:07:19,409][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2021-12-10 22:07:19,409][ceph_deploy.cli][INFO  ]  default_release               : False
[2021-12-10 22:07:19,410][ceph_deploy.cli][INFO  ]  keyrings                      : None
[2021-12-10 22:07:19,410][ceph_deploy.mon][DEBUG ] Deploying mon, cluster ceph hosts mon1
[2021-12-10 22:07:19,410][ceph_deploy.mon][DEBUG ] detecting platform for host mon1 ...
[2021-12-10 22:07:19,581][mon1][DEBUG ] connection detected need for sudo
[2021-12-10 22:07:19,751][mon1][DEBUG ] connected to host: mon1 
[2021-12-10 22:07:19,751][mon1][DEBUG ] detect platform information from remote host
[2021-12-10 22:07:19,770][mon1][DEBUG ] detect machine type
[2021-12-10 22:07:19,775][mon1][DEBUG ] find the location of an executable
[2021-12-10 22:07:19,777][ceph_deploy.mon][INFO  ] distro info: CentOS Linux 7.9.2009 Core
[2021-12-10 22:07:19,777][mon1][DEBUG ] determining if provided host has same hostname in remote
[2021-12-10 22:07:19,777][mon1][DEBUG ] get remote short hostname
[2021-12-10 22:07:19,778][mon1][DEBUG ] deploying mon to mon1
[2021-12-10 22:07:19,778][mon1][DEBUG ] get remote short hostname
[2021-12-10 22:07:19,779][mon1][DEBUG ] remote hostname: mon1
[2021-12-10 22:07:19,781][mon1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2021-12-10 22:07:19,782][ceph_deploy.mon][ERROR ] RuntimeError: config file /etc/ceph/ceph.conf exists with different content; use --overwrite-conf to overwrite
[2021-12-10 22:07:19,783][ceph_deploy][ERROR ] GenericError: Failed to create 1 monitors

[2021-12-10 22:08:00,313][ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephuser/.cephdeploy.conf
[2021-12-10 22:08:00,313][ceph_deploy.cli][INFO  ] Invoked (1.5.39): /usr/bin/ceph-deploy mon create-initial
[2021-12-10 22:08:00,313][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2021-12-10 22:08:00,313][ceph_deploy.cli][INFO  ]  username                      : None
[2021-12-10 22:08:00,313][ceph_deploy.cli][INFO  ]  verbose                       : False
[2021-12-10 22:08:00,313][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2021-12-10 22:08:00,313][ceph_deploy.cli][INFO  ]  subcommand                    : create-initial
[2021-12-10 22:08:00,313][ceph_deploy.cli][INFO  ]  quiet                         : False
[2021-12-10 22:08:00,313][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f1ff9266f38>
[2021-12-10 22:08:00,313][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2021-12-10 22:08:00,313][ceph_deploy.cli][INFO  ]  func                          : <function mon at 0x7f1ff924d758>
[2021-12-10 22:08:00,313][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2021-12-10 22:08:00,313][ceph_deploy.cli][INFO  ]  default_release               : False
[2021-12-10 22:08:00,313][ceph_deploy.cli][INFO  ]  keyrings                      : None
[2021-12-10 22:08:00,314][ceph_deploy.mon][DEBUG ] Deploying mon, cluster ceph hosts mon1
[2021-12-10 22:08:00,314][ceph_deploy.mon][DEBUG ] detecting platform for host mon1 ...
[2021-12-10 22:08:00,480][mon1][DEBUG ] connection detected need for sudo
[2021-12-10 22:08:00,647][mon1][DEBUG ] connected to host: mon1 
[2021-12-10 22:08:00,648][mon1][DEBUG ] detect platform information from remote host
[2021-12-10 22:08:00,666][mon1][DEBUG ] detect machine type
[2021-12-10 22:08:00,670][mon1][DEBUG ] find the location of an executable
[2021-12-10 22:08:00,672][ceph_deploy.mon][INFO  ] distro info: CentOS Linux 7.9.2009 Core
[2021-12-10 22:08:00,672][mon1][DEBUG ] determining if provided host has same hostname in remote
[2021-12-10 22:08:00,672][mon1][DEBUG ] get remote short hostname
[2021-12-10 22:08:00,673][mon1][DEBUG ] deploying mon to mon1
[2021-12-10 22:08:00,673][mon1][DEBUG ] get remote short hostname
[2021-12-10 22:08:00,674][mon1][DEBUG ] remote hostname: mon1
[2021-12-10 22:08:00,676][mon1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2021-12-10 22:08:00,678][mon1][DEBUG ] create the mon path if it does not exist
[2021-12-10 22:08:00,679][mon1][DEBUG ] checking for done path: /var/lib/ceph/mon/ceph-mon1/done
[2021-12-10 22:08:00,680][mon1][DEBUG ] create a done file to avoid re-doing the mon deployment
[2021-12-10 22:08:00,681][mon1][DEBUG ] create the init path if it does not exist
[2021-12-10 22:08:00,683][mon1][INFO  ] Running command: sudo systemctl enable ceph.target
[2021-12-10 22:08:00,753][mon1][INFO  ] Running command: sudo systemctl enable ceph-mon@mon1
[2021-12-10 22:08:00,821][mon1][INFO  ] Running command: sudo systemctl start ceph-mon@mon1
[2021-12-10 22:08:02,891][mon1][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.mon1.asok mon_status
[2021-12-10 22:08:02,956][mon1][DEBUG ] ********************************************************************************
[2021-12-10 22:08:02,956][mon1][DEBUG ] status for monitor: mon.mon1
[2021-12-10 22:08:02,956][mon1][DEBUG ] {
[2021-12-10 22:08:02,956][mon1][DEBUG ]   "election_epoch": 3, 
[2021-12-10 22:08:02,956][mon1][DEBUG ]   "extra_probe_peers": [], 
[2021-12-10 22:08:02,956][mon1][DEBUG ]   "monmap": {
[2021-12-10 22:08:02,956][mon1][DEBUG ]     "created": "2021-12-10 21:35:26.299692", 
[2021-12-10 22:08:02,956][mon1][DEBUG ]     "epoch": 1, 
[2021-12-10 22:08:02,956][mon1][DEBUG ]     "fsid": "67d9e224-a6f7-43d3-ae36-e6100c59258e", 
[2021-12-10 22:08:02,956][mon1][DEBUG ]     "modified": "2021-12-10 21:35:26.299692", 
[2021-12-10 22:08:02,956][mon1][DEBUG ]     "mons": [
[2021-12-10 22:08:02,956][mon1][DEBUG ]       {
[2021-12-10 22:08:02,957][mon1][DEBUG ]         "addr": "192.168.1.93:6789/0", 
[2021-12-10 22:08:02,957][mon1][DEBUG ]         "name": "mon1", 
[2021-12-10 22:08:02,957][mon1][DEBUG ]         "rank": 0
[2021-12-10 22:08:02,957][mon1][DEBUG ]       }
[2021-12-10 22:08:02,957][mon1][DEBUG ]     ]
[2021-12-10 22:08:02,957][mon1][DEBUG ]   }, 
[2021-12-10 22:08:02,957][mon1][DEBUG ]   "name": "mon1", 
[2021-12-10 22:08:02,957][mon1][DEBUG ]   "outside_quorum": [], 
[2021-12-10 22:08:02,957][mon1][DEBUG ]   "quorum": [
[2021-12-10 22:08:02,957][mon1][DEBUG ]     0
[2021-12-10 22:08:02,957][mon1][DEBUG ]   ], 
[2021-12-10 22:08:02,957][mon1][DEBUG ]   "rank": 0, 
[2021-12-10 22:08:02,957][mon1][DEBUG ]   "state": "leader", 
[2021-12-10 22:08:02,957][mon1][DEBUG ]   "sync_provider": []
[2021-12-10 22:08:02,957][mon1][DEBUG ] }
[2021-12-10 22:08:02,957][mon1][DEBUG ] ********************************************************************************
[2021-12-10 22:08:02,957][mon1][INFO  ] monitor: mon.mon1 is running
[2021-12-10 22:08:02,959][mon1][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.mon1.asok mon_status
[2021-12-10 22:08:03,124][ceph_deploy.mon][INFO  ] processing monitor mon.mon1
[2021-12-10 22:08:03,284][mon1][DEBUG ] connection detected need for sudo
[2021-12-10 22:08:03,449][mon1][DEBUG ] connected to host: mon1 
[2021-12-10 22:08:03,450][mon1][DEBUG ] detect platform information from remote host
[2021-12-10 22:08:03,468][mon1][DEBUG ] detect machine type
[2021-12-10 22:08:03,473][mon1][DEBUG ] find the location of an executable
[2021-12-10 22:08:03,475][mon1][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.mon1.asok mon_status
[2021-12-10 22:08:03,540][ceph_deploy.mon][INFO  ] mon.mon1 monitor has reached quorum!
[2021-12-10 22:08:03,540][ceph_deploy.mon][INFO  ] all initial monitors are running and have formed quorum
[2021-12-10 22:08:03,540][ceph_deploy.mon][INFO  ] Running gatherkeys...
[2021-12-10 22:08:03,541][ceph_deploy.gatherkeys][INFO  ] Storing keys in temp directory /tmp/tmpUVT0Kf
[2021-12-10 22:08:03,701][mon1][DEBUG ] connection detected need for sudo
[2021-12-10 22:08:03,868][mon1][DEBUG ] connected to host: mon1 
[2021-12-10 22:08:03,869][mon1][DEBUG ] detect platform information from remote host
[2021-12-10 22:08:03,888][mon1][DEBUG ] detect machine type
[2021-12-10 22:08:03,893][mon1][DEBUG ] get remote short hostname
[2021-12-10 22:08:03,894][mon1][DEBUG ] fetch remote file
[2021-12-10 22:08:03,898][mon1][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --admin-daemon=/var/run/ceph/ceph-mon.mon1.asok mon_status
[2021-12-10 22:08:04,015][mon1][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-mon1/keyring auth get client.admin
[2021-12-10 22:08:04,181][mon1][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-mon1/keyring auth get-or-create client.admin osd allow * mds allow * mon allow * mgr allow *
[2021-12-10 22:08:04,347][mon1][ERROR ] "ceph auth get-or-create for keytype admin returned 22
[2021-12-10 22:08:04,347][mon1][DEBUG ] Error EINVAL: unknown cap type 'mgr'
[2021-12-10 22:08:04,347][mon1][ERROR ] Failed to return 'admin' key from host mon1
[2021-12-10 22:08:04,347][ceph_deploy.gatherkeys][ERROR ] Failed to connect to host:mon1
[2021-12-10 22:08:04,347][ceph_deploy.gatherkeys][INFO  ] Destroy temp directory /tmp/tmpUVT0Kf
[2021-12-10 22:08:04,347][ceph_deploy][ERROR ] RuntimeError: Failed to connect any mon

[2021-12-10 22:10:58,251][ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephuser/.cephdeploy.conf
[2021-12-10 22:10:58,251][ceph_deploy.cli][INFO  ] Invoked (1.5.39): /usr/bin/ceph-deploy install ceph-admin osd1 osd2 osd3 osd4 osd5 mon1
[2021-12-10 22:10:58,251][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2021-12-10 22:10:58,251][ceph_deploy.cli][INFO  ]  verbose                       : False
[2021-12-10 22:10:58,251][ceph_deploy.cli][INFO  ]  testing                       : None
[2021-12-10 22:10:58,251][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f4a863d5200>
[2021-12-10 22:10:58,251][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2021-12-10 22:10:58,251][ceph_deploy.cli][INFO  ]  dev_commit                    : None
[2021-12-10 22:10:58,251][ceph_deploy.cli][INFO  ]  install_mds                   : False
[2021-12-10 22:10:58,251][ceph_deploy.cli][INFO  ]  stable                        : None
[2021-12-10 22:10:58,251][ceph_deploy.cli][INFO  ]  default_release               : False
[2021-12-10 22:10:58,251][ceph_deploy.cli][INFO  ]  username                      : None
[2021-12-10 22:10:58,252][ceph_deploy.cli][INFO  ]  adjust_repos                  : True
[2021-12-10 22:10:58,252][ceph_deploy.cli][INFO  ]  func                          : <function install at 0x7f4a87099de8>
[2021-12-10 22:10:58,252][ceph_deploy.cli][INFO  ]  install_mgr                   : False
[2021-12-10 22:10:58,252][ceph_deploy.cli][INFO  ]  install_all                   : False
[2021-12-10 22:10:58,252][ceph_deploy.cli][INFO  ]  repo                          : False
[2021-12-10 22:10:58,252][ceph_deploy.cli][INFO  ]  host                          : ['ceph-admin', 'osd1', 'osd2', 'osd3', 'osd4', 'osd5', 'mon1']
[2021-12-10 22:10:58,252][ceph_deploy.cli][INFO  ]  install_rgw                   : False
[2021-12-10 22:10:58,252][ceph_deploy.cli][INFO  ]  install_tests                 : False
[2021-12-10 22:10:58,252][ceph_deploy.cli][INFO  ]  repo_url                      : None
[2021-12-10 22:10:58,252][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2021-12-10 22:10:58,252][ceph_deploy.cli][INFO  ]  install_osd                   : False
[2021-12-10 22:10:58,252][ceph_deploy.cli][INFO  ]  version_kind                  : stable
[2021-12-10 22:10:58,252][ceph_deploy.cli][INFO  ]  install_common                : False
[2021-12-10 22:10:58,252][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2021-12-10 22:10:58,252][ceph_deploy.cli][INFO  ]  quiet                         : False
[2021-12-10 22:10:58,252][ceph_deploy.cli][INFO  ]  dev                           : master
[2021-12-10 22:10:58,252][ceph_deploy.cli][INFO  ]  nogpgcheck                    : False
[2021-12-10 22:10:58,252][ceph_deploy.cli][INFO  ]  local_mirror                  : None
[2021-12-10 22:10:58,252][ceph_deploy.cli][INFO  ]  release                       : None
[2021-12-10 22:10:58,252][ceph_deploy.cli][INFO  ]  install_mon                   : False
[2021-12-10 22:10:58,252][ceph_deploy.cli][INFO  ]  gpg_url                       : None
[2021-12-10 22:10:58,252][ceph_deploy.install][DEBUG ] Installing stable version jewel on cluster ceph hosts ceph-admin osd1 osd2 osd3 osd4 osd5 mon1
[2021-12-10 22:10:58,253][ceph_deploy.install][DEBUG ] Detecting platform for host ceph-admin ...
[2021-12-10 22:10:58,308][ceph-admin][DEBUG ] connection detected need for sudo
[2021-12-10 22:10:58,353][ceph-admin][DEBUG ] connected to host: ceph-admin 
[2021-12-10 22:10:58,353][ceph-admin][DEBUG ] detect platform information from remote host
[2021-12-10 22:10:58,376][ceph-admin][DEBUG ] detect machine type
[2021-12-10 22:10:58,380][ceph_deploy.install][INFO  ] Distro info: CentOS Linux 7.9.2009 Core
[2021-12-10 22:10:58,380][ceph-admin][INFO  ] installing Ceph on ceph-admin
[2021-12-10 22:10:58,382][ceph-admin][INFO  ] Running command: sudo yum clean all
[2021-12-10 22:10:59,170][ceph-admin][DEBUG ] Loaded plugins: fastestmirror, priorities
[2021-12-10 22:10:59,242][ceph-admin][DEBUG ] Cleaning repos: Ceph Ceph-noarch base ceph-source epel extras updates
[2021-12-10 22:10:59,242][ceph-admin][DEBUG ] Cleaning up list of fastest mirrors
[2021-12-10 22:10:59,244][ceph-admin][INFO  ] Running command: sudo yum -y install epel-release
[2021-12-10 22:10:59,389][ceph-admin][DEBUG ] Loaded plugins: fastestmirror, priorities
[2021-12-10 22:10:59,402][ceph-admin][DEBUG ] Determining fastest mirrors
[2021-12-10 22:11:49,586][ceph-admin][DEBUG ]  * base: mirror.arizona.edu
[2021-12-10 22:11:49,586][ceph-admin][DEBUG ]  * epel: pubmirror1.math.uh.edu
[2021-12-10 22:11:49,586][ceph-admin][DEBUG ]  * extras: centos.mirror.lstn.net
[2021-12-10 22:11:49,587][ceph-admin][DEBUG ]  * updates: bay.uchicago.edu
[2021-12-10 22:12:51,322][ceph-admin][DEBUG ] 12 packages excluded due to repository priority protections
[2021-12-10 22:12:52,114][ceph-admin][DEBUG ] Resolving Dependencies
[2021-12-10 22:12:52,114][ceph-admin][DEBUG ] --> Running transaction check
[2021-12-10 22:12:52,114][ceph-admin][DEBUG ] ---> Package epel-release.noarch 0:7-11 will be updated
[2021-12-10 22:12:52,280][ceph-admin][DEBUG ] ---> Package epel-release.noarch 0:7-14 will be an update
[2021-12-10 22:12:53,037][ceph-admin][DEBUG ] --> Finished Dependency Resolution
[2021-12-10 22:12:53,077][ceph-admin][DEBUG ] 
[2021-12-10 22:12:53,077][ceph-admin][DEBUG ] Dependencies Resolved
[2021-12-10 22:12:53,077][ceph-admin][DEBUG ] 
[2021-12-10 22:12:53,077][ceph-admin][DEBUG ] ================================================================================
[2021-12-10 22:12:53,077][ceph-admin][DEBUG ]  Package                Arch             Version           Repository      Size
[2021-12-10 22:12:53,077][ceph-admin][DEBUG ] ================================================================================
[2021-12-10 22:12:53,077][ceph-admin][DEBUG ] Updating:
[2021-12-10 22:12:53,077][ceph-admin][DEBUG ]  epel-release           noarch           7-14              epel            15 k
[2021-12-10 22:12:53,077][ceph-admin][DEBUG ] 
[2021-12-10 22:12:53,078][ceph-admin][DEBUG ] Transaction Summary
[2021-12-10 22:12:53,078][ceph-admin][DEBUG ] ================================================================================
[2021-12-10 22:12:53,078][ceph-admin][DEBUG ] Upgrade  1 Package
[2021-12-10 22:12:53,078][ceph-admin][DEBUG ] 
[2021-12-10 22:12:53,078][ceph-admin][DEBUG ] Total download size: 15 k
[2021-12-10 22:12:53,078][ceph-admin][DEBUG ] Downloading packages:
[2021-12-10 22:12:53,078][ceph-admin][DEBUG ] Delta RPMs disabled because /usr/bin/applydeltarpm not installed.
[2021-12-10 22:12:58,773][ceph-admin][DEBUG ] Running transaction check
[2021-12-10 22:12:58,773][ceph-admin][DEBUG ] Running transaction test
[2021-12-10 22:12:58,839][ceph-admin][DEBUG ] Transaction test succeeded
[2021-12-10 22:12:58,839][ceph-admin][DEBUG ] Running transaction
[2021-12-10 22:13:00,056][ceph-admin][DEBUG ]   Updating   : epel-release-7-14.noarch                                     1/2 
[2021-12-10 22:13:00,425][ceph-admin][DEBUG ]   Cleanup    : epel-release-7-11.noarch                                     2/2 
[2021-12-10 22:13:00,549][ceph-admin][DEBUG ]   Verifying  : epel-release-7-14.noarch                                     1/2 
[2021-12-10 22:13:00,764][ceph-admin][DEBUG ]   Verifying  : epel-release-7-11.noarch                                     2/2 
[2021-12-10 22:13:00,764][ceph-admin][DEBUG ] 
[2021-12-10 22:13:00,764][ceph-admin][DEBUG ] Updated:
[2021-12-10 22:13:00,764][ceph-admin][DEBUG ]   epel-release.noarch 0:7-14                                                    
[2021-12-10 22:13:00,764][ceph-admin][DEBUG ] 
[2021-12-10 22:13:00,764][ceph-admin][DEBUG ] Complete!
[2021-12-10 22:13:00,891][ceph-admin][INFO  ] Running command: sudo yum -y install yum-plugin-priorities
[2021-12-10 22:13:01,035][ceph-admin][DEBUG ] Loaded plugins: fastestmirror, priorities
[2021-12-10 22:13:01,067][ceph-admin][DEBUG ] Loading mirror speeds from cached hostfile
[2021-12-10 22:13:01,067][ceph-admin][DEBUG ]  * base: mirror.arizona.edu
[2021-12-10 22:13:01,067][ceph-admin][DEBUG ]  * epel: pubmirror1.math.uh.edu
[2021-12-10 22:13:01,067][ceph-admin][DEBUG ]  * extras: centos.mirror.lstn.net
[2021-12-10 22:13:01,067][ceph-admin][DEBUG ]  * updates: bay.uchicago.edu
[2021-12-10 22:13:01,802][ceph-admin][DEBUG ] 12 packages excluded due to repository priority protections
[2021-12-10 22:13:02,313][ceph-admin][DEBUG ] Package yum-plugin-priorities-1.1.31-54.el7_8.noarch already installed and latest version
[2021-12-10 22:13:02,313][ceph-admin][DEBUG ] Nothing to do
[2021-12-10 22:13:02,352][ceph-admin][DEBUG ] Configure Yum priorities to include obsoletes
[2021-12-10 22:13:02,354][ceph-admin][WARNING] check_obsoletes has been enabled for Yum priorities plugin
[2021-12-10 22:13:02,355][ceph-admin][INFO  ] Running command: sudo rpm --import https://download.ceph.com/keys/release.asc
[2021-12-10 22:13:08,203][ceph-admin][INFO  ] Running command: sudo yum remove -y ceph-release
[2021-12-10 22:13:08,350][ceph-admin][DEBUG ] Loaded plugins: fastestmirror, priorities
[2021-12-10 22:13:08,421][ceph-admin][DEBUG ] Resolving Dependencies
[2021-12-10 22:13:08,421][ceph-admin][DEBUG ] --> Running transaction check
[2021-12-10 22:13:08,421][ceph-admin][DEBUG ] ---> Package ceph-release.noarch 0:1-1.el7 will be erased
[2021-12-10 22:13:08,685][ceph-admin][DEBUG ] --> Finished Dependency Resolution
[2021-12-10 22:13:08,786][ceph-admin][DEBUG ] 
[2021-12-10 22:13:08,786][ceph-admin][DEBUG ] Dependencies Resolved
[2021-12-10 22:13:08,786][ceph-admin][DEBUG ] 
[2021-12-10 22:13:08,786][ceph-admin][DEBUG ] ================================================================================
[2021-12-10 22:13:08,786][ceph-admin][DEBUG ]  Package         Arch      Version       Repository                        Size
[2021-12-10 22:13:08,786][ceph-admin][DEBUG ] ================================================================================
[2021-12-10 22:13:08,786][ceph-admin][DEBUG ] Removing:
[2021-12-10 22:13:08,787][ceph-admin][DEBUG ]  ceph-release    noarch    1-1.el7       @/ceph-release-1-0.el7.noarch    535  
[2021-12-10 22:13:08,787][ceph-admin][DEBUG ] 
[2021-12-10 22:13:08,787][ceph-admin][DEBUG ] Transaction Summary
[2021-12-10 22:13:08,787][ceph-admin][DEBUG ] ================================================================================
[2021-12-10 22:13:08,787][ceph-admin][DEBUG ] Remove  1 Package
[2021-12-10 22:13:08,787][ceph-admin][DEBUG ] 
[2021-12-10 22:13:08,787][ceph-admin][DEBUG ] Installed size: 535  
[2021-12-10 22:13:08,787][ceph-admin][DEBUG ] Downloading packages:
[2021-12-10 22:13:08,787][ceph-admin][DEBUG ] Running transaction check
[2021-12-10 22:13:08,787][ceph-admin][DEBUG ] Running transaction test
[2021-12-10 22:13:08,787][ceph-admin][DEBUG ] Transaction test succeeded
[2021-12-10 22:13:08,787][ceph-admin][DEBUG ] Running transaction
[2021-12-10 22:13:09,052][ceph-admin][DEBUG ]   Erasing    : ceph-release-1-1.el7.noarch                                  1/1 
[2021-12-10 22:13:09,052][ceph-admin][DEBUG ] warning: /etc/yum.repos.d/ceph.repo saved as /etc/yum.repos.d/ceph.repo.rpmsave
[2021-12-10 22:13:09,778][ceph-admin][DEBUG ]   Verifying  : ceph-release-1-1.el7.noarch                                  1/1 
[2021-12-10 22:13:09,778][ceph-admin][DEBUG ] 
[2021-12-10 22:13:09,778][ceph-admin][DEBUG ] Removed:
[2021-12-10 22:13:09,778][ceph-admin][DEBUG ]   ceph-release.noarch 0:1-1.el7                                                 
[2021-12-10 22:13:09,778][ceph-admin][DEBUG ] 
[2021-12-10 22:13:09,778][ceph-admin][DEBUG ] Complete!
[2021-12-10 22:13:09,789][ceph-admin][INFO  ] Running command: sudo yum install -y https://download.ceph.com/rpm-jewel/el7/noarch/ceph-release-1-0.el7.noarch.rpm
[2021-12-10 22:13:09,935][ceph-admin][DEBUG ] Loaded plugins: fastestmirror, priorities
[2021-12-10 22:13:15,723][ceph-admin][DEBUG ] Examining /var/tmp/yum-root-9nvExA/ceph-release-1-0.el7.noarch.rpm: ceph-release-1-1.el7.noarch
[2021-12-10 22:13:15,723][ceph-admin][DEBUG ] Marking /var/tmp/yum-root-9nvExA/ceph-release-1-0.el7.noarch.rpm to be installed
[2021-12-10 22:13:15,723][ceph-admin][DEBUG ] Resolving Dependencies
[2021-12-10 22:13:15,723][ceph-admin][DEBUG ] --> Running transaction check
[2021-12-10 22:13:15,723][ceph-admin][DEBUG ] ---> Package ceph-release.noarch 0:1-1.el7 will be installed
[2021-12-10 22:13:15,974][ceph-admin][DEBUG ] --> Finished Dependency Resolution
[2021-12-10 22:13:16,127][ceph-admin][DEBUG ] 
[2021-12-10 22:13:16,127][ceph-admin][DEBUG ] Dependencies Resolved
[2021-12-10 22:13:16,127][ceph-admin][DEBUG ] 
[2021-12-10 22:13:16,127][ceph-admin][DEBUG ] ================================================================================
[2021-12-10 22:13:16,127][ceph-admin][DEBUG ]  Package          Arch       Version     Repository                        Size
[2021-12-10 22:13:16,127][ceph-admin][DEBUG ] ================================================================================
[2021-12-10 22:13:16,127][ceph-admin][DEBUG ] Installing:
[2021-12-10 22:13:16,127][ceph-admin][DEBUG ]  ceph-release     noarch     1-1.el7     /ceph-release-1-0.el7.noarch     535  
[2021-12-10 22:13:16,127][ceph-admin][DEBUG ] 
[2021-12-10 22:13:16,127][ceph-admin][DEBUG ] Transaction Summary
[2021-12-10 22:13:16,127][ceph-admin][DEBUG ] ================================================================================
[2021-12-10 22:13:16,127][ceph-admin][DEBUG ] Install  1 Package
[2021-12-10 22:13:16,127][ceph-admin][DEBUG ] 
[2021-12-10 22:13:16,127][ceph-admin][DEBUG ] Total size: 535  
[2021-12-10 22:13:16,128][ceph-admin][DEBUG ] Installed size: 535  
[2021-12-10 22:13:16,128][ceph-admin][DEBUG ] Downloading packages:
[2021-12-10 22:13:16,128][ceph-admin][DEBUG ] Running transaction check
[2021-12-10 22:13:16,128][ceph-admin][DEBUG ] Running transaction test
[2021-12-10 22:13:16,128][ceph-admin][DEBUG ] Transaction test succeeded
[2021-12-10 22:13:16,128][ceph-admin][DEBUG ] Running transaction
[2021-12-10 22:13:16,944][ceph-admin][DEBUG ]   Installing : ceph-release-1-1.el7.noarch                                  1/1 
[2021-12-10 22:13:17,366][ceph-admin][DEBUG ]   Verifying  : ceph-release-1-1.el7.noarch                                  1/1 
[2021-12-10 22:13:17,367][ceph-admin][DEBUG ] 
[2021-12-10 22:13:17,367][ceph-admin][DEBUG ] Installed:
[2021-12-10 22:13:17,367][ceph-admin][DEBUG ]   ceph-release.noarch 0:1-1.el7                                                 
[2021-12-10 22:13:17,367][ceph-admin][DEBUG ] 
[2021-12-10 22:13:17,367][ceph-admin][DEBUG ] Complete!
[2021-12-10 22:13:17,368][ceph-admin][WARNING] ensuring that /etc/yum.repos.d/ceph.repo contains a high priority
[2021-12-10 22:13:17,370][ceph-admin][WARNING] altered ceph.repo priorities to contain: priority=1
[2021-12-10 22:13:17,371][ceph-admin][INFO  ] Running command: sudo yum -y install ceph ceph-radosgw
[2021-12-10 22:13:17,518][ceph-admin][DEBUG ] Loaded plugins: fastestmirror, priorities
[2021-12-10 22:13:17,532][ceph-admin][DEBUG ] Loading mirror speeds from cached hostfile
[2021-12-10 22:13:17,533][ceph-admin][DEBUG ]  * base: mirror.arizona.edu
[2021-12-10 22:13:17,533][ceph-admin][DEBUG ]  * epel: pubmirror1.math.uh.edu
[2021-12-10 22:13:17,534][ceph-admin][DEBUG ]  * extras: centos.mirror.lstn.net
[2021-12-10 22:13:17,534][ceph-admin][DEBUG ]  * updates: bay.uchicago.edu
[2021-12-10 22:13:24,015][ceph-admin][DEBUG ] 12 packages excluded due to repository priority protections
[2021-12-10 22:13:24,480][ceph-admin][DEBUG ] Package 2:ceph-10.2.11-0.el7.x86_64 already installed and latest version
[2021-12-10 22:13:24,748][ceph-admin][DEBUG ] Package 2:ceph-radosgw-10.2.11-0.el7.x86_64 already installed and latest version
[2021-12-10 22:13:24,748][ceph-admin][DEBUG ] Nothing to do
[2021-12-10 22:13:24,842][ceph-admin][INFO  ] Running command: sudo ceph --version
[2021-12-10 22:13:24,932][ceph-admin][DEBUG ] ceph version 10.2.11 (e4b061b47f07f583c92a050d9e84b1813a35671e)
[2021-12-10 22:13:24,932][ceph_deploy.install][DEBUG ] Detecting platform for host osd1 ...
[2021-12-10 22:13:25,135][osd1][DEBUG ] connection detected need for sudo
[2021-12-10 22:13:25,385][osd1][DEBUG ] connected to host: osd1 
[2021-12-10 22:13:25,385][osd1][DEBUG ] detect platform information from remote host
[2021-12-10 22:13:25,411][osd1][DEBUG ] detect machine type
[2021-12-10 22:13:25,416][ceph_deploy.install][INFO  ] Distro info: CentOS Linux 7.9.2009 Core
[2021-12-10 22:13:25,416][osd1][INFO  ] installing Ceph on osd1
[2021-12-10 22:13:25,418][osd1][INFO  ] Running command: sudo yum clean all
[2021-12-10 22:13:26,446][osd1][DEBUG ] Loaded plugins: fastestmirror, priorities
[2021-12-10 22:13:26,478][osd1][DEBUG ] Cleaning repos: Ceph Ceph-noarch base ceph-source epel extras updates
[2021-12-10 22:13:26,478][osd1][WARNING] Repodata is over 2 weeks old. Install yum-cron? Or run: yum makecache fast
[2021-12-10 22:13:26,494][osd1][DEBUG ] Cleaning up list of fastest mirrors
[2021-12-10 22:13:26,496][osd1][INFO  ] Running command: sudo yum -y install epel-release
[2021-12-10 22:13:26,612][osd1][DEBUG ] Loaded plugins: fastestmirror, priorities
[2021-12-10 22:13:26,676][osd1][DEBUG ] Determining fastest mirrors
[2021-12-10 22:14:03,591][osd1][DEBUG ]  * base: mirror.mobap.edu
[2021-12-10 22:14:03,591][osd1][DEBUG ]  * epel: pubmirror1.math.uh.edu
[2021-12-10 22:14:03,591][osd1][DEBUG ]  * extras: centos.mirror.lstn.net
[2021-12-10 22:14:03,592][osd1][DEBUG ]  * updates: bay.uchicago.edu
[2021-12-10 22:15:00,209][osd1][DEBUG ] 12 packages excluded due to repository priority protections
[2021-12-10 22:15:01,427][osd1][DEBUG ] Resolving Dependencies
[2021-12-10 22:15:01,427][osd1][DEBUG ] --> Running transaction check
[2021-12-10 22:15:01,427][osd1][DEBUG ] ---> Package epel-release.noarch 0:7-11 will be updated
[2021-12-10 22:15:01,491][osd1][DEBUG ] ---> Package epel-release.noarch 0:7-14 will be an update
[2021-12-10 22:15:02,458][osd1][DEBUG ] --> Finished Dependency Resolution
[2021-12-10 22:15:02,522][osd1][DEBUG ] 
[2021-12-10 22:15:02,522][osd1][DEBUG ] Dependencies Resolved
[2021-12-10 22:15:02,522][osd1][DEBUG ] 
[2021-12-10 22:15:02,522][osd1][DEBUG ] ================================================================================
[2021-12-10 22:15:02,522][osd1][DEBUG ]  Package                Arch             Version           Repository      Size
[2021-12-10 22:15:02,522][osd1][DEBUG ] ================================================================================
[2021-12-10 22:15:02,522][osd1][DEBUG ] Updating:
[2021-12-10 22:15:02,523][osd1][DEBUG ]  epel-release           noarch           7-14              epel            15 k
[2021-12-10 22:15:02,523][osd1][DEBUG ] 
[2021-12-10 22:15:02,523][osd1][DEBUG ] Transaction Summary
[2021-12-10 22:15:02,523][osd1][DEBUG ] ================================================================================
[2021-12-10 22:15:02,523][osd1][DEBUG ] Upgrade  1 Package
[2021-12-10 22:15:02,523][osd1][DEBUG ] 
[2021-12-10 22:15:02,523][osd1][DEBUG ] Total download size: 15 k
[2021-12-10 22:15:02,523][osd1][DEBUG ] Downloading packages:
[2021-12-10 22:15:02,523][osd1][DEBUG ] Delta RPMs disabled because /usr/bin/applydeltarpm not installed.
[2021-12-10 22:15:17,475][osd1][DEBUG ] Running transaction check
[2021-12-10 22:15:17,476][osd1][DEBUG ] Running transaction test
[2021-12-10 22:15:17,590][osd1][DEBUG ] Transaction test succeeded
[2021-12-10 22:15:17,590][osd1][DEBUG ] Running transaction
[2021-12-10 22:15:18,306][osd1][DEBUG ]   Updating   : epel-release-7-14.noarch                                     1/2 
[2021-12-10 22:15:18,620][osd1][DEBUG ]   Cleanup    : epel-release-7-11.noarch                                     2/2 
[2021-12-10 22:15:18,785][osd1][DEBUG ]   Verifying  : epel-release-7-14.noarch                                     1/2 
[2021-12-10 22:15:19,000][osd1][DEBUG ]   Verifying  : epel-release-7-11.noarch                                     2/2 
[2021-12-10 22:15:19,000][osd1][DEBUG ] 
[2021-12-10 22:15:19,000][osd1][DEBUG ] Updated:
[2021-12-10 22:15:19,000][osd1][DEBUG ]   epel-release.noarch 0:7-14                                                    
[2021-12-10 22:15:19,000][osd1][DEBUG ] 
[2021-12-10 22:15:19,000][osd1][DEBUG ] Complete!
[2021-12-10 22:15:19,167][osd1][INFO  ] Running command: sudo yum -y install yum-plugin-priorities
[2021-12-10 22:15:19,284][osd1][DEBUG ] Loaded plugins: fastestmirror, priorities
[2021-12-10 22:15:19,315][osd1][DEBUG ] Loading mirror speeds from cached hostfile
[2021-12-10 22:15:19,331][osd1][DEBUG ]  * base: mirror.mobap.edu
[2021-12-10 22:15:19,331][osd1][DEBUG ]  * epel: pubmirror1.math.uh.edu
[2021-12-10 22:15:19,331][osd1][DEBUG ]  * extras: centos.mirror.lstn.net
[2021-12-10 22:15:19,331][osd1][DEBUG ]  * updates: bay.uchicago.edu
[2021-12-10 22:15:20,047][osd1][DEBUG ] 12 packages excluded due to repository priority protections
[2021-12-10 22:15:20,562][osd1][DEBUG ] Package yum-plugin-priorities-1.1.31-54.el7_8.noarch already installed and latest version
[2021-12-10 22:15:20,562][osd1][DEBUG ] Nothing to do
[2021-12-10 22:15:20,626][osd1][DEBUG ] Configure Yum priorities to include obsoletes
[2021-12-10 22:15:20,629][osd1][WARNING] check_obsoletes has been enabled for Yum priorities plugin
[2021-12-10 22:15:20,630][osd1][INFO  ] Running command: sudo rpm --import https://download.ceph.com/keys/release.asc
[2021-12-10 22:15:26,463][osd1][INFO  ] Running command: sudo yum remove -y ceph-release
[2021-12-10 22:15:26,580][osd1][DEBUG ] Loaded plugins: fastestmirror, priorities
[2021-12-10 22:15:26,694][osd1][DEBUG ] Resolving Dependencies
[2021-12-10 22:15:26,694][osd1][DEBUG ] --> Running transaction check
[2021-12-10 22:15:26,694][osd1][DEBUG ] ---> Package ceph-release.noarch 0:1-1.el7 will be erased
[2021-12-10 22:15:26,808][osd1][DEBUG ] --> Finished Dependency Resolution
[2021-12-10 22:15:26,922][osd1][DEBUG ] 
[2021-12-10 22:15:26,923][osd1][DEBUG ] Dependencies Resolved
[2021-12-10 22:15:26,923][osd1][DEBUG ] 
[2021-12-10 22:15:26,923][osd1][DEBUG ] ================================================================================
[2021-12-10 22:15:26,923][osd1][DEBUG ]  Package         Arch      Version       Repository                        Size
[2021-12-10 22:15:26,923][osd1][DEBUG ] ================================================================================
[2021-12-10 22:15:26,923][osd1][DEBUG ] Removing:
[2021-12-10 22:15:26,923][osd1][DEBUG ]  ceph-release    noarch    1-1.el7       @/ceph-release-1-0.el7.noarch    535  
[2021-12-10 22:15:26,923][osd1][DEBUG ] 
[2021-12-10 22:15:26,923][osd1][DEBUG ] Transaction Summary
[2021-12-10 22:15:26,923][osd1][DEBUG ] ================================================================================
[2021-12-10 22:15:26,923][osd1][DEBUG ] Remove  1 Package
[2021-12-10 22:15:26,923][osd1][DEBUG ] 
[2021-12-10 22:15:26,923][osd1][DEBUG ] Installed size: 535  
[2021-12-10 22:15:26,923][osd1][DEBUG ] Downloading packages:
[2021-12-10 22:15:26,923][osd1][DEBUG ] Running transaction check
[2021-12-10 22:15:26,923][osd1][DEBUG ] Running transaction test
[2021-12-10 22:15:26,923][osd1][DEBUG ] Transaction test succeeded
[2021-12-10 22:15:26,924][osd1][DEBUG ] Running transaction
[2021-12-10 22:15:27,238][osd1][DEBUG ]   Erasing    : ceph-release-1-1.el7.noarch                                  1/1 
[2021-12-10 22:15:27,238][osd1][DEBUG ] warning: /etc/yum.repos.d/ceph.repo saved as /etc/yum.repos.d/ceph.repo.rpmsave
[2021-12-10 22:15:28,104][osd1][DEBUG ]   Verifying  : ceph-release-1-1.el7.noarch                                  1/1 
[2021-12-10 22:15:28,105][osd1][DEBUG ] 
[2021-12-10 22:15:28,105][osd1][DEBUG ] Removed:
[2021-12-10 22:15:28,105][osd1][DEBUG ]   ceph-release.noarch 0:1-1.el7                                                 
[2021-12-10 22:15:28,105][osd1][DEBUG ] 
[2021-12-10 22:15:28,105][osd1][DEBUG ] Complete!
[2021-12-10 22:15:28,139][osd1][INFO  ] Running command: sudo yum install -y https://download.ceph.com/rpm-jewel/el7/noarch/ceph-release-1-0.el7.noarch.rpm
[2021-12-10 22:15:28,255][osd1][DEBUG ] Loaded plugins: fastestmirror, priorities
[2021-12-10 22:15:34,034][osd1][DEBUG ] Examining /var/tmp/yum-root-28Wur9/ceph-release-1-0.el7.noarch.rpm: ceph-release-1-1.el7.noarch
[2021-12-10 22:15:34,034][osd1][DEBUG ] Marking /var/tmp/yum-root-28Wur9/ceph-release-1-0.el7.noarch.rpm to be installed
[2021-12-10 22:15:34,034][osd1][DEBUG ] Resolving Dependencies
[2021-12-10 22:15:34,034][osd1][DEBUG ] --> Running transaction check
[2021-12-10 22:15:34,034][osd1][DEBUG ] ---> Package ceph-release.noarch 0:1-1.el7 will be installed
[2021-12-10 22:15:34,198][osd1][DEBUG ] --> Finished Dependency Resolution
[2021-12-10 22:15:34,313][osd1][DEBUG ] 
[2021-12-10 22:15:34,313][osd1][DEBUG ] Dependencies Resolved
[2021-12-10 22:15:34,313][osd1][DEBUG ] 
[2021-12-10 22:15:34,313][osd1][DEBUG ] ================================================================================
[2021-12-10 22:15:34,313][osd1][DEBUG ]  Package          Arch       Version     Repository                        Size
[2021-12-10 22:15:34,313][osd1][DEBUG ] ================================================================================
[2021-12-10 22:15:34,313][osd1][DEBUG ] Installing:
[2021-12-10 22:15:34,313][osd1][DEBUG ]  ceph-release     noarch     1-1.el7     /ceph-release-1-0.el7.noarch     535  
[2021-12-10 22:15:34,314][osd1][DEBUG ] 
[2021-12-10 22:15:34,314][osd1][DEBUG ] Transaction Summary
[2021-12-10 22:15:34,314][osd1][DEBUG ] ================================================================================
[2021-12-10 22:15:34,314][osd1][DEBUG ] Install  1 Package
[2021-12-10 22:15:34,314][osd1][DEBUG ] 
[2021-12-10 22:15:34,314][osd1][DEBUG ] Total size: 535  
[2021-12-10 22:15:34,314][osd1][DEBUG ] Installed size: 535  
[2021-12-10 22:15:34,314][osd1][DEBUG ] Downloading packages:
[2021-12-10 22:15:34,314][osd1][DEBUG ] Running transaction check
[2021-12-10 22:15:34,314][osd1][DEBUG ] Running transaction test
[2021-12-10 22:15:34,314][osd1][DEBUG ] Transaction test succeeded
[2021-12-10 22:15:34,314][osd1][DEBUG ] Running transaction
[2021-12-10 22:15:35,230][osd1][DEBUG ]   Installing : ceph-release-1-1.el7.noarch                                  1/1 
[2021-12-10 22:15:35,696][osd1][DEBUG ]   Verifying  : ceph-release-1-1.el7.noarch                                  1/1 
[2021-12-10 22:15:35,696][osd1][DEBUG ] 
[2021-12-10 22:15:35,696][osd1][DEBUG ] Installed:
[2021-12-10 22:15:35,696][osd1][DEBUG ]   ceph-release.noarch 0:1-1.el7                                                 
[2021-12-10 22:15:35,696][osd1][DEBUG ] 
[2021-12-10 22:15:35,696][osd1][DEBUG ] Complete!
[2021-12-10 22:15:35,696][osd1][WARNING] ensuring that /etc/yum.repos.d/ceph.repo contains a high priority
[2021-12-10 22:15:35,699][osd1][WARNING] altered ceph.repo priorities to contain: priority=1
[2021-12-10 22:15:35,700][osd1][INFO  ] Running command: sudo yum -y install ceph ceph-radosgw
[2021-12-10 22:15:35,817][osd1][DEBUG ] Loaded plugins: fastestmirror, priorities
[2021-12-10 22:15:35,849][osd1][DEBUG ] Loading mirror speeds from cached hostfile
[2021-12-10 22:15:35,852][osd1][DEBUG ]  * base: mirror.mobap.edu
[2021-12-10 22:15:35,852][osd1][DEBUG ]  * epel: pubmirror1.math.uh.edu
[2021-12-10 22:15:35,853][osd1][DEBUG ]  * extras: centos.mirror.lstn.net
[2021-12-10 22:15:35,853][osd1][DEBUG ]  * updates: bay.uchicago.edu
[2021-12-10 22:15:42,284][osd1][DEBUG ] 12 packages excluded due to repository priority protections
[2021-12-10 22:15:42,749][osd1][DEBUG ] Package 2:ceph-10.2.11-0.el7.x86_64 already installed and latest version
[2021-12-10 22:15:43,064][osd1][DEBUG ] Package 2:ceph-radosgw-10.2.11-0.el7.x86_64 already installed and latest version
[2021-12-10 22:15:43,064][osd1][DEBUG ] Nothing to do
[2021-12-10 22:15:43,130][osd1][INFO  ] Running command: sudo ceph --version
[2021-12-10 22:15:43,196][osd1][DEBUG ] ceph version 10.2.11 (e4b061b47f07f583c92a050d9e84b1813a35671e)
[2021-12-10 22:15:43,197][ceph_deploy.install][DEBUG ] Detecting platform for host osd2 ...
[2021-12-10 22:15:43,358][osd2][DEBUG ] connection detected need for sudo
[2021-12-10 22:15:43,574][osd2][DEBUG ] connected to host: osd2 
[2021-12-10 22:15:43,575][osd2][DEBUG ] detect platform information from remote host
[2021-12-10 22:15:43,608][osd2][DEBUG ] detect machine type
[2021-12-10 22:15:43,613][ceph_deploy.install][INFO  ] Distro info: CentOS Linux 7.9.2009 Core
[2021-12-10 22:15:43,613][osd2][INFO  ] installing Ceph on osd2
[2021-12-10 22:15:43,615][osd2][INFO  ] Running command: sudo yum clean all
[2021-12-10 22:15:44,434][osd2][DEBUG ] Loaded plugins: fastestmirror, priorities
[2021-12-10 22:15:44,498][osd2][DEBUG ] Cleaning repos: Ceph Ceph-noarch base ceph-source epel extras updates
[2021-12-10 22:15:44,498][osd2][WARNING] Repodata is over 2 weeks old. Install yum-cron? Or run: yum makecache fast
[2021-12-10 22:15:44,498][osd2][DEBUG ] Cleaning up list of fastest mirrors
[2021-12-10 22:15:44,500][osd2][INFO  ] Running command: sudo yum -y install epel-release
[2021-12-10 22:15:44,617][osd2][DEBUG ] Loaded plugins: fastestmirror, priorities
[2021-12-10 22:15:44,649][osd2][DEBUG ] Determining fastest mirrors
[2021-12-10 22:16:24,868][osd2][DEBUG ]  * base: mirror.arizona.edu
[2021-12-10 22:16:24,868][osd2][DEBUG ]  * epel: pubmirror1.math.uh.edu
[2021-12-10 22:16:24,868][osd2][DEBUG ]  * extras: centos.mirror.lstn.net
[2021-12-10 22:16:24,868][osd2][DEBUG ]  * updates: bay.uchicago.edu
[2021-12-10 22:17:25,374][osd2][DEBUG ] 12 packages excluded due to repository priority protections
[2021-12-10 22:17:26,795][osd2][DEBUG ] Resolving Dependencies
[2021-12-10 22:17:26,795][osd2][DEBUG ] --> Running transaction check
[2021-12-10 22:17:26,795][osd2][DEBUG ] ---> Package epel-release.noarch 0:7-11 will be updated
[2021-12-10 22:17:26,827][osd2][DEBUG ] ---> Package epel-release.noarch 0:7-14 will be an update
[2021-12-10 22:17:27,643][osd2][DEBUG ] --> Finished Dependency Resolution
[2021-12-10 22:17:27,707][osd2][DEBUG ] 
[2021-12-10 22:17:27,708][osd2][DEBUG ] Dependencies Resolved
[2021-12-10 22:17:27,708][osd2][DEBUG ] 
[2021-12-10 22:17:27,708][osd2][DEBUG ] ================================================================================
[2021-12-10 22:17:27,708][osd2][DEBUG ]  Package                Arch             Version           Repository      Size
[2021-12-10 22:17:27,708][osd2][DEBUG ] ================================================================================
[2021-12-10 22:17:27,708][osd2][DEBUG ] Updating:
[2021-12-10 22:17:27,708][osd2][DEBUG ]  epel-release           noarch           7-14              epel            15 k
[2021-12-10 22:17:27,708][osd2][DEBUG ] 
[2021-12-10 22:17:27,708][osd2][DEBUG ] Transaction Summary
[2021-12-10 22:17:27,708][osd2][DEBUG ] ================================================================================
[2021-12-10 22:17:27,708][osd2][DEBUG ] Upgrade  1 Package
[2021-12-10 22:17:27,708][osd2][DEBUG ] 
[2021-12-10 22:17:27,708][osd2][DEBUG ] Total download size: 15 k
[2021-12-10 22:17:27,708][osd2][DEBUG ] Downloading packages:
[2021-12-10 22:17:27,708][osd2][DEBUG ] Delta RPMs disabled because /usr/bin/applydeltarpm not installed.
[2021-12-10 22:17:33,395][osd2][DEBUG ] Running transaction check
[2021-12-10 22:17:33,395][osd2][DEBUG ] Running transaction test
[2021-12-10 22:17:33,459][osd2][DEBUG ] Transaction test succeeded
[2021-12-10 22:17:33,459][osd2][DEBUG ] Running transaction
[2021-12-10 22:17:34,275][osd2][DEBUG ]   Updating   : epel-release-7-14.noarch                                     1/2 
[2021-12-10 22:17:34,590][osd2][DEBUG ]   Cleanup    : epel-release-7-11.noarch                                     2/2 
[2021-12-10 22:17:34,704][osd2][DEBUG ]   Verifying  : epel-release-7-14.noarch                                     1/2 
[2021-12-10 22:17:34,918][osd2][DEBUG ]   Verifying  : epel-release-7-11.noarch                                     2/2 
[2021-12-10 22:17:34,919][osd2][DEBUG ] 
[2021-12-10 22:17:34,919][osd2][DEBUG ] Updated:
[2021-12-10 22:17:34,919][osd2][DEBUG ]   epel-release.noarch 0:7-14                                                    
[2021-12-10 22:17:34,919][osd2][DEBUG ] 
[2021-12-10 22:17:34,919][osd2][DEBUG ] Complete!
[2021-12-10 22:17:35,035][osd2][INFO  ] Running command: sudo yum -y install yum-plugin-priorities
[2021-12-10 22:17:35,152][osd2][DEBUG ] Loaded plugins: fastestmirror, priorities
[2021-12-10 22:17:35,216][osd2][DEBUG ] Loading mirror speeds from cached hostfile
[2021-12-10 22:17:35,216][osd2][DEBUG ]  * base: mirror.arizona.edu
[2021-12-10 22:17:35,216][osd2][DEBUG ]  * epel: pubmirror1.math.uh.edu
[2021-12-10 22:17:35,216][osd2][DEBUG ]  * extras: centos.mirror.lstn.net
[2021-12-10 22:17:35,217][osd2][DEBUG ]  * updates: bay.uchicago.edu
[2021-12-10 22:17:35,932][osd2][DEBUG ] 12 packages excluded due to repository priority protections
[2021-12-10 22:17:36,448][osd2][DEBUG ] Package yum-plugin-priorities-1.1.31-54.el7_8.noarch already installed and latest version
[2021-12-10 22:17:36,449][osd2][DEBUG ] Nothing to do
[2021-12-10 22:17:36,563][osd2][DEBUG ] Configure Yum priorities to include obsoletes
[2021-12-10 22:17:36,565][osd2][WARNING] check_obsoletes has been enabled for Yum priorities plugin
[2021-12-10 22:17:36,567][osd2][INFO  ] Running command: sudo rpm --import https://download.ceph.com/keys/release.asc
[2021-12-10 22:17:42,454][osd2][INFO  ] Running command: sudo yum remove -y ceph-release
[2021-12-10 22:17:42,571][osd2][DEBUG ] Loaded plugins: fastestmirror, priorities
[2021-12-10 22:17:42,686][osd2][DEBUG ] Resolving Dependencies
[2021-12-10 22:17:42,686][osd2][DEBUG ] --> Running transaction check
[2021-12-10 22:17:42,686][osd2][DEBUG ] ---> Package ceph-release.noarch 0:1-1.el7 will be erased
[2021-12-10 22:17:42,851][osd2][DEBUG ] --> Finished Dependency Resolution
[2021-12-10 22:17:42,965][osd2][DEBUG ] 
[2021-12-10 22:17:42,965][osd2][DEBUG ] Dependencies Resolved
[2021-12-10 22:17:42,965][osd2][DEBUG ] 
[2021-12-10 22:17:42,965][osd2][DEBUG ] ================================================================================
[2021-12-10 22:17:42,965][osd2][DEBUG ]  Package         Arch      Version       Repository                        Size
[2021-12-10 22:17:42,965][osd2][DEBUG ] ================================================================================
[2021-12-10 22:17:42,965][osd2][DEBUG ] Removing:
[2021-12-10 22:17:42,965][osd2][DEBUG ]  ceph-release    noarch    1-1.el7       @/ceph-release-1-0.el7.noarch    535  
[2021-12-10 22:17:42,965][osd2][DEBUG ] 
[2021-12-10 22:17:42,966][osd2][DEBUG ] Transaction Summary
[2021-12-10 22:17:42,966][osd2][DEBUG ] ================================================================================
[2021-12-10 22:17:42,966][osd2][DEBUG ] Remove  1 Package
[2021-12-10 22:17:42,966][osd2][DEBUG ] 
[2021-12-10 22:17:42,966][osd2][DEBUG ] Installed size: 535  
[2021-12-10 22:17:42,966][osd2][DEBUG ] Downloading packages:
[2021-12-10 22:17:42,966][osd2][DEBUG ] Running transaction check
[2021-12-10 22:17:42,966][osd2][DEBUG ] Running transaction test
[2021-12-10 22:17:42,966][osd2][DEBUG ] Transaction test succeeded
[2021-12-10 22:17:42,966][osd2][DEBUG ] Running transaction
[2021-12-10 22:17:43,231][osd2][DEBUG ]   Erasing    : ceph-release-1-1.el7.noarch                                  1/1 
[2021-12-10 22:17:43,231][osd2][DEBUG ] warning: /etc/yum.repos.d/ceph.repo saved as /etc/yum.repos.d/ceph.repo.rpmsave
[2021-12-10 22:17:43,896][osd2][DEBUG ]   Verifying  : ceph-release-1-1.el7.noarch                                  1/1 
[2021-12-10 22:17:43,897][osd2][DEBUG ] 
[2021-12-10 22:17:43,897][osd2][DEBUG ] Removed:
[2021-12-10 22:17:43,897][osd2][DEBUG ]   ceph-release.noarch 0:1-1.el7                                                 
[2021-12-10 22:17:43,897][osd2][DEBUG ] 
[2021-12-10 22:17:43,897][osd2][DEBUG ] Complete!
[2021-12-10 22:17:43,902][osd2][INFO  ] Running command: sudo yum install -y https://download.ceph.com/rpm-jewel/el7/noarch/ceph-release-1-0.el7.noarch.rpm
[2021-12-10 22:17:44,018][osd2][DEBUG ] Loaded plugins: fastestmirror, priorities
[2021-12-10 22:17:49,849][osd2][DEBUG ] Examining /var/tmp/yum-root-0YuKxx/ceph-release-1-0.el7.noarch.rpm: ceph-release-1-1.el7.noarch
[2021-12-10 22:17:49,849][osd2][DEBUG ] Marking /var/tmp/yum-root-0YuKxx/ceph-release-1-0.el7.noarch.rpm to be installed
[2021-12-10 22:17:49,850][osd2][DEBUG ] Resolving Dependencies
[2021-12-10 22:17:49,850][osd2][DEBUG ] --> Running transaction check
[2021-12-10 22:17:49,850][osd2][DEBUG ] ---> Package ceph-release.noarch 0:1-1.el7 will be installed
[2021-12-10 22:17:49,964][osd2][DEBUG ] --> Finished Dependency Resolution
[2021-12-10 22:17:50,128][osd2][DEBUG ] 
[2021-12-10 22:17:50,128][osd2][DEBUG ] Dependencies Resolved
[2021-12-10 22:17:50,129][osd2][DEBUG ] 
[2021-12-10 22:17:50,129][osd2][DEBUG ] ================================================================================
[2021-12-10 22:17:50,129][osd2][DEBUG ]  Package          Arch       Version     Repository                        Size
[2021-12-10 22:17:50,129][osd2][DEBUG ] ================================================================================
[2021-12-10 22:17:50,129][osd2][DEBUG ] Installing:
[2021-12-10 22:17:50,129][osd2][DEBUG ]  ceph-release     noarch     1-1.el7     /ceph-release-1-0.el7.noarch     535  
[2021-12-10 22:17:50,129][osd2][DEBUG ] 
[2021-12-10 22:17:50,129][osd2][DEBUG ] Transaction Summary
[2021-12-10 22:17:50,129][osd2][DEBUG ] ================================================================================
[2021-12-10 22:17:50,129][osd2][DEBUG ] Install  1 Package
[2021-12-10 22:17:50,129][osd2][DEBUG ] 
[2021-12-10 22:17:50,129][osd2][DEBUG ] Total size: 535  
[2021-12-10 22:17:50,129][osd2][DEBUG ] Installed size: 535  
[2021-12-10 22:17:50,129][osd2][DEBUG ] Downloading packages:
[2021-12-10 22:17:50,129][osd2][DEBUG ] Running transaction check
[2021-12-10 22:17:50,129][osd2][DEBUG ] Running transaction test
[2021-12-10 22:17:50,130][osd2][DEBUG ] Transaction test succeeded
[2021-12-10 22:17:50,130][osd2][DEBUG ] Running transaction
[2021-12-10 22:17:50,845][osd2][DEBUG ]   Installing : ceph-release-1-1.el7.noarch                                  1/1 
[2021-12-10 22:17:51,260][osd2][DEBUG ]   Verifying  : ceph-release-1-1.el7.noarch                                  1/1 
[2021-12-10 22:17:51,260][osd2][DEBUG ] 
[2021-12-10 22:17:51,261][osd2][DEBUG ] Installed:
[2021-12-10 22:17:51,261][osd2][DEBUG ]   ceph-release.noarch 0:1-1.el7                                                 
[2021-12-10 22:17:51,261][osd2][DEBUG ] 
[2021-12-10 22:17:51,261][osd2][DEBUG ] Complete!
[2021-12-10 22:17:51,268][osd2][WARNING] ensuring that /etc/yum.repos.d/ceph.repo contains a high priority
[2021-12-10 22:17:51,271][osd2][WARNING] altered ceph.repo priorities to contain: priority=1
[2021-12-10 22:17:51,272][osd2][INFO  ] Running command: sudo yum -y install ceph ceph-radosgw
[2021-12-10 22:17:51,389][osd2][DEBUG ] Loaded plugins: fastestmirror, priorities
[2021-12-10 22:17:51,453][osd2][DEBUG ] Loading mirror speeds from cached hostfile
[2021-12-10 22:17:51,453][osd2][DEBUG ]  * base: mirror.arizona.edu
[2021-12-10 22:17:51,453][osd2][DEBUG ]  * epel: pubmirror1.math.uh.edu
[2021-12-10 22:17:51,453][osd2][DEBUG ]  * extras: centos.mirror.lstn.net
[2021-12-10 22:17:51,453][osd2][DEBUG ]  * updates: bay.uchicago.edu
[2021-12-10 22:17:57,886][osd2][DEBUG ] 12 packages excluded due to repository priority protections
[2021-12-10 22:17:58,401][osd2][DEBUG ] Package 2:ceph-10.2.11-0.el7.x86_64 already installed and latest version
[2021-12-10 22:17:58,666][osd2][DEBUG ] Package 2:ceph-radosgw-10.2.11-0.el7.x86_64 already installed and latest version
[2021-12-10 22:17:58,666][osd2][DEBUG ] Nothing to do
[2021-12-10 22:17:58,782][osd2][INFO  ] Running command: sudo ceph --version
[2021-12-10 22:17:58,848][osd2][DEBUG ] ceph version 10.2.11 (e4b061b47f07f583c92a050d9e84b1813a35671e)
[2021-12-10 22:17:58,848][ceph_deploy.install][DEBUG ] Detecting platform for host osd3 ...
[2021-12-10 22:17:59,150][osd3][DEBUG ] connection detected need for sudo
[2021-12-10 22:17:59,377][osd3][DEBUG ] connected to host: osd3 
[2021-12-10 22:17:59,378][osd3][DEBUG ] detect platform information from remote host
[2021-12-10 22:17:59,433][osd3][DEBUG ] detect machine type
[2021-12-10 22:17:59,449][ceph_deploy.install][INFO  ] Distro info: CentOS Linux 7.9.2009 Core
[2021-12-10 22:17:59,450][osd3][INFO  ] installing Ceph on osd3
[2021-12-10 22:17:59,451][osd3][INFO  ] Running command: sudo yum clean all
[2021-12-10 22:18:00,671][osd3][DEBUG ] Loaded plugins: fastestmirror, priorities
[2021-12-10 22:18:00,703][osd3][DEBUG ] Cleaning repos: Ceph Ceph-noarch base ceph-source epel extras updates
[2021-12-10 22:18:00,703][osd3][WARNING] Repodata is over 2 weeks old. Install yum-cron? Or run: yum makecache fast
[2021-12-10 22:18:00,719][osd3][DEBUG ] Cleaning up list of fastest mirrors
[2021-12-10 22:18:00,721][osd3][INFO  ] Running command: sudo yum -y install epel-release
[2021-12-10 22:18:00,837][osd3][DEBUG ] Loaded plugins: fastestmirror, priorities
[2021-12-10 22:18:00,869][osd3][DEBUG ] Determining fastest mirrors
[2021-12-10 22:18:37,894][osd3][DEBUG ]  * base: mirror.mobap.edu
[2021-12-10 22:18:37,895][osd3][DEBUG ]  * epel: pubmirror1.math.uh.edu
[2021-12-10 22:18:37,895][osd3][DEBUG ]  * extras: centos.mirror.lstn.net
[2021-12-10 22:18:37,895][osd3][DEBUG ]  * updates: bay.uchicago.edu
[2021-12-10 22:19:33,774][osd3][DEBUG ] 12 packages excluded due to repository priority protections
[2021-12-10 22:19:35,041][osd3][DEBUG ] Resolving Dependencies
[2021-12-10 22:19:35,042][osd3][DEBUG ] --> Running transaction check
[2021-12-10 22:19:35,042][osd3][DEBUG ] ---> Package epel-release.noarch 0:7-11 will be updated
[2021-12-10 22:19:35,074][osd3][DEBUG ] ---> Package epel-release.noarch 0:7-14 will be an update
[2021-12-10 22:19:35,940][osd3][DEBUG ] --> Finished Dependency Resolution
[2021-12-10 22:19:36,004][osd3][DEBUG ] 
[2021-12-10 22:19:36,004][osd3][DEBUG ] Dependencies Resolved
[2021-12-10 22:19:36,004][osd3][DEBUG ] 
[2021-12-10 22:19:36,004][osd3][DEBUG ] ================================================================================
[2021-12-10 22:19:36,004][osd3][DEBUG ]  Package                Arch             Version           Repository      Size
[2021-12-10 22:19:36,004][osd3][DEBUG ] ================================================================================
[2021-12-10 22:19:36,004][osd3][DEBUG ] Updating:
[2021-12-10 22:19:36,004][osd3][DEBUG ]  epel-release           noarch           7-14              epel            15 k
[2021-12-10 22:19:36,005][osd3][DEBUG ] 
[2021-12-10 22:19:36,005][osd3][DEBUG ] Transaction Summary
[2021-12-10 22:19:36,005][osd3][DEBUG ] ================================================================================
[2021-12-10 22:19:36,005][osd3][DEBUG ] Upgrade  1 Package
[2021-12-10 22:19:36,005][osd3][DEBUG ] 
[2021-12-10 22:19:36,005][osd3][DEBUG ] Total download size: 15 k
[2021-12-10 22:19:36,005][osd3][DEBUG ] Downloading packages:
[2021-12-10 22:19:36,005][osd3][DEBUG ] Delta RPMs disabled because /usr/bin/applydeltarpm not installed.
[2021-12-10 22:19:44,342][osd3][DEBUG ] Running transaction check
[2021-12-10 22:19:44,342][osd3][DEBUG ] Running transaction test
[2021-12-10 22:19:44,346][osd3][DEBUG ] Transaction test succeeded
[2021-12-10 22:19:44,346][osd3][DEBUG ] Running transaction
[2021-12-10 22:19:45,162][osd3][DEBUG ]   Updating   : epel-release-7-14.noarch                                     1/2 
[2021-12-10 22:19:45,476][osd3][DEBUG ]   Cleanup    : epel-release-7-11.noarch                                     2/2 
[2021-12-10 22:19:45,591][osd3][DEBUG ]   Verifying  : epel-release-7-14.noarch                                     1/2 
[2021-12-10 22:19:45,855][osd3][DEBUG ]   Verifying  : epel-release-7-11.noarch                                     2/2 
[2021-12-10 22:19:45,855][osd3][DEBUG ] 
[2021-12-10 22:19:45,855][osd3][DEBUG ] Updated:
[2021-12-10 22:19:45,855][osd3][DEBUG ]   epel-release.noarch 0:7-14                                                    
[2021-12-10 22:19:45,855][osd3][DEBUG ] 
[2021-12-10 22:19:45,856][osd3][DEBUG ] Complete!
[2021-12-10 22:19:46,021][osd3][INFO  ] Running command: sudo yum -y install yum-plugin-priorities
[2021-12-10 22:19:46,238][osd3][DEBUG ] Loaded plugins: fastestmirror, priorities
[2021-12-10 22:19:46,238][osd3][DEBUG ] Loading mirror speeds from cached hostfile
[2021-12-10 22:19:46,254][osd3][DEBUG ]  * base: mirror.mobap.edu
[2021-12-10 22:19:46,254][osd3][DEBUG ]  * epel: pubmirror1.math.uh.edu
[2021-12-10 22:19:46,254][osd3][DEBUG ]  * extras: centos.mirror.lstn.net
[2021-12-10 22:19:46,254][osd3][DEBUG ]  * updates: bay.uchicago.edu
[2021-12-10 22:19:47,021][osd3][DEBUG ] 12 packages excluded due to repository priority protections
[2021-12-10 22:19:47,486][osd3][DEBUG ] Package yum-plugin-priorities-1.1.31-54.el7_8.noarch already installed and latest version
[2021-12-10 22:19:47,486][osd3][DEBUG ] Nothing to do
[2021-12-10 22:19:47,550][osd3][DEBUG ] Configure Yum priorities to include obsoletes
[2021-12-10 22:19:47,552][osd3][WARNING] check_obsoletes has been enabled for Yum priorities plugin
[2021-12-10 22:19:47,554][osd3][INFO  ] Running command: sudo rpm --import https://download.ceph.com/keys/release.asc
[2021-12-10 22:19:53,439][osd3][INFO  ] Running command: sudo yum remove -y ceph-release
[2021-12-10 22:19:53,556][osd3][DEBUG ] Loaded plugins: fastestmirror, priorities
[2021-12-10 22:19:53,670][osd3][DEBUG ] Resolving Dependencies
[2021-12-10 22:19:53,670][osd3][DEBUG ] --> Running transaction check
[2021-12-10 22:19:53,670][osd3][DEBUG ] ---> Package ceph-release.noarch 0:1-1.el7 will be erased
[2021-12-10 22:19:53,784][osd3][DEBUG ] --> Finished Dependency Resolution
[2021-12-10 22:19:53,898][osd3][DEBUG ] 
[2021-12-10 22:19:53,899][osd3][DEBUG ] Dependencies Resolved
[2021-12-10 22:19:53,899][osd3][DEBUG ] 
[2021-12-10 22:19:53,899][osd3][DEBUG ] ================================================================================
[2021-12-10 22:19:53,899][osd3][DEBUG ]  Package         Arch      Version       Repository                        Size
[2021-12-10 22:19:53,899][osd3][DEBUG ] ================================================================================
[2021-12-10 22:19:53,899][osd3][DEBUG ] Removing:
[2021-12-10 22:19:53,899][osd3][DEBUG ]  ceph-release    noarch    1-1.el7       @/ceph-release-1-0.el7.noarch    535  
[2021-12-10 22:19:53,899][osd3][DEBUG ] 
[2021-12-10 22:19:53,899][osd3][DEBUG ] Transaction Summary
[2021-12-10 22:19:53,899][osd3][DEBUG ] ================================================================================
[2021-12-10 22:19:53,899][osd3][DEBUG ] Remove  1 Package
[2021-12-10 22:19:53,899][osd3][DEBUG ] 
[2021-12-10 22:19:53,899][osd3][DEBUG ] Installed size: 535  
[2021-12-10 22:19:53,899][osd3][DEBUG ] Downloading packages:
[2021-12-10 22:19:53,899][osd3][DEBUG ] Running transaction check
[2021-12-10 22:19:53,900][osd3][DEBUG ] Running transaction test
[2021-12-10 22:19:53,900][osd3][DEBUG ] Transaction test succeeded
[2021-12-10 22:19:53,900][osd3][DEBUG ] Running transaction
[2021-12-10 22:19:54,264][osd3][DEBUG ]   Erasing    : ceph-release-1-1.el7.noarch                                  1/1 
[2021-12-10 22:19:54,265][osd3][DEBUG ] warning: /etc/yum.repos.d/ceph.repo saved as /etc/yum.repos.d/ceph.repo.rpmsave
[2021-12-10 22:19:54,980][osd3][DEBUG ]   Verifying  : ceph-release-1-1.el7.noarch                                  1/1 
[2021-12-10 22:19:54,981][osd3][DEBUG ] 
[2021-12-10 22:19:54,981][osd3][DEBUG ] Removed:
[2021-12-10 22:19:54,981][osd3][DEBUG ]   ceph-release.noarch 0:1-1.el7                                                 
[2021-12-10 22:19:54,981][osd3][DEBUG ] 
[2021-12-10 22:19:54,981][osd3][DEBUG ] Complete!
[2021-12-10 22:19:55,014][osd3][INFO  ] Running command: sudo yum install -y https://download.ceph.com/rpm-jewel/el7/noarch/ceph-release-1-0.el7.noarch.rpm
[2021-12-10 22:19:55,130][osd3][DEBUG ] Loaded plugins: fastestmirror, priorities
[2021-12-10 22:20:01,061][osd3][DEBUG ] Examining /var/tmp/yum-root-YSfZ49/ceph-release-1-0.el7.noarch.rpm: ceph-release-1-1.el7.noarch
[2021-12-10 22:20:01,061][osd3][DEBUG ] Marking /var/tmp/yum-root-YSfZ49/ceph-release-1-0.el7.noarch.rpm to be installed
[2021-12-10 22:20:01,061][osd3][DEBUG ] Resolving Dependencies
[2021-12-10 22:20:01,062][osd3][DEBUG ] --> Running transaction check
[2021-12-10 22:20:01,062][osd3][DEBUG ] ---> Package ceph-release.noarch 0:1-1.el7 will be installed
[2021-12-10 22:20:01,226][osd3][DEBUG ] --> Finished Dependency Resolution
[2021-12-10 22:20:01,340][osd3][DEBUG ] 
[2021-12-10 22:20:01,340][osd3][DEBUG ] Dependencies Resolved
[2021-12-10 22:20:01,340][osd3][DEBUG ] 
[2021-12-10 22:20:01,340][osd3][DEBUG ] ================================================================================
[2021-12-10 22:20:01,340][osd3][DEBUG ]  Package          Arch       Version     Repository                        Size
[2021-12-10 22:20:01,340][osd3][DEBUG ] ================================================================================
[2021-12-10 22:20:01,340][osd3][DEBUG ] Installing:
[2021-12-10 22:20:01,340][osd3][DEBUG ]  ceph-release     noarch     1-1.el7     /ceph-release-1-0.el7.noarch     535  
[2021-12-10 22:20:01,340][osd3][DEBUG ] 
[2021-12-10 22:20:01,340][osd3][DEBUG ] Transaction Summary
[2021-12-10 22:20:01,340][osd3][DEBUG ] ================================================================================
[2021-12-10 22:20:01,341][osd3][DEBUG ] Install  1 Package
[2021-12-10 22:20:01,341][osd3][DEBUG ] 
[2021-12-10 22:20:01,341][osd3][DEBUG ] Total size: 535  
[2021-12-10 22:20:01,341][osd3][DEBUG ] Installed size: 535  
[2021-12-10 22:20:01,341][osd3][DEBUG ] Downloading packages:
[2021-12-10 22:20:01,341][osd3][DEBUG ] Running transaction check
[2021-12-10 22:20:01,341][osd3][DEBUG ] Running transaction test
[2021-12-10 22:20:01,341][osd3][DEBUG ] Transaction test succeeded
[2021-12-10 22:20:01,341][osd3][DEBUG ] Running transaction
[2021-12-10 22:20:01,906][osd3][DEBUG ]   Installing : ceph-release-1-1.el7.noarch                                  1/1 
[2021-12-10 22:20:02,321][osd3][DEBUG ]   Verifying  : ceph-release-1-1.el7.noarch                                  1/1 
[2021-12-10 22:20:02,321][osd3][DEBUG ] 
[2021-12-10 22:20:02,322][osd3][DEBUG ] Installed:
[2021-12-10 22:20:02,322][osd3][DEBUG ]   ceph-release.noarch 0:1-1.el7                                                 
[2021-12-10 22:20:02,322][osd3][DEBUG ] 
[2021-12-10 22:20:02,322][osd3][DEBUG ] Complete!
[2021-12-10 22:20:02,353][osd3][WARNING] ensuring that /etc/yum.repos.d/ceph.repo contains a high priority
[2021-12-10 22:20:02,356][osd3][WARNING] altered ceph.repo priorities to contain: priority=1
[2021-12-10 22:20:02,357][osd3][INFO  ] Running command: sudo yum -y install ceph ceph-radosgw
[2021-12-10 22:20:02,474][osd3][DEBUG ] Loaded plugins: fastestmirror, priorities
[2021-12-10 22:20:02,505][osd3][DEBUG ] Loading mirror speeds from cached hostfile
[2021-12-10 22:20:02,509][osd3][DEBUG ]  * base: mirror.mobap.edu
[2021-12-10 22:20:02,509][osd3][DEBUG ]  * epel: pubmirror1.math.uh.edu
[2021-12-10 22:20:02,509][osd3][DEBUG ]  * extras: centos.mirror.lstn.net
[2021-12-10 22:20:02,509][osd3][DEBUG ]  * updates: bay.uchicago.edu
[2021-12-10 22:20:08,992][osd3][DEBUG ] 12 packages excluded due to repository priority protections
[2021-12-10 22:20:09,457][osd3][DEBUG ] Package 2:ceph-10.2.11-0.el7.x86_64 already installed and latest version
[2021-12-10 22:20:09,771][osd3][DEBUG ] Package 2:ceph-radosgw-10.2.11-0.el7.x86_64 already installed and latest version
[2021-12-10 22:20:09,771][osd3][DEBUG ] Nothing to do
[2021-12-10 22:20:09,837][osd3][INFO  ] Running command: sudo ceph --version
[2021-12-10 22:20:10,255][osd3][DEBUG ] ceph version 10.2.11 (e4b061b47f07f583c92a050d9e84b1813a35671e)
[2021-12-10 22:20:10,255][ceph_deploy.install][DEBUG ] Detecting platform for host osd4 ...
[2021-12-10 22:20:10,426][osd4][DEBUG ] connection detected need for sudo
[2021-12-10 22:20:10,659][osd4][DEBUG ] connected to host: osd4 
[2021-12-10 22:20:10,660][osd4][DEBUG ] detect platform information from remote host
[2021-12-10 22:20:10,695][osd4][DEBUG ] detect machine type
[2021-12-10 22:20:10,701][ceph_deploy.install][INFO  ] Distro info: CentOS Linux 7.9.2009 Core
[2021-12-10 22:20:10,701][osd4][INFO  ] installing Ceph on osd4
[2021-12-10 22:20:10,703][osd4][INFO  ] Running command: sudo yum clean all
[2021-12-10 22:20:11,873][osd4][DEBUG ] Loaded plugins: fastestmirror, priorities
[2021-12-10 22:20:11,937][osd4][DEBUG ] Cleaning repos: Ceph Ceph-noarch base ceph-source epel extras updates
[2021-12-10 22:20:11,937][osd4][WARNING] Repodata is over 2 weeks old. Install yum-cron? Or run: yum makecache fast
[2021-12-10 22:20:11,952][osd4][DEBUG ] Cleaning up list of fastest mirrors
[2021-12-10 22:20:11,954][osd4][INFO  ] Running command: sudo yum -y install epel-release
[2021-12-10 22:20:12,071][osd4][DEBUG ] Loaded plugins: fastestmirror, priorities
[2021-12-10 22:20:12,103][osd4][DEBUG ] Determining fastest mirrors
[2021-12-10 22:20:49,074][osd4][DEBUG ]  * base: mirror.mobap.edu
[2021-12-10 22:20:49,074][osd4][DEBUG ]  * epel: pubmirror1.math.uh.edu
[2021-12-10 22:20:49,074][osd4][DEBUG ]  * extras: centos.mirror.lstn.net
[2021-12-10 22:20:49,074][osd4][DEBUG ]  * updates: bay.uchicago.edu
[2021-12-10 22:21:48,559][osd4][DEBUG ] 12 packages excluded due to repository priority protections
[2021-12-10 22:21:49,976][osd4][DEBUG ] Resolving Dependencies
[2021-12-10 22:21:49,977][osd4][DEBUG ] --> Running transaction check
[2021-12-10 22:21:49,977][osd4][DEBUG ] ---> Package epel-release.noarch 0:7-11 will be updated
[2021-12-10 22:21:50,041][osd4][DEBUG ] ---> Package epel-release.noarch 0:7-14 will be an update
[2021-12-10 22:21:50,907][osd4][DEBUG ] --> Finished Dependency Resolution
[2021-12-10 22:21:50,971][osd4][DEBUG ] 
[2021-12-10 22:21:50,971][osd4][DEBUG ] Dependencies Resolved
[2021-12-10 22:21:50,971][osd4][DEBUG ] 
[2021-12-10 22:21:50,971][osd4][DEBUG ] ================================================================================
[2021-12-10 22:21:50,971][osd4][DEBUG ]  Package                Arch             Version           Repository      Size
[2021-12-10 22:21:50,971][osd4][DEBUG ] ================================================================================
[2021-12-10 22:21:50,971][osd4][DEBUG ] Updating:
[2021-12-10 22:21:50,972][osd4][DEBUG ]  epel-release           noarch           7-14              epel            15 k
[2021-12-10 22:21:50,972][osd4][DEBUG ] 
[2021-12-10 22:21:50,972][osd4][DEBUG ] Transaction Summary
[2021-12-10 22:21:50,972][osd4][DEBUG ] ================================================================================
[2021-12-10 22:21:50,972][osd4][DEBUG ] Upgrade  1 Package
[2021-12-10 22:21:50,972][osd4][DEBUG ] 
[2021-12-10 22:21:50,972][osd4][DEBUG ] Total download size: 15 k
[2021-12-10 22:21:50,972][osd4][DEBUG ] Downloading packages:
[2021-12-10 22:21:50,972][osd4][DEBUG ] Delta RPMs disabled because /usr/bin/applydeltarpm not installed.
[2021-12-10 22:21:57,103][osd4][DEBUG ] Running transaction check
[2021-12-10 22:21:57,103][osd4][DEBUG ] Running transaction test
[2021-12-10 22:21:57,103][osd4][DEBUG ] Transaction test succeeded
[2021-12-10 22:21:57,103][osd4][DEBUG ] Running transaction
[2021-12-10 22:21:59,424][osd4][DEBUG ]   Updating   : epel-release-7-14.noarch                                     1/2 
[2021-12-10 22:21:59,689][osd4][DEBUG ]   Cleanup    : epel-release-7-11.noarch                                     2/2 
[2021-12-10 22:21:59,803][osd4][DEBUG ]   Verifying  : epel-release-7-14.noarch                                     1/2 
[2021-12-10 22:22:00,118][osd4][DEBUG ]   Verifying  : epel-release-7-11.noarch                                     2/2 
[2021-12-10 22:22:00,118][osd4][DEBUG ] 
[2021-12-10 22:22:00,118][osd4][DEBUG ] Updated:
[2021-12-10 22:22:00,118][osd4][DEBUG ]   epel-release.noarch 0:7-14                                                    
[2021-12-10 22:22:00,118][osd4][DEBUG ] 
[2021-12-10 22:22:00,118][osd4][DEBUG ] Complete!
[2021-12-10 22:22:00,284][osd4][INFO  ] Running command: sudo yum -y install yum-plugin-priorities
[2021-12-10 22:22:00,451][osd4][DEBUG ] Loaded plugins: fastestmirror, priorities
[2021-12-10 22:22:00,482][osd4][DEBUG ] Loading mirror speeds from cached hostfile
[2021-12-10 22:22:00,490][osd4][DEBUG ]  * base: mirror.mobap.edu
[2021-12-10 22:22:00,490][osd4][DEBUG ]  * epel: pubmirror1.math.uh.edu
[2021-12-10 22:22:00,490][osd4][DEBUG ]  * extras: centos.mirror.lstn.net
[2021-12-10 22:22:00,490][osd4][DEBUG ]  * updates: bay.uchicago.edu
[2021-12-10 22:22:01,256][osd4][DEBUG ] 12 packages excluded due to repository priority protections
[2021-12-10 22:22:01,721][osd4][DEBUG ] Package yum-plugin-priorities-1.1.31-54.el7_8.noarch already installed and latest version
[2021-12-10 22:22:01,721][osd4][DEBUG ] Nothing to do
[2021-12-10 22:22:01,785][osd4][DEBUG ] Configure Yum priorities to include obsoletes
[2021-12-10 22:22:01,788][osd4][WARNING] check_obsoletes has been enabled for Yum priorities plugin
[2021-12-10 22:22:01,789][osd4][INFO  ] Running command: sudo rpm --import https://download.ceph.com/keys/release.asc
[2021-12-10 22:22:07,674][osd4][INFO  ] Running command: sudo yum remove -y ceph-release
[2021-12-10 22:22:07,791][osd4][DEBUG ] Loaded plugins: fastestmirror, priorities
[2021-12-10 22:22:07,905][osd4][DEBUG ] Resolving Dependencies
[2021-12-10 22:22:07,905][osd4][DEBUG ] --> Running transaction check
[2021-12-10 22:22:07,905][osd4][DEBUG ] ---> Package ceph-release.noarch 0:1-1.el7 will be erased
[2021-12-10 22:22:08,019][osd4][DEBUG ] --> Finished Dependency Resolution
[2021-12-10 22:22:08,133][osd4][DEBUG ] 
[2021-12-10 22:22:08,134][osd4][DEBUG ] Dependencies Resolved
[2021-12-10 22:22:08,134][osd4][DEBUG ] 
[2021-12-10 22:22:08,134][osd4][DEBUG ] ================================================================================
[2021-12-10 22:22:08,134][osd4][DEBUG ]  Package         Arch      Version       Repository                        Size
[2021-12-10 22:22:08,134][osd4][DEBUG ] ================================================================================
[2021-12-10 22:22:08,134][osd4][DEBUG ] Removing:
[2021-12-10 22:22:08,134][osd4][DEBUG ]  ceph-release    noarch    1-1.el7       @/ceph-release-1-0.el7.noarch    535  
[2021-12-10 22:22:08,134][osd4][DEBUG ] 
[2021-12-10 22:22:08,134][osd4][DEBUG ] Transaction Summary
[2021-12-10 22:22:08,134][osd4][DEBUG ] ================================================================================
[2021-12-10 22:22:08,134][osd4][DEBUG ] Remove  1 Package
[2021-12-10 22:22:08,134][osd4][DEBUG ] 
[2021-12-10 22:22:08,134][osd4][DEBUG ] Installed size: 535  
[2021-12-10 22:22:08,134][osd4][DEBUG ] Downloading packages:
[2021-12-10 22:22:08,134][osd4][DEBUG ] Running transaction check
[2021-12-10 22:22:08,134][osd4][DEBUG ] Running transaction test
[2021-12-10 22:22:08,134][osd4][DEBUG ] Transaction test succeeded
[2021-12-10 22:22:08,134][osd4][DEBUG ] Running transaction
[2021-12-10 22:22:08,399][osd4][DEBUG ]   Erasing    : ceph-release-1-1.el7.noarch                                  1/1 
[2021-12-10 22:22:08,399][osd4][DEBUG ] warning: /etc/yum.repos.d/ceph.repo saved as /etc/yum.repos.d/ceph.repo.rpmsave
[2021-12-10 22:22:09,215][osd4][DEBUG ]   Verifying  : ceph-release-1-1.el7.noarch                                  1/1 
[2021-12-10 22:22:09,215][osd4][DEBUG ] 
[2021-12-10 22:22:09,215][osd4][DEBUG ] Removed:
[2021-12-10 22:22:09,216][osd4][DEBUG ]   ceph-release.noarch 0:1-1.el7                                                 
[2021-12-10 22:22:09,216][osd4][DEBUG ] 
[2021-12-10 22:22:09,216][osd4][DEBUG ] Complete!
[2021-12-10 22:22:09,221][osd4][INFO  ] Running command: sudo yum install -y https://download.ceph.com/rpm-jewel/el7/noarch/ceph-release-1-0.el7.noarch.rpm
[2021-12-10 22:22:09,337][osd4][DEBUG ] Loaded plugins: fastestmirror, priorities
[2021-12-10 22:22:15,117][osd4][DEBUG ] Examining /var/tmp/yum-root-eG7ltK/ceph-release-1-0.el7.noarch.rpm: ceph-release-1-1.el7.noarch
[2021-12-10 22:22:15,118][osd4][DEBUG ] Marking /var/tmp/yum-root-eG7ltK/ceph-release-1-0.el7.noarch.rpm to be installed
[2021-12-10 22:22:15,118][osd4][DEBUG ] Resolving Dependencies
[2021-12-10 22:22:15,118][osd4][DEBUG ] --> Running transaction check
[2021-12-10 22:22:15,118][osd4][DEBUG ] ---> Package ceph-release.noarch 0:1-1.el7 will be installed
[2021-12-10 22:22:15,282][osd4][DEBUG ] --> Finished Dependency Resolution
[2021-12-10 22:22:15,446][osd4][DEBUG ] 
[2021-12-10 22:22:15,446][osd4][DEBUG ] Dependencies Resolved
[2021-12-10 22:22:15,446][osd4][DEBUG ] 
[2021-12-10 22:22:15,446][osd4][DEBUG ] ================================================================================
[2021-12-10 22:22:15,446][osd4][DEBUG ]  Package          Arch       Version     Repository                        Size
[2021-12-10 22:22:15,446][osd4][DEBUG ] ================================================================================
[2021-12-10 22:22:15,446][osd4][DEBUG ] Installing:
[2021-12-10 22:22:15,447][osd4][DEBUG ]  ceph-release     noarch     1-1.el7     /ceph-release-1-0.el7.noarch     535  
[2021-12-10 22:22:15,447][osd4][DEBUG ] 
[2021-12-10 22:22:15,447][osd4][DEBUG ] Transaction Summary
[2021-12-10 22:22:15,447][osd4][DEBUG ] ================================================================================
[2021-12-10 22:22:15,447][osd4][DEBUG ] Install  1 Package
[2021-12-10 22:22:15,447][osd4][DEBUG ] 
[2021-12-10 22:22:15,447][osd4][DEBUG ] Total size: 535  
[2021-12-10 22:22:15,447][osd4][DEBUG ] Installed size: 535  
[2021-12-10 22:22:15,447][osd4][DEBUG ] Downloading packages:
[2021-12-10 22:22:15,447][osd4][DEBUG ] Running transaction check
[2021-12-10 22:22:15,447][osd4][DEBUG ] Running transaction test
[2021-12-10 22:22:15,447][osd4][DEBUG ] Transaction test succeeded
[2021-12-10 22:22:15,447][osd4][DEBUG ] Running transaction
[2021-12-10 22:22:16,163][osd4][DEBUG ]   Installing : ceph-release-1-1.el7.noarch                                  1/1 
[2021-12-10 22:22:16,578][osd4][DEBUG ]   Verifying  : ceph-release-1-1.el7.noarch                                  1/1 
[2021-12-10 22:22:16,578][osd4][DEBUG ] 
[2021-12-10 22:22:16,578][osd4][DEBUG ] Installed:
[2021-12-10 22:22:16,578][osd4][DEBUG ]   ceph-release.noarch 0:1-1.el7                                                 
[2021-12-10 22:22:16,578][osd4][DEBUG ] 
[2021-12-10 22:22:16,578][osd4][DEBUG ] Complete!
[2021-12-10 22:22:16,578][osd4][WARNING] ensuring that /etc/yum.repos.d/ceph.repo contains a high priority
[2021-12-10 22:22:16,581][osd4][WARNING] altered ceph.repo priorities to contain: priority=1
[2021-12-10 22:22:16,582][osd4][INFO  ] Running command: sudo yum -y install ceph ceph-radosgw
[2021-12-10 22:22:16,698][osd4][DEBUG ] Loaded plugins: fastestmirror, priorities
[2021-12-10 22:22:16,763][osd4][DEBUG ] Loading mirror speeds from cached hostfile
[2021-12-10 22:22:16,763][osd4][DEBUG ]  * base: mirror.mobap.edu
[2021-12-10 22:22:16,763][osd4][DEBUG ]  * epel: pubmirror1.math.uh.edu
[2021-12-10 22:22:16,763][osd4][DEBUG ]  * extras: centos.mirror.lstn.net
[2021-12-10 22:22:16,763][osd4][DEBUG ]  * updates: bay.uchicago.edu
[2021-12-10 22:22:23,195][osd4][DEBUG ] 12 packages excluded due to repository priority protections
[2021-12-10 22:22:23,710][osd4][DEBUG ] Package 2:ceph-10.2.11-0.el7.x86_64 already installed and latest version
[2021-12-10 22:22:23,975][osd4][DEBUG ] Package 2:ceph-radosgw-10.2.11-0.el7.x86_64 already installed and latest version
[2021-12-10 22:22:23,975][osd4][DEBUG ] Nothing to do
[2021-12-10 22:22:24,040][osd4][INFO  ] Running command: sudo ceph --version
[2021-12-10 22:22:24,257][osd4][DEBUG ] ceph version 10.2.11 (e4b061b47f07f583c92a050d9e84b1813a35671e)
[2021-12-10 22:22:24,257][ceph_deploy.install][DEBUG ] Detecting platform for host osd5 ...
[2021-12-10 22:22:24,429][osd5][DEBUG ] connection detected need for sudo
[2021-12-10 22:22:24,602][osd5][DEBUG ] connected to host: osd5 
[2021-12-10 22:22:24,602][osd5][DEBUG ] detect platform information from remote host
[2021-12-10 22:22:24,628][osd5][DEBUG ] detect machine type
[2021-12-10 22:22:24,633][ceph_deploy.install][INFO  ] Distro info: CentOS Linux 7.9.2009 Core
[2021-12-10 22:22:24,633][osd5][INFO  ] installing Ceph on osd5
[2021-12-10 22:22:24,635][osd5][INFO  ] Running command: sudo yum clean all
[2021-12-10 22:22:25,203][osd5][DEBUG ] Loaded plugins: fastestmirror, priorities
[2021-12-10 22:22:25,235][osd5][DEBUG ] Cleaning repos: Ceph Ceph-noarch base ceph-source epel extras updates
[2021-12-10 22:22:25,235][osd5][WARNING] Repodata is over 2 weeks old. Install yum-cron? Or run: yum makecache fast
[2021-12-10 22:22:25,235][osd5][DEBUG ] Cleaning up list of fastest mirrors
[2021-12-10 22:22:25,238][osd5][INFO  ] Running command: sudo yum -y install epel-release
[2021-12-10 22:22:25,354][osd5][DEBUG ] Loaded plugins: fastestmirror, priorities
[2021-12-10 22:22:25,418][osd5][DEBUG ] Determining fastest mirrors
[2021-12-10 22:22:57,574][osd5][DEBUG ]  * base: mirror.mobap.edu
[2021-12-10 22:22:57,574][osd5][DEBUG ]  * epel: pubmirror1.math.uh.edu
[2021-12-10 22:22:57,574][osd5][DEBUG ]  * extras: centos.mirror.lstn.net
[2021-12-10 22:22:57,574][osd5][DEBUG ]  * updates: bay.uchicago.edu
[2021-12-10 22:23:50,341][osd5][DEBUG ] 12 packages excluded due to repository priority protections
[2021-12-10 22:23:51,508][osd5][DEBUG ] Resolving Dependencies
[2021-12-10 22:23:51,509][osd5][DEBUG ] --> Running transaction check
[2021-12-10 22:23:51,509][osd5][DEBUG ] ---> Package epel-release.noarch 0:7-11 will be updated
[2021-12-10 22:23:51,509][osd5][DEBUG ] ---> Package epel-release.noarch 0:7-14 will be an update
[2021-12-10 22:23:52,275][osd5][DEBUG ] --> Finished Dependency Resolution
[2021-12-10 22:23:52,283][osd5][DEBUG ] 
[2021-12-10 22:23:52,283][osd5][DEBUG ] Dependencies Resolved
[2021-12-10 22:23:52,283][osd5][DEBUG ] 
[2021-12-10 22:23:52,283][osd5][DEBUG ] ================================================================================
[2021-12-10 22:23:52,283][osd5][DEBUG ]  Package                Arch             Version           Repository      Size
[2021-12-10 22:23:52,283][osd5][DEBUG ] ================================================================================
[2021-12-10 22:23:52,283][osd5][DEBUG ] Updating:
[2021-12-10 22:23:52,283][osd5][DEBUG ]  epel-release           noarch           7-14              epel            15 k
[2021-12-10 22:23:52,283][osd5][DEBUG ] 
[2021-12-10 22:23:52,283][osd5][DEBUG ] Transaction Summary
[2021-12-10 22:23:52,283][osd5][DEBUG ] ================================================================================
[2021-12-10 22:23:52,283][osd5][DEBUG ] Upgrade  1 Package
[2021-12-10 22:23:52,283][osd5][DEBUG ] 
[2021-12-10 22:23:52,283][osd5][DEBUG ] Total download size: 15 k
[2021-12-10 22:23:52,284][osd5][DEBUG ] Downloading packages:
[2021-12-10 22:23:52,284][osd5][DEBUG ] Delta RPMs disabled because /usr/bin/applydeltarpm not installed.
[2021-12-10 22:23:58,014][osd5][DEBUG ] Running transaction check
[2021-12-10 22:23:58,014][osd5][DEBUG ] Running transaction test
[2021-12-10 22:23:58,018][osd5][DEBUG ] Transaction test succeeded
[2021-12-10 22:23:58,018][osd5][DEBUG ] Running transaction
[2021-12-10 22:23:58,934][osd5][DEBUG ]   Updating   : epel-release-7-14.noarch                                     1/2 
[2021-12-10 22:23:59,299][osd5][DEBUG ]   Cleanup    : epel-release-7-11.noarch                                     2/2 
[2021-12-10 22:23:59,463][osd5][DEBUG ]   Verifying  : epel-release-7-14.noarch                                     1/2 
[2021-12-10 22:23:59,678][osd5][DEBUG ]   Verifying  : epel-release-7-11.noarch                                     2/2 
[2021-12-10 22:23:59,678][osd5][DEBUG ] 
[2021-12-10 22:23:59,678][osd5][DEBUG ] Updated:
[2021-12-10 22:23:59,678][osd5][DEBUG ]   epel-release.noarch 0:7-14                                                    
[2021-12-10 22:23:59,678][osd5][DEBUG ] 
[2021-12-10 22:23:59,678][osd5][DEBUG ] Complete!
[2021-12-10 22:23:59,844][osd5][INFO  ] Running command: sudo yum -y install yum-plugin-priorities
[2021-12-10 22:23:59,961][osd5][DEBUG ] Loaded plugins: fastestmirror, priorities
[2021-12-10 22:24:00,025][osd5][DEBUG ] Loading mirror speeds from cached hostfile
[2021-12-10 22:24:00,025][osd5][DEBUG ]  * base: mirror.mobap.edu
[2021-12-10 22:24:00,025][osd5][DEBUG ]  * epel: pubmirror1.math.uh.edu
[2021-12-10 22:24:00,025][osd5][DEBUG ]  * extras: centos.mirror.lstn.net
[2021-12-10 22:24:00,025][osd5][DEBUG ]  * updates: bay.uchicago.edu
[2021-12-10 22:24:00,791][osd5][DEBUG ] 12 packages excluded due to repository priority protections
[2021-12-10 22:24:01,256][osd5][DEBUG ] Package yum-plugin-priorities-1.1.31-54.el7_8.noarch already installed and latest version
[2021-12-10 22:24:01,257][osd5][DEBUG ] Nothing to do
[2021-12-10 22:24:01,321][osd5][DEBUG ] Configure Yum priorities to include obsoletes
[2021-12-10 22:24:01,323][osd5][WARNING] check_obsoletes has been enabled for Yum priorities plugin
[2021-12-10 22:24:01,324][osd5][INFO  ] Running command: sudo rpm --import https://download.ceph.com/keys/release.asc
[2021-12-10 22:24:07,159][osd5][INFO  ] Running command: sudo yum remove -y ceph-release
[2021-12-10 22:24:07,276][osd5][DEBUG ] Loaded plugins: fastestmirror, priorities
[2021-12-10 22:24:07,390][osd5][DEBUG ] Resolving Dependencies
[2021-12-10 22:24:07,390][osd5][DEBUG ] --> Running transaction check
[2021-12-10 22:24:07,390][osd5][DEBUG ] ---> Package ceph-release.noarch 0:1-1.el7 will be erased
[2021-12-10 22:24:07,504][osd5][DEBUG ] --> Finished Dependency Resolution
[2021-12-10 22:24:07,618][osd5][DEBUG ] 
[2021-12-10 22:24:07,618][osd5][DEBUG ] Dependencies Resolved
[2021-12-10 22:24:07,618][osd5][DEBUG ] 
[2021-12-10 22:24:07,619][osd5][DEBUG ] ================================================================================
[2021-12-10 22:24:07,619][osd5][DEBUG ]  Package         Arch      Version       Repository                        Size
[2021-12-10 22:24:07,619][osd5][DEBUG ] ================================================================================
[2021-12-10 22:24:07,619][osd5][DEBUG ] Removing:
[2021-12-10 22:24:07,619][osd5][DEBUG ]  ceph-release    noarch    1-1.el7       @/ceph-release-1-0.el7.noarch    535  
[2021-12-10 22:24:07,619][osd5][DEBUG ] 
[2021-12-10 22:24:07,619][osd5][DEBUG ] Transaction Summary
[2021-12-10 22:24:07,619][osd5][DEBUG ] ================================================================================
[2021-12-10 22:24:07,619][osd5][DEBUG ] Remove  1 Package
[2021-12-10 22:24:07,619][osd5][DEBUG ] 
[2021-12-10 22:24:07,619][osd5][DEBUG ] Installed size: 535  
[2021-12-10 22:24:07,619][osd5][DEBUG ] Downloading packages:
[2021-12-10 22:24:07,619][osd5][DEBUG ] Running transaction check
[2021-12-10 22:24:07,619][osd5][DEBUG ] Running transaction test
[2021-12-10 22:24:07,619][osd5][DEBUG ] Transaction test succeeded
[2021-12-10 22:24:07,619][osd5][DEBUG ] Running transaction
[2021-12-10 22:24:07,884][osd5][DEBUG ]   Erasing    : ceph-release-1-1.el7.noarch                                  1/1 
[2021-12-10 22:24:07,884][osd5][DEBUG ] warning: /etc/yum.repos.d/ceph.repo saved as /etc/yum.repos.d/ceph.repo.rpmsave
[2021-12-10 22:24:08,550][osd5][DEBUG ]   Verifying  : ceph-release-1-1.el7.noarch                                  1/1 
[2021-12-10 22:24:08,550][osd5][DEBUG ] 
[2021-12-10 22:24:08,550][osd5][DEBUG ] Removed:
[2021-12-10 22:24:08,550][osd5][DEBUG ]   ceph-release.noarch 0:1-1.el7                                                 
[2021-12-10 22:24:08,550][osd5][DEBUG ] 
[2021-12-10 22:24:08,550][osd5][DEBUG ] Complete!
[2021-12-10 22:24:08,584][osd5][INFO  ] Running command: sudo yum install -y https://download.ceph.com/rpm-jewel/el7/noarch/ceph-release-1-0.el7.noarch.rpm
[2021-12-10 22:24:08,700][osd5][DEBUG ] Loaded plugins: fastestmirror, priorities
[2021-12-10 22:24:14,481][osd5][DEBUG ] Examining /var/tmp/yum-root-JXTzFr/ceph-release-1-0.el7.noarch.rpm: ceph-release-1-1.el7.noarch
[2021-12-10 22:24:14,481][osd5][DEBUG ] Marking /var/tmp/yum-root-JXTzFr/ceph-release-1-0.el7.noarch.rpm to be installed
[2021-12-10 22:24:14,481][osd5][DEBUG ] Resolving Dependencies
[2021-12-10 22:24:14,481][osd5][DEBUG ] --> Running transaction check
[2021-12-10 22:24:14,481][osd5][DEBUG ] ---> Package ceph-release.noarch 0:1-1.el7 will be installed
[2021-12-10 22:24:14,646][osd5][DEBUG ] --> Finished Dependency Resolution
[2021-12-10 22:24:14,760][osd5][DEBUG ] 
[2021-12-10 22:24:14,760][osd5][DEBUG ] Dependencies Resolved
[2021-12-10 22:24:14,760][osd5][DEBUG ] 
[2021-12-10 22:24:14,760][osd5][DEBUG ] ================================================================================
[2021-12-10 22:24:14,760][osd5][DEBUG ]  Package          Arch       Version     Repository                        Size
[2021-12-10 22:24:14,760][osd5][DEBUG ] ================================================================================
[2021-12-10 22:24:14,760][osd5][DEBUG ] Installing:
[2021-12-10 22:24:14,760][osd5][DEBUG ]  ceph-release     noarch     1-1.el7     /ceph-release-1-0.el7.noarch     535  
[2021-12-10 22:24:14,760][osd5][DEBUG ] 
[2021-12-10 22:24:14,760][osd5][DEBUG ] Transaction Summary
[2021-12-10 22:24:14,761][osd5][DEBUG ] ================================================================================
[2021-12-10 22:24:14,761][osd5][DEBUG ] Install  1 Package
[2021-12-10 22:24:14,761][osd5][DEBUG ] 
[2021-12-10 22:24:14,761][osd5][DEBUG ] Total size: 535  
[2021-12-10 22:24:14,761][osd5][DEBUG ] Installed size: 535  
[2021-12-10 22:24:14,761][osd5][DEBUG ] Downloading packages:
[2021-12-10 22:24:14,761][osd5][DEBUG ] Running transaction check
[2021-12-10 22:24:14,761][osd5][DEBUG ] Running transaction test
[2021-12-10 22:24:14,761][osd5][DEBUG ] Transaction test succeeded
[2021-12-10 22:24:14,761][osd5][DEBUG ] Running transaction
[2021-12-10 22:24:15,376][osd5][DEBUG ]   Installing : ceph-release-1-1.el7.noarch                                  1/1 
[2021-12-10 22:24:15,791][osd5][DEBUG ]   Verifying  : ceph-release-1-1.el7.noarch                                  1/1 
[2021-12-10 22:24:15,792][osd5][DEBUG ] 
[2021-12-10 22:24:15,792][osd5][DEBUG ] Installed:
[2021-12-10 22:24:15,792][osd5][DEBUG ]   ceph-release.noarch 0:1-1.el7                                                 
[2021-12-10 22:24:15,792][osd5][DEBUG ] 
[2021-12-10 22:24:15,792][osd5][DEBUG ] Complete!
[2021-12-10 22:24:15,792][osd5][WARNING] ensuring that /etc/yum.repos.d/ceph.repo contains a high priority
[2021-12-10 22:24:15,794][osd5][WARNING] altered ceph.repo priorities to contain: priority=1
[2021-12-10 22:24:15,796][osd5][INFO  ] Running command: sudo yum -y install ceph ceph-radosgw
[2021-12-10 22:24:15,912][osd5][DEBUG ] Loaded plugins: fastestmirror, priorities
[2021-12-10 22:24:15,976][osd5][DEBUG ] Loading mirror speeds from cached hostfile
[2021-12-10 22:24:15,976][osd5][DEBUG ]  * base: mirror.mobap.edu
[2021-12-10 22:24:15,976][osd5][DEBUG ]  * epel: pubmirror1.math.uh.edu
[2021-12-10 22:24:15,976][osd5][DEBUG ]  * extras: centos.mirror.lstn.net
[2021-12-10 22:24:15,976][osd5][DEBUG ]  * updates: bay.uchicago.edu
[2021-12-10 22:24:22,409][osd5][DEBUG ] 12 packages excluded due to repository priority protections
[2021-12-10 22:24:22,874][osd5][DEBUG ] Package 2:ceph-10.2.11-0.el7.x86_64 already installed and latest version
[2021-12-10 22:24:23,189][osd5][DEBUG ] Package 2:ceph-radosgw-10.2.11-0.el7.x86_64 already installed and latest version
[2021-12-10 22:24:23,189][osd5][DEBUG ] Nothing to do
[2021-12-10 22:24:23,256][osd5][INFO  ] Running command: sudo ceph --version
[2021-12-10 22:24:23,322][osd5][DEBUG ] ceph version 10.2.11 (e4b061b47f07f583c92a050d9e84b1813a35671e)
[2021-12-10 22:24:23,323][ceph_deploy.install][DEBUG ] Detecting platform for host mon1 ...
[2021-12-10 22:24:23,480][mon1][DEBUG ] connection detected need for sudo
[2021-12-10 22:24:23,644][mon1][DEBUG ] connected to host: mon1 
[2021-12-10 22:24:23,644][mon1][DEBUG ] detect platform information from remote host
[2021-12-10 22:24:23,662][mon1][DEBUG ] detect machine type
[2021-12-10 22:24:23,669][ceph_deploy.install][INFO  ] Distro info: CentOS Linux 7.9.2009 Core
[2021-12-10 22:24:23,669][mon1][INFO  ] installing Ceph on mon1
[2021-12-10 22:24:23,672][mon1][INFO  ] Running command: sudo yum clean all
[2021-12-10 22:24:23,789][mon1][DEBUG ] Loaded plugins: fastestmirror, priorities
[2021-12-10 22:24:23,853][mon1][DEBUG ] Cleaning repos: Ceph Ceph-noarch base ceph-source epel extras updates
[2021-12-10 22:24:23,853][mon1][DEBUG ] Cleaning up list of fastest mirrors
[2021-12-10 22:24:23,871][mon1][INFO  ] Running command: sudo yum -y install epel-release
[2021-12-10 22:24:23,987][mon1][DEBUG ] Loaded plugins: fastestmirror, priorities
[2021-12-10 22:24:24,052][mon1][DEBUG ] Determining fastest mirrors
[2021-12-10 22:24:56,308][mon1][DEBUG ]  * base: mirror.mobap.edu
[2021-12-10 22:24:56,308][mon1][DEBUG ]  * epel: pubmirror1.math.uh.edu
[2021-12-10 22:24:56,308][mon1][DEBUG ]  * extras: centos.mirror.lstn.net
[2021-12-10 22:24:56,308][mon1][DEBUG ]  * updates: bay.uchicago.edu
[2021-12-10 22:25:49,175][mon1][DEBUG ] 12 packages excluded due to repository priority protections
[2021-12-10 22:25:49,640][mon1][DEBUG ] Package epel-release-7-14.noarch already installed and latest version
[2021-12-10 22:25:49,640][mon1][DEBUG ] Nothing to do
[2021-12-10 22:25:49,705][mon1][INFO  ] Running command: sudo yum -y install yum-plugin-priorities
[2021-12-10 22:25:49,822][mon1][DEBUG ] Loaded plugins: fastestmirror, priorities
[2021-12-10 22:25:49,886][mon1][DEBUG ] Loading mirror speeds from cached hostfile
[2021-12-10 22:25:49,886][mon1][DEBUG ]  * base: mirror.mobap.edu
[2021-12-10 22:25:49,886][mon1][DEBUG ]  * epel: pubmirror1.math.uh.edu
[2021-12-10 22:25:49,886][mon1][DEBUG ]  * extras: centos.mirror.lstn.net
[2021-12-10 22:25:49,886][mon1][DEBUG ]  * updates: bay.uchicago.edu
[2021-12-10 22:25:50,652][mon1][DEBUG ] 12 packages excluded due to repository priority protections
[2021-12-10 22:25:51,118][mon1][DEBUG ] Package yum-plugin-priorities-1.1.31-54.el7_8.noarch already installed and latest version
[2021-12-10 22:25:51,118][mon1][DEBUG ] Nothing to do
[2021-12-10 22:25:51,182][mon1][DEBUG ] Configure Yum priorities to include obsoletes
[2021-12-10 22:25:51,184][mon1][WARNING] check_obsoletes has been enabled for Yum priorities plugin
[2021-12-10 22:25:51,185][mon1][INFO  ] Running command: sudo rpm --import https://download.ceph.com/keys/release.asc
[2021-12-10 22:25:57,021][mon1][INFO  ] Running command: sudo yum remove -y ceph-release
[2021-12-10 22:25:57,137][mon1][DEBUG ] Loaded plugins: fastestmirror, priorities
[2021-12-10 22:25:57,252][mon1][DEBUG ] Resolving Dependencies
[2021-12-10 22:25:57,252][mon1][DEBUG ] --> Running transaction check
[2021-12-10 22:25:57,252][mon1][DEBUG ] ---> Package ceph-release.noarch 0:1-1.el7 will be erased
[2021-12-10 22:25:57,667][mon1][DEBUG ] --> Finished Dependency Resolution
[2021-12-10 22:25:57,731][mon1][DEBUG ] 
[2021-12-10 22:25:57,731][mon1][DEBUG ] Dependencies Resolved
[2021-12-10 22:25:57,731][mon1][DEBUG ] 
[2021-12-10 22:25:57,732][mon1][DEBUG ] ================================================================================
[2021-12-10 22:25:57,732][mon1][DEBUG ]  Package         Arch      Version       Repository                        Size
[2021-12-10 22:25:57,732][mon1][DEBUG ] ================================================================================
[2021-12-10 22:25:57,732][mon1][DEBUG ] Removing:
[2021-12-10 22:25:57,732][mon1][DEBUG ]  ceph-release    noarch    1-1.el7       @/ceph-release-1-0.el7.noarch    535  
[2021-12-10 22:25:57,732][mon1][DEBUG ] 
[2021-12-10 22:25:57,732][mon1][DEBUG ] Transaction Summary
[2021-12-10 22:25:57,732][mon1][DEBUG ] ================================================================================
[2021-12-10 22:25:57,732][mon1][DEBUG ] Remove  1 Package
[2021-12-10 22:25:57,732][mon1][DEBUG ] 
[2021-12-10 22:25:57,732][mon1][DEBUG ] Installed size: 535  
[2021-12-10 22:25:57,732][mon1][DEBUG ] Downloading packages:
[2021-12-10 22:25:57,732][mon1][DEBUG ] Running transaction check
[2021-12-10 22:25:57,732][mon1][DEBUG ] Running transaction test
[2021-12-10 22:25:57,732][mon1][DEBUG ] Transaction test succeeded
[2021-12-10 22:25:57,732][mon1][DEBUG ] Running transaction
[2021-12-10 22:25:59,702][mon1][DEBUG ]   Erasing    : ceph-release-1-1.el7.noarch                                  1/1 
[2021-12-10 22:25:59,702][mon1][DEBUG ] warning: /etc/yum.repos.d/ceph.repo saved as /etc/yum.repos.d/ceph.repo.rpmsave
[2021-12-10 22:26:00,618][mon1][DEBUG ]   Verifying  : ceph-release-1-1.el7.noarch                                  1/1 
[2021-12-10 22:26:00,619][mon1][DEBUG ] 
[2021-12-10 22:26:00,619][mon1][DEBUG ] Removed:
[2021-12-10 22:26:00,619][mon1][DEBUG ]   ceph-release.noarch 0:1-1.el7                                                 
[2021-12-10 22:26:00,619][mon1][DEBUG ] 
[2021-12-10 22:26:00,619][mon1][DEBUG ] Complete!
[2021-12-10 22:26:00,621][mon1][INFO  ] Running command: sudo yum install -y https://download.ceph.com/rpm-jewel/el7/noarch/ceph-release-1-0.el7.noarch.rpm
[2021-12-10 22:26:00,737][mon1][DEBUG ] Loaded plugins: fastestmirror, priorities
[2021-12-10 22:26:06,769][mon1][DEBUG ] Examining /var/tmp/yum-root-frbOM9/ceph-release-1-0.el7.noarch.rpm: ceph-release-1-1.el7.noarch
[2021-12-10 22:26:06,769][mon1][DEBUG ] Marking /var/tmp/yum-root-frbOM9/ceph-release-1-0.el7.noarch.rpm to be installed
[2021-12-10 22:26:06,769][mon1][DEBUG ] Resolving Dependencies
[2021-12-10 22:26:06,769][mon1][DEBUG ] --> Running transaction check
[2021-12-10 22:26:06,769][mon1][DEBUG ] ---> Package ceph-release.noarch 0:1-1.el7 will be installed
[2021-12-10 22:26:06,983][mon1][DEBUG ] --> Finished Dependency Resolution
[2021-12-10 22:26:07,148][mon1][DEBUG ] 
[2021-12-10 22:26:07,148][mon1][DEBUG ] Dependencies Resolved
[2021-12-10 22:26:07,148][mon1][DEBUG ] 
[2021-12-10 22:26:07,148][mon1][DEBUG ] ================================================================================
[2021-12-10 22:26:07,148][mon1][DEBUG ]  Package          Arch       Version     Repository                        Size
[2021-12-10 22:26:07,148][mon1][DEBUG ] ================================================================================
[2021-12-10 22:26:07,148][mon1][DEBUG ] Installing:
[2021-12-10 22:26:07,148][mon1][DEBUG ]  ceph-release     noarch     1-1.el7     /ceph-release-1-0.el7.noarch     535  
[2021-12-10 22:26:07,149][mon1][DEBUG ] 
[2021-12-10 22:26:07,149][mon1][DEBUG ] Transaction Summary
[2021-12-10 22:26:07,149][mon1][DEBUG ] ================================================================================
[2021-12-10 22:26:07,149][mon1][DEBUG ] Install  1 Package
[2021-12-10 22:26:07,149][mon1][DEBUG ] 
[2021-12-10 22:26:07,149][mon1][DEBUG ] Total size: 535  
[2021-12-10 22:26:07,149][mon1][DEBUG ] Installed size: 535  
[2021-12-10 22:26:07,149][mon1][DEBUG ] Downloading packages:
[2021-12-10 22:26:07,149][mon1][DEBUG ] Running transaction check
[2021-12-10 22:26:07,149][mon1][DEBUG ] Running transaction test
[2021-12-10 22:26:07,149][mon1][DEBUG ] Transaction test succeeded
[2021-12-10 22:26:07,149][mon1][DEBUG ] Running transaction
[2021-12-10 22:26:07,715][mon1][DEBUG ]   Installing : ceph-release-1-1.el7.noarch                                  1/1 
[2021-12-10 22:26:08,079][mon1][DEBUG ]   Verifying  : ceph-release-1-1.el7.noarch                                  1/1 
[2021-12-10 22:26:08,080][mon1][DEBUG ] 
[2021-12-10 22:26:08,080][mon1][DEBUG ] Installed:
[2021-12-10 22:26:08,080][mon1][DEBUG ]   ceph-release.noarch 0:1-1.el7                                                 
[2021-12-10 22:26:08,080][mon1][DEBUG ] 
[2021-12-10 22:26:08,080][mon1][DEBUG ] Complete!
[2021-12-10 22:26:08,112][mon1][WARNING] ensuring that /etc/yum.repos.d/ceph.repo contains a high priority
[2021-12-10 22:26:08,114][mon1][WARNING] altered ceph.repo priorities to contain: priority=1
[2021-12-10 22:26:08,116][mon1][INFO  ] Running command: sudo yum -y install ceph ceph-radosgw
[2021-12-10 22:26:08,232][mon1][DEBUG ] Loaded plugins: fastestmirror, priorities
[2021-12-10 22:26:08,296][mon1][DEBUG ] Loading mirror speeds from cached hostfile
[2021-12-10 22:26:08,296][mon1][DEBUG ]  * base: mirror.mobap.edu
[2021-12-10 22:26:08,296][mon1][DEBUG ]  * epel: pubmirror1.math.uh.edu
[2021-12-10 22:26:08,296][mon1][DEBUG ]  * extras: centos.mirror.lstn.net
[2021-12-10 22:26:08,296][mon1][DEBUG ]  * updates: bay.uchicago.edu
[2021-12-10 22:26:14,778][mon1][DEBUG ] 12 packages excluded due to repository priority protections
[2021-12-10 22:26:15,244][mon1][DEBUG ] Package 2:ceph-10.2.11-0.el7.x86_64 already installed and latest version
[2021-12-10 22:26:15,559][mon1][DEBUG ] Package 2:ceph-radosgw-10.2.11-0.el7.x86_64 already installed and latest version
[2021-12-10 22:26:15,559][mon1][DEBUG ] Nothing to do
[2021-12-10 22:26:15,592][mon1][INFO  ] Running command: sudo ceph --version
[2021-12-10 22:26:15,658][mon1][DEBUG ] ceph version 10.2.11 (e4b061b47f07f583c92a050d9e84b1813a35671e)
[2021-12-10 22:56:46,885][ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephuser/.cephdeploy.conf
[2021-12-10 22:56:46,885][ceph_deploy.cli][INFO  ] Invoked (1.5.39): /usr/bin/ceph-deploy admin client
[2021-12-10 22:56:46,885][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2021-12-10 22:56:46,885][ceph_deploy.cli][INFO  ]  username                      : None
[2021-12-10 22:56:46,885][ceph_deploy.cli][INFO  ]  verbose                       : False
[2021-12-10 22:56:46,885][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2021-12-10 22:56:46,885][ceph_deploy.cli][INFO  ]  quiet                         : False
[2021-12-10 22:56:46,885][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f234b64eb48>
[2021-12-10 22:56:46,885][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2021-12-10 22:56:46,885][ceph_deploy.cli][INFO  ]  client                        : ['client']
[2021-12-10 22:56:46,885][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7f234c367a28>
[2021-12-10 22:56:46,885][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2021-12-10 22:56:46,885][ceph_deploy.cli][INFO  ]  default_release               : False
[2021-12-10 22:56:46,886][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to client
[2021-12-10 22:56:49,878][client][DEBUG ] connection detected need for sudo
[2021-12-10 22:56:52,274][client][DEBUG ] connected to host: client 
[2021-12-10 22:56:52,275][client][DEBUG ] detect platform information from remote host
[2021-12-10 22:56:52,361][client][DEBUG ] detect machine type
[2021-12-10 22:56:52,366][client][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2021-12-10 22:56:52,368][ceph_deploy.admin][ERROR ] RuntimeError: config file /etc/ceph/ceph.conf exists with different content; use --overwrite-conf to overwrite
[2021-12-10 22:56:52,368][ceph_deploy][ERROR ] GenericError: Failed to configure 1 admin hosts

[2021-12-10 22:56:58,727][ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephuser/.cephdeploy.conf
[2021-12-10 22:56:58,727][ceph_deploy.cli][INFO  ] Invoked (1.5.39): /usr/bin/ceph-deploy --overwrite-conf admin client
[2021-12-10 22:56:58,727][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2021-12-10 22:56:58,727][ceph_deploy.cli][INFO  ]  username                      : None
[2021-12-10 22:56:58,728][ceph_deploy.cli][INFO  ]  verbose                       : False
[2021-12-10 22:56:58,728][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2021-12-10 22:56:58,728][ceph_deploy.cli][INFO  ]  quiet                         : False
[2021-12-10 22:56:58,728][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7eff74395b48>
[2021-12-10 22:56:58,728][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2021-12-10 22:56:58,728][ceph_deploy.cli][INFO  ]  client                        : ['client']
[2021-12-10 22:56:58,728][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7eff750aea28>
[2021-12-10 22:56:58,728][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2021-12-10 22:56:58,728][ceph_deploy.cli][INFO  ]  default_release               : False
[2021-12-10 22:56:58,728][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to client
[2021-12-10 22:57:00,629][client][DEBUG ] connection detected need for sudo
[2021-12-10 22:57:02,396][client][DEBUG ] connected to host: client 
[2021-12-10 22:57:02,396][client][DEBUG ] detect platform information from remote host
[2021-12-10 22:57:02,413][client][DEBUG ] detect machine type
[2021-12-10 22:57:02,418][client][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2021-12-10 22:58:49,691][ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephuser/.cephdeploy.conf
[2021-12-10 22:58:49,691][ceph_deploy.cli][INFO  ] Invoked (1.5.39): /usr/bin/ceph-deploy --overwrite-conf admin client
[2021-12-10 22:58:49,691][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2021-12-10 22:58:49,691][ceph_deploy.cli][INFO  ]  username                      : None
[2021-12-10 22:58:49,691][ceph_deploy.cli][INFO  ]  verbose                       : False
[2021-12-10 22:58:49,691][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2021-12-10 22:58:49,691][ceph_deploy.cli][INFO  ]  quiet                         : False
[2021-12-10 22:58:49,692][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f04a50b4b48>
[2021-12-10 22:58:49,692][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2021-12-10 22:58:49,692][ceph_deploy.cli][INFO  ]  client                        : ['client']
[2021-12-10 22:58:49,692][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7f04a5dcda28>
[2021-12-10 22:58:49,692][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2021-12-10 22:58:49,692][ceph_deploy.cli][INFO  ]  default_release               : False
[2021-12-10 22:58:49,692][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to client
[2021-12-10 22:58:52,654][client][DEBUG ] connection detected need for sudo
[2021-12-10 22:58:56,603][client][DEBUG ] connected to host: client 
[2021-12-10 22:58:56,604][client][DEBUG ] detect platform information from remote host
[2021-12-10 22:58:56,622][client][DEBUG ] detect machine type
[2021-12-10 22:58:56,627][client][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2021-12-10 23:01:39,950][ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephuser/.cephdeploy.conf
[2021-12-10 23:01:39,951][ceph_deploy.cli][INFO  ] Invoked (1.5.39): /usr/bin/ceph-deploy admin ceph-admin cleint
[2021-12-10 23:01:39,951][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2021-12-10 23:01:39,951][ceph_deploy.cli][INFO  ]  username                      : None
[2021-12-10 23:01:39,951][ceph_deploy.cli][INFO  ]  verbose                       : False
[2021-12-10 23:01:39,951][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2021-12-10 23:01:39,951][ceph_deploy.cli][INFO  ]  quiet                         : False
[2021-12-10 23:01:39,951][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fcf87be4b48>
[2021-12-10 23:01:39,951][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2021-12-10 23:01:39,951][ceph_deploy.cli][INFO  ]  client                        : ['ceph-admin', 'cleint']
[2021-12-10 23:01:39,951][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7fcf888fda28>
[2021-12-10 23:01:39,952][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2021-12-10 23:01:39,952][ceph_deploy.cli][INFO  ]  default_release               : False
[2021-12-10 23:01:39,952][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to ceph-admin
[2021-12-10 23:01:39,985][ceph-admin][DEBUG ] connection detected need for sudo
[2021-12-10 23:01:40,014][ceph-admin][DEBUG ] connected to host: ceph-admin 
[2021-12-10 23:01:40,015][ceph-admin][DEBUG ] detect platform information from remote host
[2021-12-10 23:01:40,032][ceph-admin][DEBUG ] detect machine type
[2021-12-10 23:01:40,036][ceph-admin][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2021-12-10 23:01:40,037][ceph_deploy.admin][ERROR ] RuntimeError: config file /etc/ceph/ceph.conf exists with different content; use --overwrite-conf to overwrite
[2021-12-10 23:01:40,038][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to cleint
[2021-12-10 23:01:43,551][ceph_deploy][ERROR ] KeyboardInterrupt

[2021-12-10 23:02:12,696][ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephuser/.cephdeploy.conf
[2021-12-10 23:02:12,696][ceph_deploy.cli][INFO  ] Invoked (1.5.39): /usr/bin/ceph-deploy admin ceph-admin cleint
[2021-12-10 23:02:12,696][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2021-12-10 23:02:12,696][ceph_deploy.cli][INFO  ]  username                      : None
[2021-12-10 23:02:12,696][ceph_deploy.cli][INFO  ]  verbose                       : False
[2021-12-10 23:02:12,696][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2021-12-10 23:02:12,696][ceph_deploy.cli][INFO  ]  quiet                         : False
[2021-12-10 23:02:12,696][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7faef6d2eb48>
[2021-12-10 23:02:12,696][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2021-12-10 23:02:12,696][ceph_deploy.cli][INFO  ]  client                        : ['ceph-admin', 'cleint']
[2021-12-10 23:02:12,696][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7faef7a47a28>
[2021-12-10 23:02:12,696][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2021-12-10 23:02:12,696][ceph_deploy.cli][INFO  ]  default_release               : False
[2021-12-10 23:02:12,697][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to ceph-admin
[2021-12-10 23:02:12,730][ceph-admin][DEBUG ] connection detected need for sudo
[2021-12-10 23:02:12,756][ceph-admin][DEBUG ] connected to host: ceph-admin 
[2021-12-10 23:02:12,757][ceph-admin][DEBUG ] detect platform information from remote host
[2021-12-10 23:02:12,773][ceph-admin][DEBUG ] detect machine type
[2021-12-10 23:02:12,777][ceph-admin][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2021-12-10 23:02:12,778][ceph_deploy.admin][ERROR ] RuntimeError: config file /etc/ceph/ceph.conf exists with different content; use --overwrite-conf to overwrite
[2021-12-10 23:02:12,778][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to cleint
[2021-12-10 23:02:17,801][ceph_deploy.admin][ERROR ] connecting to host: cleint resulted in errors: HostNotFound cleint
[2021-12-10 23:02:17,802][ceph_deploy][ERROR ] GenericError: Failed to configure 2 admin hosts

[2021-12-10 23:04:34,826][ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephuser/.cephdeploy.conf
[2021-12-10 23:04:34,826][ceph_deploy.cli][INFO  ] Invoked (1.5.39): /usr/bin/ceph-deploy admin client
[2021-12-10 23:04:34,826][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2021-12-10 23:04:34,826][ceph_deploy.cli][INFO  ]  username                      : None
[2021-12-10 23:04:34,826][ceph_deploy.cli][INFO  ]  verbose                       : False
[2021-12-10 23:04:34,827][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2021-12-10 23:04:34,827][ceph_deploy.cli][INFO  ]  quiet                         : False
[2021-12-10 23:04:34,827][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f98b9c64b48>
[2021-12-10 23:04:34,827][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2021-12-10 23:04:34,827][ceph_deploy.cli][INFO  ]  client                        : ['client']
[2021-12-10 23:04:34,827][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7f98ba97da28>
[2021-12-10 23:04:34,827][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2021-12-10 23:04:34,827][ceph_deploy.cli][INFO  ]  default_release               : False
[2021-12-10 23:04:34,827][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to client
[2021-12-10 23:04:38,951][client][DEBUG ] connection detected need for sudo
[2021-12-10 23:04:43,009][client][DEBUG ] connected to host: client 
[2021-12-10 23:04:43,010][client][DEBUG ] detect platform information from remote host
[2021-12-10 23:04:43,027][client][DEBUG ] detect machine type
[2021-12-10 23:04:43,032][client][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2021-12-10 23:09:21,411][ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephuser/.cephdeploy.conf
[2021-12-10 23:09:21,411][ceph_deploy.cli][INFO  ] Invoked (1.5.39): /usr/bin/ceph-deploy admin client
[2021-12-10 23:09:21,411][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2021-12-10 23:09:21,411][ceph_deploy.cli][INFO  ]  username                      : None
[2021-12-10 23:09:21,411][ceph_deploy.cli][INFO  ]  verbose                       : False
[2021-12-10 23:09:21,411][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2021-12-10 23:09:21,411][ceph_deploy.cli][INFO  ]  quiet                         : False
[2021-12-10 23:09:21,412][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f3fc6007b48>
[2021-12-10 23:09:21,412][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2021-12-10 23:09:21,412][ceph_deploy.cli][INFO  ]  client                        : ['client']
[2021-12-10 23:09:21,412][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7f3fc6d20a28>
[2021-12-10 23:09:21,412][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2021-12-10 23:09:21,412][ceph_deploy.cli][INFO  ]  default_release               : False
[2021-12-10 23:09:21,412][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to client
[2021-12-10 23:09:21,576][client][DEBUG ] connection detected need for sudo
[2021-12-10 23:09:21,741][client][DEBUG ] connected to host: client 
[2021-12-10 23:09:21,742][client][DEBUG ] detect platform information from remote host
[2021-12-10 23:09:21,760][client][DEBUG ] detect machine type
[2021-12-10 23:09:21,765][client][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2021-12-10 23:16:40,653][ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephuser/.cephdeploy.conf
[2021-12-10 23:16:40,654][ceph_deploy.cli][INFO  ] Invoked (1.5.39): /usr/bin/ceph-deploy admin client
[2021-12-10 23:16:40,654][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2021-12-10 23:16:40,654][ceph_deploy.cli][INFO  ]  username                      : None
[2021-12-10 23:16:40,654][ceph_deploy.cli][INFO  ]  verbose                       : False
[2021-12-10 23:16:40,654][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2021-12-10 23:16:40,654][ceph_deploy.cli][INFO  ]  quiet                         : False
[2021-12-10 23:16:40,654][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f20e8ffab48>
[2021-12-10 23:16:40,654][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2021-12-10 23:16:40,654][ceph_deploy.cli][INFO  ]  client                        : ['client']
[2021-12-10 23:16:40,654][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7f20e9d13a28>
[2021-12-10 23:16:40,654][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2021-12-10 23:16:40,655][ceph_deploy.cli][INFO  ]  default_release               : False
[2021-12-10 23:16:40,655][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to client
[2021-12-10 23:16:40,820][client][DEBUG ] connection detected need for sudo
[2021-12-10 23:16:40,987][client][DEBUG ] connected to host: client 
[2021-12-10 23:16:40,987][client][DEBUG ] detect platform information from remote host
[2021-12-10 23:16:41,004][client][DEBUG ] detect machine type
[2021-12-10 23:16:41,009][client][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2021-12-10 23:16:48,187][ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephuser/.cephdeploy.conf
[2021-12-10 23:16:48,188][ceph_deploy.cli][INFO  ] Invoked (1.5.39): /usr/bin/ceph-deploy install client
[2021-12-10 23:16:48,188][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2021-12-10 23:16:48,188][ceph_deploy.cli][INFO  ]  verbose                       : False
[2021-12-10 23:16:48,188][ceph_deploy.cli][INFO  ]  testing                       : None
[2021-12-10 23:16:48,188][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f7fbd09a200>
[2021-12-10 23:16:48,188][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2021-12-10 23:16:48,188][ceph_deploy.cli][INFO  ]  dev_commit                    : None
[2021-12-10 23:16:48,188][ceph_deploy.cli][INFO  ]  install_mds                   : False
[2021-12-10 23:16:48,188][ceph_deploy.cli][INFO  ]  stable                        : None
[2021-12-10 23:16:48,188][ceph_deploy.cli][INFO  ]  default_release               : False
[2021-12-10 23:16:48,188][ceph_deploy.cli][INFO  ]  username                      : None
[2021-12-10 23:16:48,188][ceph_deploy.cli][INFO  ]  adjust_repos                  : True
[2021-12-10 23:16:48,188][ceph_deploy.cli][INFO  ]  func                          : <function install at 0x7f7fbdd5ede8>
[2021-12-10 23:16:48,188][ceph_deploy.cli][INFO  ]  install_mgr                   : False
[2021-12-10 23:16:48,189][ceph_deploy.cli][INFO  ]  install_all                   : False
[2021-12-10 23:16:48,189][ceph_deploy.cli][INFO  ]  repo                          : False
[2021-12-10 23:16:48,189][ceph_deploy.cli][INFO  ]  host                          : ['client']
[2021-12-10 23:16:48,189][ceph_deploy.cli][INFO  ]  install_rgw                   : False
[2021-12-10 23:16:48,189][ceph_deploy.cli][INFO  ]  install_tests                 : False
[2021-12-10 23:16:48,189][ceph_deploy.cli][INFO  ]  repo_url                      : None
[2021-12-10 23:16:48,189][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2021-12-10 23:16:48,189][ceph_deploy.cli][INFO  ]  install_osd                   : False
[2021-12-10 23:16:48,189][ceph_deploy.cli][INFO  ]  version_kind                  : stable
[2021-12-10 23:16:48,189][ceph_deploy.cli][INFO  ]  install_common                : False
[2021-12-10 23:16:48,189][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2021-12-10 23:16:48,189][ceph_deploy.cli][INFO  ]  quiet                         : False
[2021-12-10 23:16:48,189][ceph_deploy.cli][INFO  ]  dev                           : master
[2021-12-10 23:16:48,189][ceph_deploy.cli][INFO  ]  nogpgcheck                    : False
[2021-12-10 23:16:48,189][ceph_deploy.cli][INFO  ]  local_mirror                  : None
[2021-12-10 23:16:48,189][ceph_deploy.cli][INFO  ]  release                       : None
[2021-12-10 23:16:48,189][ceph_deploy.cli][INFO  ]  install_mon                   : False
[2021-12-10 23:16:48,189][ceph_deploy.cli][INFO  ]  gpg_url                       : None
[2021-12-10 23:16:48,189][ceph_deploy.install][DEBUG ] Installing stable version jewel on cluster ceph hosts client
[2021-12-10 23:16:48,190][ceph_deploy.install][DEBUG ] Detecting platform for host client ...
[2021-12-10 23:16:48,358][client][DEBUG ] connection detected need for sudo
[2021-12-10 23:16:48,526][client][DEBUG ] connected to host: client 
[2021-12-10 23:16:48,527][client][DEBUG ] detect platform information from remote host
[2021-12-10 23:16:48,543][client][DEBUG ] detect machine type
[2021-12-10 23:16:48,548][ceph_deploy.install][INFO  ] Distro info: CentOS Linux 7.9.2009 Core
[2021-12-10 23:16:48,549][client][INFO  ] installing Ceph on client
[2021-12-10 23:16:48,551][client][INFO  ] Running command: sudo yum clean all
[2021-12-10 23:16:49,572][client][DEBUG ] Loaded plugins: fastestmirror, priorities
[2021-12-10 23:16:49,687][client][DEBUG ] Cleaning repos: Ceph Ceph-noarch base ceph-source epel extras updates
[2021-12-10 23:16:49,687][client][WARNING] Repodata is over 2 weeks old. Install yum-cron? Or run: yum makecache fast
[2021-12-10 23:16:49,687][client][DEBUG ] Cleaning up list of fastest mirrors
[2021-12-10 23:16:49,689][client][INFO  ] Running command: sudo yum -y install epel-release
[2021-12-10 23:16:49,805][client][DEBUG ] Loaded plugins: fastestmirror, priorities
[2021-12-10 23:16:49,837][client][DEBUG ] Determining fastest mirrors
[2021-12-10 23:17:22,119][client][DEBUG ]  * base: centos.mirrors.tds.net
[2021-12-10 23:17:22,119][client][DEBUG ]  * epel: pubmirror1.math.uh.edu
[2021-12-10 23:17:22,119][client][DEBUG ]  * extras: repo.ialab.dsu.edu
[2021-12-10 23:17:22,119][client][DEBUG ]  * updates: mirror.compevo.com
[2021-12-10 23:18:08,079][client][DEBUG ] 12 packages excluded due to repository priority protections
[2021-12-10 23:18:10,449][client][DEBUG ] Resolving Dependencies
[2021-12-10 23:18:10,450][client][DEBUG ] --> Running transaction check
[2021-12-10 23:18:10,450][client][DEBUG ] ---> Package epel-release.noarch 0:7-11 will be updated
[2021-12-10 23:18:10,514][client][DEBUG ] ---> Package epel-release.noarch 0:7-14 will be an update
[2021-12-10 23:18:11,730][client][DEBUG ] --> Finished Dependency Resolution
[2021-12-10 23:18:11,794][client][DEBUG ] 
[2021-12-10 23:18:11,795][client][DEBUG ] Dependencies Resolved
[2021-12-10 23:18:11,795][client][DEBUG ] 
[2021-12-10 23:18:11,795][client][DEBUG ] ================================================================================
[2021-12-10 23:18:11,795][client][DEBUG ]  Package                Arch             Version           Repository      Size
[2021-12-10 23:18:11,795][client][DEBUG ] ================================================================================
[2021-12-10 23:18:11,795][client][DEBUG ] Updating:
[2021-12-10 23:18:11,795][client][DEBUG ]  epel-release           noarch           7-14              epel            15 k
[2021-12-10 23:18:11,795][client][DEBUG ] 
[2021-12-10 23:18:11,795][client][DEBUG ] Transaction Summary
[2021-12-10 23:18:11,795][client][DEBUG ] ================================================================================
[2021-12-10 23:18:11,795][client][DEBUG ] Upgrade  1 Package
[2021-12-10 23:18:11,795][client][DEBUG ] 
[2021-12-10 23:18:11,795][client][DEBUG ] Total download size: 15 k
[2021-12-10 23:18:11,795][client][DEBUG ] Downloading packages:
[2021-12-10 23:18:11,795][client][DEBUG ] Delta RPMs disabled because /usr/bin/applydeltarpm not installed.
[2021-12-10 23:18:17,474][client][DEBUG ] Running transaction check
[2021-12-10 23:18:17,474][client][DEBUG ] Running transaction test
[2021-12-10 23:18:17,538][client][DEBUG ] Transaction test succeeded
[2021-12-10 23:18:17,538][client][DEBUG ] Running transaction
[2021-12-10 23:18:18,856][client][DEBUG ]   Updating   : epel-release-7-14.noarch                                     1/2 
[2021-12-10 23:18:19,170][client][DEBUG ]   Cleanup    : epel-release-7-11.noarch                                     2/2 
[2021-12-10 23:18:19,334][client][DEBUG ]   Verifying  : epel-release-7-14.noarch                                     1/2 
[2021-12-10 23:18:19,599][client][DEBUG ]   Verifying  : epel-release-7-11.noarch                                     2/2 
[2021-12-10 23:18:19,599][client][DEBUG ] 
[2021-12-10 23:18:19,599][client][DEBUG ] Updated:
[2021-12-10 23:18:19,599][client][DEBUG ]   epel-release.noarch 0:7-14                                                    
[2021-12-10 23:18:19,599][client][DEBUG ] 
[2021-12-10 23:18:19,599][client][DEBUG ] Complete!
[2021-12-10 23:18:19,765][client][INFO  ] Running command: sudo yum -y install yum-plugin-priorities
[2021-12-10 23:18:19,882][client][DEBUG ] Loaded plugins: fastestmirror, priorities
[2021-12-10 23:18:19,946][client][DEBUG ] Loading mirror speeds from cached hostfile
[2021-12-10 23:18:19,946][client][DEBUG ]  * base: centos.mirrors.tds.net
[2021-12-10 23:18:19,946][client][DEBUG ]  * epel: pubmirror1.math.uh.edu
[2021-12-10 23:18:19,946][client][DEBUG ]  * extras: repo.ialab.dsu.edu
[2021-12-10 23:18:19,946][client][DEBUG ]  * updates: mirror.compevo.com
[2021-12-10 23:18:20,662][client][DEBUG ] 12 packages excluded due to repository priority protections
[2021-12-10 23:18:21,177][client][DEBUG ] Package yum-plugin-priorities-1.1.31-54.el7_8.noarch already installed and latest version
[2021-12-10 23:18:21,177][client][DEBUG ] Nothing to do
[2021-12-10 23:18:21,241][client][DEBUG ] Configure Yum priorities to include obsoletes
[2021-12-10 23:18:21,243][client][WARNING] check_obsoletes has been enabled for Yum priorities plugin
[2021-12-10 23:18:21,245][client][INFO  ] Running command: sudo rpm --import https://download.ceph.com/keys/release.asc
[2021-12-10 23:18:27,078][client][INFO  ] Running command: sudo yum remove -y ceph-release
[2021-12-10 23:18:27,194][client][DEBUG ] Loaded plugins: fastestmirror, priorities
[2021-12-10 23:18:27,308][client][DEBUG ] Resolving Dependencies
[2021-12-10 23:18:27,308][client][DEBUG ] --> Running transaction check
[2021-12-10 23:18:27,309][client][DEBUG ] ---> Package ceph-release.noarch 0:1-1.el7 will be erased
[2021-12-10 23:18:27,473][client][DEBUG ] --> Finished Dependency Resolution
[2021-12-10 23:18:27,587][client][DEBUG ] 
[2021-12-10 23:18:27,587][client][DEBUG ] Dependencies Resolved
[2021-12-10 23:18:27,587][client][DEBUG ] 
[2021-12-10 23:18:27,587][client][DEBUG ] ================================================================================
[2021-12-10 23:18:27,587][client][DEBUG ]  Package         Arch      Version       Repository                        Size
[2021-12-10 23:18:27,587][client][DEBUG ] ================================================================================
[2021-12-10 23:18:27,587][client][DEBUG ] Removing:
[2021-12-10 23:18:27,587][client][DEBUG ]  ceph-release    noarch    1-1.el7       @/ceph-release-1-0.el7.noarch    535  
[2021-12-10 23:18:27,587][client][DEBUG ] 
[2021-12-10 23:18:27,587][client][DEBUG ] Transaction Summary
[2021-12-10 23:18:27,587][client][DEBUG ] ================================================================================
[2021-12-10 23:18:27,587][client][DEBUG ] Remove  1 Package
[2021-12-10 23:18:27,587][client][DEBUG ] 
[2021-12-10 23:18:27,588][client][DEBUG ] Installed size: 535  
[2021-12-10 23:18:27,588][client][DEBUG ] Downloading packages:
[2021-12-10 23:18:27,588][client][DEBUG ] Running transaction check
[2021-12-10 23:18:27,588][client][DEBUG ] Running transaction test
[2021-12-10 23:18:27,588][client][DEBUG ] Transaction test succeeded
[2021-12-10 23:18:27,588][client][DEBUG ] Running transaction
[2021-12-10 23:18:27,802][client][DEBUG ]   Erasing    : ceph-release-1-1.el7.noarch                                  1/1 
[2021-12-10 23:18:27,802][client][DEBUG ] warning: /etc/yum.repos.d/ceph.repo saved as /etc/yum.repos.d/ceph.repo.rpmsave
[2021-12-10 23:18:28,468][client][DEBUG ]   Verifying  : ceph-release-1-1.el7.noarch                                  1/1 
[2021-12-10 23:18:28,468][client][DEBUG ] 
[2021-12-10 23:18:28,468][client][DEBUG ] Removed:
[2021-12-10 23:18:28,468][client][DEBUG ]   ceph-release.noarch 0:1-1.el7                                                 
[2021-12-10 23:18:28,468][client][DEBUG ] 
[2021-12-10 23:18:28,468][client][DEBUG ] Complete!
[2021-12-10 23:18:28,470][client][INFO  ] Running command: sudo yum install -y https://download.ceph.com/rpm-jewel/el7/noarch/ceph-release-1-0.el7.noarch.rpm
[2021-12-10 23:18:28,586][client][DEBUG ] Loaded plugins: fastestmirror, priorities
[2021-12-10 23:18:34,416][client][DEBUG ] Examining /var/tmp/yum-root-3skFzx/ceph-release-1-0.el7.noarch.rpm: ceph-release-1-1.el7.noarch
[2021-12-10 23:18:34,416][client][DEBUG ] Marking /var/tmp/yum-root-3skFzx/ceph-release-1-0.el7.noarch.rpm to be installed
[2021-12-10 23:18:34,416][client][DEBUG ] Resolving Dependencies
[2021-12-10 23:18:34,416][client][DEBUG ] --> Running transaction check
[2021-12-10 23:18:34,416][client][DEBUG ] ---> Package ceph-release.noarch 0:1-1.el7 will be installed
[2021-12-10 23:18:34,580][client][DEBUG ] --> Finished Dependency Resolution
[2021-12-10 23:18:34,744][client][DEBUG ] 
[2021-12-10 23:18:34,744][client][DEBUG ] Dependencies Resolved
[2021-12-10 23:18:34,744][client][DEBUG ] 
[2021-12-10 23:18:34,745][client][DEBUG ] ================================================================================
[2021-12-10 23:18:34,745][client][DEBUG ]  Package          Arch       Version     Repository                        Size
[2021-12-10 23:18:34,745][client][DEBUG ] ================================================================================
[2021-12-10 23:18:34,745][client][DEBUG ] Installing:
[2021-12-10 23:18:34,745][client][DEBUG ]  ceph-release     noarch     1-1.el7     /ceph-release-1-0.el7.noarch     535  
[2021-12-10 23:18:34,745][client][DEBUG ] 
[2021-12-10 23:18:34,745][client][DEBUG ] Transaction Summary
[2021-12-10 23:18:34,745][client][DEBUG ] ================================================================================
[2021-12-10 23:18:34,745][client][DEBUG ] Install  1 Package
[2021-12-10 23:18:34,745][client][DEBUG ] 
[2021-12-10 23:18:34,745][client][DEBUG ] Total size: 535  
[2021-12-10 23:18:34,745][client][DEBUG ] Installed size: 535  
[2021-12-10 23:18:34,745][client][DEBUG ] Downloading packages:
[2021-12-10 23:18:34,745][client][DEBUG ] Running transaction check
[2021-12-10 23:18:34,745][client][DEBUG ] Running transaction test
[2021-12-10 23:18:34,745][client][DEBUG ] Transaction test succeeded
[2021-12-10 23:18:34,745][client][DEBUG ] Running transaction
[2021-12-10 23:18:35,561][client][DEBUG ]   Installing : ceph-release-1-1.el7.noarch                                  1/1 
[2021-12-10 23:18:35,976][client][DEBUG ]   Verifying  : ceph-release-1-1.el7.noarch                                  1/1 
[2021-12-10 23:18:35,976][client][DEBUG ] 
[2021-12-10 23:18:35,976][client][DEBUG ] Installed:
[2021-12-10 23:18:35,977][client][DEBUG ]   ceph-release.noarch 0:1-1.el7                                                 
[2021-12-10 23:18:35,977][client][DEBUG ] 
[2021-12-10 23:18:35,977][client][DEBUG ] Complete!
[2021-12-10 23:18:35,992][client][WARNING] ensuring that /etc/yum.repos.d/ceph.repo contains a high priority
[2021-12-10 23:18:35,995][client][WARNING] altered ceph.repo priorities to contain: priority=1
[2021-12-10 23:18:35,997][client][INFO  ] Running command: sudo yum -y install ceph ceph-radosgw
[2021-12-10 23:18:36,113][client][DEBUG ] Loaded plugins: fastestmirror, priorities
[2021-12-10 23:18:36,177][client][DEBUG ] Loading mirror speeds from cached hostfile
[2021-12-10 23:18:36,177][client][DEBUG ]  * base: centos.mirrors.tds.net
[2021-12-10 23:18:36,177][client][DEBUG ]  * epel: pubmirror1.math.uh.edu
[2021-12-10 23:18:36,177][client][DEBUG ]  * extras: repo.ialab.dsu.edu
[2021-12-10 23:18:36,177][client][DEBUG ]  * updates: mirror.compevo.com
[2021-12-10 23:18:42,608][client][DEBUG ] 12 packages excluded due to repository priority protections
[2021-12-10 23:18:43,124][client][DEBUG ] Package 2:ceph-10.2.11-0.el7.x86_64 already installed and latest version
[2021-12-10 23:18:43,438][client][DEBUG ] Package 2:ceph-radosgw-10.2.11-0.el7.x86_64 already installed and latest version
[2021-12-10 23:18:43,439][client][DEBUG ] Nothing to do
[2021-12-10 23:18:43,504][client][INFO  ] Running command: sudo ceph --version
[2021-12-10 23:18:43,570][client][DEBUG ] ceph version 10.2.11 (e4b061b47f07f583c92a050d9e84b1813a35671e)
[2021-12-10 23:19:15,832][ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephuser/.cephdeploy.conf
[2021-12-10 23:19:15,832][ceph_deploy.cli][INFO  ] Invoked (1.5.39): /usr/bin/ceph-deploy admin client
[2021-12-10 23:19:15,832][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2021-12-10 23:19:15,832][ceph_deploy.cli][INFO  ]  username                      : None
[2021-12-10 23:19:15,832][ceph_deploy.cli][INFO  ]  verbose                       : False
[2021-12-10 23:19:15,832][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2021-12-10 23:19:15,832][ceph_deploy.cli][INFO  ]  quiet                         : False
[2021-12-10 23:19:15,833][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f4109ac7b48>
[2021-12-10 23:19:15,833][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2021-12-10 23:19:15,833][ceph_deploy.cli][INFO  ]  client                        : ['client']
[2021-12-10 23:19:15,833][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7f410a7e0a28>
[2021-12-10 23:19:15,833][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2021-12-10 23:19:15,833][ceph_deploy.cli][INFO  ]  default_release               : False
[2021-12-10 23:19:15,833][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to client
[2021-12-10 23:19:15,998][client][DEBUG ] connection detected need for sudo
[2021-12-10 23:19:16,161][client][DEBUG ] connected to host: client 
[2021-12-10 23:19:16,162][client][DEBUG ] detect platform information from remote host
[2021-12-10 23:19:16,179][client][DEBUG ] detect machine type
[2021-12-10 23:19:16,183][client][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2021-12-10 23:23:05,556][ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephuser/.cephdeploy.conf
[2021-12-10 23:23:05,556][ceph_deploy.cli][INFO  ] Invoked (1.5.39): /usr/bin/ceph-deploy install client
[2021-12-10 23:23:05,556][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2021-12-10 23:23:05,556][ceph_deploy.cli][INFO  ]  verbose                       : False
[2021-12-10 23:23:05,556][ceph_deploy.cli][INFO  ]  testing                       : None
[2021-12-10 23:23:05,556][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f71c6f91200>
[2021-12-10 23:23:05,557][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2021-12-10 23:23:05,557][ceph_deploy.cli][INFO  ]  dev_commit                    : None
[2021-12-10 23:23:05,557][ceph_deploy.cli][INFO  ]  install_mds                   : False
[2021-12-10 23:23:05,557][ceph_deploy.cli][INFO  ]  stable                        : None
[2021-12-10 23:23:05,557][ceph_deploy.cli][INFO  ]  default_release               : False
[2021-12-10 23:23:05,557][ceph_deploy.cli][INFO  ]  username                      : None
[2021-12-10 23:23:05,557][ceph_deploy.cli][INFO  ]  adjust_repos                  : True
[2021-12-10 23:23:05,557][ceph_deploy.cli][INFO  ]  func                          : <function install at 0x7f71c7c55de8>
[2021-12-10 23:23:05,557][ceph_deploy.cli][INFO  ]  install_mgr                   : False
[2021-12-10 23:23:05,557][ceph_deploy.cli][INFO  ]  install_all                   : False
[2021-12-10 23:23:05,557][ceph_deploy.cli][INFO  ]  repo                          : False
[2021-12-10 23:23:05,557][ceph_deploy.cli][INFO  ]  host                          : ['client']
[2021-12-10 23:23:05,557][ceph_deploy.cli][INFO  ]  install_rgw                   : False
[2021-12-10 23:23:05,557][ceph_deploy.cli][INFO  ]  install_tests                 : False
[2021-12-10 23:23:05,557][ceph_deploy.cli][INFO  ]  repo_url                      : None
[2021-12-10 23:23:05,557][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2021-12-10 23:23:05,557][ceph_deploy.cli][INFO  ]  install_osd                   : False
[2021-12-10 23:23:05,557][ceph_deploy.cli][INFO  ]  version_kind                  : stable
[2021-12-10 23:23:05,557][ceph_deploy.cli][INFO  ]  install_common                : False
[2021-12-10 23:23:05,557][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2021-12-10 23:23:05,557][ceph_deploy.cli][INFO  ]  quiet                         : False
[2021-12-10 23:23:05,558][ceph_deploy.cli][INFO  ]  dev                           : master
[2021-12-10 23:23:05,558][ceph_deploy.cli][INFO  ]  nogpgcheck                    : False
[2021-12-10 23:23:05,558][ceph_deploy.cli][INFO  ]  local_mirror                  : None
[2021-12-10 23:23:05,558][ceph_deploy.cli][INFO  ]  release                       : None
[2021-12-10 23:23:05,558][ceph_deploy.cli][INFO  ]  install_mon                   : False
[2021-12-10 23:23:05,558][ceph_deploy.cli][INFO  ]  gpg_url                       : None
[2021-12-10 23:23:05,558][ceph_deploy.install][DEBUG ] Installing stable version jewel on cluster ceph hosts client
[2021-12-10 23:23:05,558][ceph_deploy.install][DEBUG ] Detecting platform for host client ...
[2021-12-10 23:23:05,723][client][DEBUG ] connection detected need for sudo
[2021-12-10 23:23:05,886][client][DEBUG ] connected to host: client 
[2021-12-10 23:23:05,886][client][DEBUG ] detect platform information from remote host
[2021-12-10 23:23:05,904][client][DEBUG ] detect machine type
[2021-12-10 23:23:05,909][ceph_deploy.install][INFO  ] Distro info: CentOS Linux 7.9.2009 Core
[2021-12-10 23:23:05,909][client][INFO  ] installing Ceph on client
[2021-12-10 23:23:05,911][client][INFO  ] Running command: sudo yum clean all
[2021-12-10 23:23:06,029][client][DEBUG ] Loaded plugins: fastestmirror, priorities
[2021-12-10 23:23:06,093][client][DEBUG ] Cleaning repos: Ceph Ceph-noarch base ceph-source epel extras updates
[2021-12-10 23:23:06,094][client][DEBUG ] Cleaning up list of fastest mirrors
[2021-12-10 23:23:06,103][client][INFO  ] Running command: sudo yum -y install epel-release
[2021-12-10 23:23:06,219][client][DEBUG ] Loaded plugins: fastestmirror, priorities
[2021-12-10 23:23:06,283][client][DEBUG ] Determining fastest mirrors
[2021-12-10 23:23:38,696][client][DEBUG ]  * base: centos.mirrors.tds.net
[2021-12-10 23:23:38,696][client][DEBUG ]  * epel: pubmirror1.math.uh.edu
[2021-12-10 23:23:38,696][client][DEBUG ]  * extras: repo.ialab.dsu.edu
[2021-12-10 23:23:38,696][client][DEBUG ]  * updates: mirror.compevo.com
[2021-12-10 23:24:30,072][client][DEBUG ] 12 packages excluded due to repository priority protections
[2021-12-10 23:24:30,587][client][DEBUG ] Package epel-release-7-14.noarch already installed and latest version
[2021-12-10 23:24:30,587][client][DEBUG ] Nothing to do
[2021-12-10 23:24:30,653][client][INFO  ] Running command: sudo yum -y install yum-plugin-priorities
[2021-12-10 23:24:30,769][client][DEBUG ] Loaded plugins: fastestmirror, priorities
[2021-12-10 23:24:30,834][client][DEBUG ] Loading mirror speeds from cached hostfile
[2021-12-10 23:24:30,834][client][DEBUG ]  * base: centos.mirrors.tds.net
[2021-12-10 23:24:30,834][client][DEBUG ]  * epel: pubmirror1.math.uh.edu
[2021-12-10 23:24:30,834][client][DEBUG ]  * extras: repo.ialab.dsu.edu
[2021-12-10 23:24:30,834][client][DEBUG ]  * updates: mirror.compevo.com
[2021-12-10 23:24:31,600][client][DEBUG ] 12 packages excluded due to repository priority protections
[2021-12-10 23:24:32,065][client][DEBUG ] Package yum-plugin-priorities-1.1.31-54.el7_8.noarch already installed and latest version
[2021-12-10 23:24:32,065][client][DEBUG ] Nothing to do
[2021-12-10 23:24:32,180][client][DEBUG ] Configure Yum priorities to include obsoletes
[2021-12-10 23:24:32,182][client][WARNING] check_obsoletes has been enabled for Yum priorities plugin
[2021-12-10 23:24:32,184][client][INFO  ] Running command: sudo rpm --import https://download.ceph.com/keys/release.asc
[2021-12-10 23:24:38,019][client][INFO  ] Running command: sudo yum remove -y ceph-release
[2021-12-10 23:24:38,135][client][DEBUG ] Loaded plugins: fastestmirror, priorities
[2021-12-10 23:24:38,250][client][DEBUG ] Resolving Dependencies
[2021-12-10 23:24:38,250][client][DEBUG ] --> Running transaction check
[2021-12-10 23:24:38,250][client][DEBUG ] ---> Package ceph-release.noarch 0:1-1.el7 will be erased
[2021-12-10 23:24:38,615][client][DEBUG ] --> Finished Dependency Resolution
[2021-12-10 23:24:38,647][client][DEBUG ] 
[2021-12-10 23:24:38,647][client][DEBUG ] Dependencies Resolved
[2021-12-10 23:24:38,647][client][DEBUG ] 
[2021-12-10 23:24:38,647][client][DEBUG ] ================================================================================
[2021-12-10 23:24:38,647][client][DEBUG ]  Package         Arch      Version       Repository                        Size
[2021-12-10 23:24:38,647][client][DEBUG ] ================================================================================
[2021-12-10 23:24:38,647][client][DEBUG ] Removing:
[2021-12-10 23:24:38,647][client][DEBUG ]  ceph-release    noarch    1-1.el7       @/ceph-release-1-0.el7.noarch    535  
[2021-12-10 23:24:38,647][client][DEBUG ] 
[2021-12-10 23:24:38,647][client][DEBUG ] Transaction Summary
[2021-12-10 23:24:38,648][client][DEBUG ] ================================================================================
[2021-12-10 23:24:38,648][client][DEBUG ] Remove  1 Package
[2021-12-10 23:24:38,648][client][DEBUG ] 
[2021-12-10 23:24:38,648][client][DEBUG ] Installed size: 535  
[2021-12-10 23:24:38,648][client][DEBUG ] Downloading packages:
[2021-12-10 23:24:38,648][client][DEBUG ] Running transaction check
[2021-12-10 23:24:38,648][client][DEBUG ] Running transaction test
[2021-12-10 23:24:38,648][client][DEBUG ] Transaction test succeeded
[2021-12-10 23:24:38,648][client][DEBUG ] Running transaction
[2021-12-10 23:24:38,913][client][DEBUG ]   Erasing    : ceph-release-1-1.el7.noarch                                  1/1 
[2021-12-10 23:24:38,913][client][DEBUG ] warning: /etc/yum.repos.d/ceph.repo saved as /etc/yum.repos.d/ceph.repo.rpmsave
[2021-12-10 23:24:39,578][client][DEBUG ]   Verifying  : ceph-release-1-1.el7.noarch                                  1/1 
[2021-12-10 23:24:39,579][client][DEBUG ] 
[2021-12-10 23:24:39,579][client][DEBUG ] Removed:
[2021-12-10 23:24:39,579][client][DEBUG ]   ceph-release.noarch 0:1-1.el7                                                 
[2021-12-10 23:24:39,579][client][DEBUG ] 
[2021-12-10 23:24:39,579][client][DEBUG ] Complete!
[2021-12-10 23:24:39,581][client][INFO  ] Running command: sudo yum install -y https://download.ceph.com/rpm-jewel/el7/noarch/ceph-release-1-0.el7.noarch.rpm
[2021-12-10 23:24:39,697][client][DEBUG ] Loaded plugins: fastestmirror, priorities
[2021-12-10 23:24:45,528][client][DEBUG ] Examining /var/tmp/yum-root-3skFzx/ceph-release-1-0.el7.noarch.rpm: ceph-release-1-1.el7.noarch
[2021-12-10 23:24:45,528][client][DEBUG ] Marking /var/tmp/yum-root-3skFzx/ceph-release-1-0.el7.noarch.rpm to be installed
[2021-12-10 23:24:45,528][client][DEBUG ] Resolving Dependencies
[2021-12-10 23:24:45,528][client][DEBUG ] --> Running transaction check
[2021-12-10 23:24:45,528][client][DEBUG ] ---> Package ceph-release.noarch 0:1-1.el7 will be installed
[2021-12-10 23:24:45,693][client][DEBUG ] --> Finished Dependency Resolution
[2021-12-10 23:24:45,807][client][DEBUG ] 
[2021-12-10 23:24:45,807][client][DEBUG ] Dependencies Resolved
[2021-12-10 23:24:45,807][client][DEBUG ] 
[2021-12-10 23:24:45,807][client][DEBUG ] ================================================================================
[2021-12-10 23:24:45,807][client][DEBUG ]  Package          Arch       Version     Repository                        Size
[2021-12-10 23:24:45,807][client][DEBUG ] ================================================================================
[2021-12-10 23:24:45,807][client][DEBUG ] Installing:
[2021-12-10 23:24:45,807][client][DEBUG ]  ceph-release     noarch     1-1.el7     /ceph-release-1-0.el7.noarch     535  
[2021-12-10 23:24:45,807][client][DEBUG ] 
[2021-12-10 23:24:45,807][client][DEBUG ] Transaction Summary
[2021-12-10 23:24:45,807][client][DEBUG ] ================================================================================
[2021-12-10 23:24:45,807][client][DEBUG ] Install  1 Package
[2021-12-10 23:24:45,807][client][DEBUG ] 
[2021-12-10 23:24:45,808][client][DEBUG ] Total size: 535  
[2021-12-10 23:24:45,808][client][DEBUG ] Installed size: 535  
[2021-12-10 23:24:45,808][client][DEBUG ] Downloading packages:
[2021-12-10 23:24:45,808][client][DEBUG ] Running transaction check
[2021-12-10 23:24:45,808][client][DEBUG ] Running transaction test
[2021-12-10 23:24:45,808][client][DEBUG ] Transaction test succeeded
[2021-12-10 23:24:45,808][client][DEBUG ] Running transaction
[2021-12-10 23:24:46,323][client][DEBUG ]   Installing : ceph-release-1-1.el7.noarch                                  1/1 
[2021-12-10 23:24:46,688][client][DEBUG ]   Verifying  : ceph-release-1-1.el7.noarch                                  1/1 
[2021-12-10 23:24:46,688][client][DEBUG ] 
[2021-12-10 23:24:46,688][client][DEBUG ] Installed:
[2021-12-10 23:24:46,688][client][DEBUG ]   ceph-release.noarch 0:1-1.el7                                                 
[2021-12-10 23:24:46,688][client][DEBUG ] 
[2021-12-10 23:24:46,688][client][DEBUG ] Complete!
[2021-12-10 23:24:46,752][client][WARNING] ensuring that /etc/yum.repos.d/ceph.repo contains a high priority
[2021-12-10 23:24:46,755][client][WARNING] altered ceph.repo priorities to contain: priority=1
[2021-12-10 23:24:46,757][client][INFO  ] Running command: sudo yum -y install ceph ceph-radosgw
[2021-12-10 23:24:46,873][client][DEBUG ] Loaded plugins: fastestmirror, priorities
[2021-12-10 23:24:46,937][client][DEBUG ] Loading mirror speeds from cached hostfile
[2021-12-10 23:24:46,938][client][DEBUG ]  * base: centos.mirrors.tds.net
[2021-12-10 23:24:46,938][client][DEBUG ]  * epel: pubmirror1.math.uh.edu
[2021-12-10 23:24:46,938][client][DEBUG ]  * extras: repo.ialab.dsu.edu
[2021-12-10 23:24:46,938][client][DEBUG ]  * updates: mirror.compevo.com
[2021-12-10 23:24:53,372][client][DEBUG ] 12 packages excluded due to repository priority protections
[2021-12-10 23:24:53,837][client][DEBUG ] Package 2:ceph-10.2.11-0.el7.x86_64 already installed and latest version
[2021-12-10 23:24:54,152][client][DEBUG ] Package 2:ceph-radosgw-10.2.11-0.el7.x86_64 already installed and latest version
[2021-12-10 23:24:54,152][client][DEBUG ] Nothing to do
[2021-12-10 23:24:54,268][client][INFO  ] Running command: sudo ceph --version
[2021-12-10 23:24:54,334][client][DEBUG ] ceph version 10.2.11 (e4b061b47f07f583c92a050d9e84b1813a35671e)
[2021-12-10 23:25:50,226][ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephuser/.cephdeploy.conf
[2021-12-10 23:25:50,226][ceph_deploy.cli][INFO  ] Invoked (1.5.39): /usr/bin/ceph-deploy admin client
[2021-12-10 23:25:50,226][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2021-12-10 23:25:50,226][ceph_deploy.cli][INFO  ]  username                      : None
[2021-12-10 23:25:50,226][ceph_deploy.cli][INFO  ]  verbose                       : False
[2021-12-10 23:25:50,226][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2021-12-10 23:25:50,226][ceph_deploy.cli][INFO  ]  quiet                         : False
[2021-12-10 23:25:50,226][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f9607f20b48>
[2021-12-10 23:25:50,226][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2021-12-10 23:25:50,227][ceph_deploy.cli][INFO  ]  client                        : ['client']
[2021-12-10 23:25:50,227][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7f9608c39a28>
[2021-12-10 23:25:50,227][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2021-12-10 23:25:50,227][ceph_deploy.cli][INFO  ]  default_release               : False
[2021-12-10 23:25:50,227][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to client
[2021-12-10 23:25:50,392][client][DEBUG ] connection detected need for sudo
[2021-12-10 23:25:50,556][client][DEBUG ] connected to host: client 
[2021-12-10 23:25:50,557][client][DEBUG ] detect platform information from remote host
[2021-12-10 23:25:50,574][client][DEBUG ] detect machine type
[2021-12-10 23:25:50,579][client][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2021-12-10 23:25:50,596][ceph_deploy.admin][ERROR ] OSError: [Errno 2] No such file or directory: '/etc/ceph/tmpAyxsNR'
[2021-12-10 23:25:50,596][ceph_deploy][ERROR ] GenericError: Failed to configure 1 admin hosts

[2021-12-10 23:26:20,205][ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephuser/.cephdeploy.conf
[2021-12-10 23:26:20,206][ceph_deploy.cli][INFO  ] Invoked (1.5.39): /usr/bin/ceph-deploy admin client
[2021-12-10 23:26:20,206][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2021-12-10 23:26:20,206][ceph_deploy.cli][INFO  ]  username                      : None
[2021-12-10 23:26:20,206][ceph_deploy.cli][INFO  ]  verbose                       : False
[2021-12-10 23:26:20,206][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2021-12-10 23:26:20,206][ceph_deploy.cli][INFO  ]  quiet                         : False
[2021-12-10 23:26:20,206][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f83856b5b48>
[2021-12-10 23:26:20,207][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2021-12-10 23:26:20,207][ceph_deploy.cli][INFO  ]  client                        : ['client']
[2021-12-10 23:26:20,207][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7f83863cea28>
[2021-12-10 23:26:20,207][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2021-12-10 23:26:20,207][ceph_deploy.cli][INFO  ]  default_release               : False
[2021-12-10 23:26:20,207][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to client
[2021-12-10 23:26:20,373][client][DEBUG ] connection detected need for sudo
[2021-12-10 23:26:20,539][client][DEBUG ] connected to host: client 
[2021-12-10 23:26:20,539][client][DEBUG ] detect platform information from remote host
[2021-12-10 23:26:20,557][client][DEBUG ] detect machine type
[2021-12-10 23:26:20,562][client][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2021-12-10 23:47:23,132][ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephuser/.cephdeploy.conf
[2021-12-10 23:47:23,132][ceph_deploy.cli][INFO  ] Invoked (1.5.39): /usr/bin/ceph-deploy osd prepare osd1:/dev/sdb osd2:/dev/sdb osd3:/dev/sdb osd4:/dev/sdb osd5:/dev/sdb
[2021-12-10 23:47:23,132][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2021-12-10 23:47:23,132][ceph_deploy.cli][INFO  ]  username                      : None
[2021-12-10 23:47:23,132][ceph_deploy.cli][INFO  ]  block_db                      : None
[2021-12-10 23:47:23,132][ceph_deploy.cli][INFO  ]  disk                          : [('osd1', '/dev/sdb', None), ('osd2', '/dev/sdb', None), ('osd3', '/dev/sdb', None), ('osd4', '/dev/sdb', None), ('osd5', '/dev/sdb', None)]
[2021-12-10 23:47:23,132][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2021-12-10 23:47:23,132][ceph_deploy.cli][INFO  ]  verbose                       : False
[2021-12-10 23:47:23,132][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2021-12-10 23:47:23,133][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2021-12-10 23:47:23,133][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2021-12-10 23:47:23,133][ceph_deploy.cli][INFO  ]  subcommand                    : prepare
[2021-12-10 23:47:23,133][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2021-12-10 23:47:23,133][ceph_deploy.cli][INFO  ]  quiet                         : False
[2021-12-10 23:47:23,133][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fb1ecb2e170>
[2021-12-10 23:47:23,133][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2021-12-10 23:47:23,133][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2021-12-10 23:47:23,133][ceph_deploy.cli][INFO  ]  filestore                     : None
[2021-12-10 23:47:23,133][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fb1ecb7b050>
[2021-12-10 23:47:23,133][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2021-12-10 23:47:23,133][ceph_deploy.cli][INFO  ]  default_release               : False
[2021-12-10 23:47:23,133][ceph_deploy.cli][INFO  ]  zap_disk                      : False
[2021-12-10 23:47:23,133][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks osd1:/dev/sdb: osd2:/dev/sdb: osd3:/dev/sdb: osd4:/dev/sdb: osd5:/dev/sdb:
[2021-12-10 23:47:23,377][osd1][DEBUG ] connection detected need for sudo
[2021-12-10 23:47:23,548][osd1][DEBUG ] connected to host: osd1 
[2021-12-10 23:47:23,549][osd1][DEBUG ] detect platform information from remote host
[2021-12-10 23:47:23,567][osd1][DEBUG ] detect machine type
[2021-12-10 23:47:23,572][osd1][DEBUG ] find the location of an executable
[2021-12-10 23:47:23,574][ceph_deploy.osd][INFO  ] Distro info: CentOS Linux 7.9.2009 Core
[2021-12-10 23:47:23,574][ceph_deploy.osd][DEBUG ] Deploying osd to osd1
[2021-12-10 23:47:23,575][osd1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2021-12-10 23:47:23,577][ceph_deploy.osd][ERROR ] RuntimeError: config file /etc/ceph/ceph.conf exists with different content; use --overwrite-conf to overwrite
[2021-12-10 23:47:23,734][osd2][DEBUG ] connection detected need for sudo
[2021-12-10 23:47:23,908][osd2][DEBUG ] connected to host: osd2 
[2021-12-10 23:47:23,909][osd2][DEBUG ] detect platform information from remote host
[2021-12-10 23:47:23,927][osd2][DEBUG ] detect machine type
[2021-12-10 23:47:23,932][osd2][DEBUG ] find the location of an executable
[2021-12-10 23:47:23,933][ceph_deploy.osd][INFO  ] Distro info: CentOS Linux 7.9.2009 Core
[2021-12-10 23:47:23,934][ceph_deploy.osd][DEBUG ] Deploying osd to osd2
[2021-12-10 23:47:23,934][osd2][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2021-12-10 23:47:23,935][ceph_deploy.osd][ERROR ] RuntimeError: config file /etc/ceph/ceph.conf exists with different content; use --overwrite-conf to overwrite
[2021-12-10 23:47:24,091][osd3][DEBUG ] connection detected need for sudo
[2021-12-10 23:47:24,261][osd3][DEBUG ] connected to host: osd3 
[2021-12-10 23:47:24,262][osd3][DEBUG ] detect platform information from remote host
[2021-12-10 23:47:24,278][osd3][DEBUG ] detect machine type
[2021-12-10 23:47:24,283][osd3][DEBUG ] find the location of an executable
[2021-12-10 23:47:24,284][ceph_deploy.osd][INFO  ] Distro info: CentOS Linux 7.9.2009 Core
[2021-12-10 23:47:24,284][ceph_deploy.osd][DEBUG ] Deploying osd to osd3
[2021-12-10 23:47:24,285][osd3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2021-12-10 23:47:24,286][ceph_deploy.osd][ERROR ] RuntimeError: config file /etc/ceph/ceph.conf exists with different content; use --overwrite-conf to overwrite
[2021-12-10 23:47:24,443][osd4][DEBUG ] connection detected need for sudo
[2021-12-10 23:47:24,613][osd4][DEBUG ] connected to host: osd4 
[2021-12-10 23:47:24,614][osd4][DEBUG ] detect platform information from remote host
[2021-12-10 23:47:24,631][osd4][DEBUG ] detect machine type
[2021-12-10 23:47:24,635][osd4][DEBUG ] find the location of an executable
[2021-12-10 23:47:24,637][ceph_deploy.osd][INFO  ] Distro info: CentOS Linux 7.9.2009 Core
[2021-12-10 23:47:24,637][ceph_deploy.osd][DEBUG ] Deploying osd to osd4
[2021-12-10 23:47:24,637][osd4][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2021-12-10 23:47:24,639][ceph_deploy.osd][ERROR ] RuntimeError: config file /etc/ceph/ceph.conf exists with different content; use --overwrite-conf to overwrite
[2021-12-10 23:47:24,797][osd5][DEBUG ] connection detected need for sudo
[2021-12-10 23:47:24,972][osd5][DEBUG ] connected to host: osd5 
[2021-12-10 23:47:24,973][osd5][DEBUG ] detect platform information from remote host
[2021-12-10 23:47:24,991][osd5][DEBUG ] detect machine type
[2021-12-10 23:47:24,996][osd5][DEBUG ] find the location of an executable
[2021-12-10 23:47:24,997][ceph_deploy.osd][INFO  ] Distro info: CentOS Linux 7.9.2009 Core
[2021-12-10 23:47:24,997][ceph_deploy.osd][DEBUG ] Deploying osd to osd5
[2021-12-10 23:47:24,997][osd5][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2021-12-10 23:47:24,999][ceph_deploy.osd][ERROR ] RuntimeError: config file /etc/ceph/ceph.conf exists with different content; use --overwrite-conf to overwrite
[2021-12-10 23:47:24,999][ceph_deploy][ERROR ] GenericError: Failed to create 5 OSDs

[2021-12-10 23:47:33,591][ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephuser/.cephdeploy.conf
[2021-12-10 23:47:33,591][ceph_deploy.cli][INFO  ] Invoked (1.5.39): /usr/bin/ceph-deploy --overwrite-conf osd prepare osd1:/dev/sdb osd2:/dev/sdb osd3:/dev/sdb osd4:/dev/sdb osd5:/dev/sdb
[2021-12-10 23:47:33,591][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2021-12-10 23:47:33,591][ceph_deploy.cli][INFO  ]  username                      : None
[2021-12-10 23:47:33,591][ceph_deploy.cli][INFO  ]  block_db                      : None
[2021-12-10 23:47:33,592][ceph_deploy.cli][INFO  ]  disk                          : [('osd1', '/dev/sdb', None), ('osd2', '/dev/sdb', None), ('osd3', '/dev/sdb', None), ('osd4', '/dev/sdb', None), ('osd5', '/dev/sdb', None)]
[2021-12-10 23:47:33,592][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2021-12-10 23:47:33,592][ceph_deploy.cli][INFO  ]  verbose                       : False
[2021-12-10 23:47:33,592][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2021-12-10 23:47:33,592][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2021-12-10 23:47:33,592][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2021-12-10 23:47:33,592][ceph_deploy.cli][INFO  ]  subcommand                    : prepare
[2021-12-10 23:47:33,592][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2021-12-10 23:47:33,592][ceph_deploy.cli][INFO  ]  quiet                         : False
[2021-12-10 23:47:33,592][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fe2cbfc8170>
[2021-12-10 23:47:33,592][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2021-12-10 23:47:33,592][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2021-12-10 23:47:33,592][ceph_deploy.cli][INFO  ]  filestore                     : None
[2021-12-10 23:47:33,592][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fe2cc015050>
[2021-12-10 23:47:33,592][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2021-12-10 23:47:33,592][ceph_deploy.cli][INFO  ]  default_release               : False
[2021-12-10 23:47:33,592][ceph_deploy.cli][INFO  ]  zap_disk                      : False
[2021-12-10 23:47:33,593][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks osd1:/dev/sdb: osd2:/dev/sdb: osd3:/dev/sdb: osd4:/dev/sdb: osd5:/dev/sdb:
[2021-12-10 23:47:33,767][osd1][DEBUG ] connection detected need for sudo
[2021-12-10 23:47:33,937][osd1][DEBUG ] connected to host: osd1 
[2021-12-10 23:47:33,938][osd1][DEBUG ] detect platform information from remote host
[2021-12-10 23:47:33,956][osd1][DEBUG ] detect machine type
[2021-12-10 23:47:33,961][osd1][DEBUG ] find the location of an executable
[2021-12-10 23:47:33,963][ceph_deploy.osd][INFO  ] Distro info: CentOS Linux 7.9.2009 Core
[2021-12-10 23:47:33,963][ceph_deploy.osd][DEBUG ] Deploying osd to osd1
[2021-12-10 23:47:33,963][osd1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2021-12-10 23:47:33,965][ceph_deploy.osd][DEBUG ] Preparing host osd1 disk /dev/sdb journal None activate False
[2021-12-10 23:47:33,965][osd1][DEBUG ] find the location of an executable
[2021-12-10 23:47:33,968][osd1][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdb
[2021-12-10 23:47:34,087][osd1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2021-12-10 23:47:34,087][osd1][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --log-file $run_dir/$cluster-osd-check.log --cluster ceph --setuser ceph --setgroup ceph
[2021-12-10 23:47:34,088][osd1][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --log-file $run_dir/$cluster-osd-check.log --cluster ceph --setuser ceph --setgroup ceph
[2021-12-10 23:47:34,089][osd1][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --log-file $run_dir/$cluster-osd-check.log --cluster ceph --setuser ceph --setgroup ceph
[2021-12-10 23:47:34,121][osd1][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-10 23:47:34,121][osd1][WARNING] set_type: Will colocate journal with data on /dev/sdb
[2021-12-10 23:47:34,121][osd1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2021-12-10 23:47:34,122][osd1][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-10 23:47:34,123][osd1][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-10 23:47:34,123][osd1][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-10 23:47:34,123][osd1][WARNING] Traceback (most recent call last):
[2021-12-10 23:47:34,123][osd1][WARNING]   File "/usr/sbin/ceph-disk", line 9, in <module>
[2021-12-10 23:47:34,123][osd1][WARNING]     load_entry_point('ceph-disk==1.0.0', 'console_scripts', 'ceph-disk')()
[2021-12-10 23:47:34,123][osd1][WARNING]   File "/usr/lib/python2.7/site-packages/ceph_disk/main.py", line 5361, in run
[2021-12-10 23:47:34,155][osd1][WARNING]     main(sys.argv[1:])
[2021-12-10 23:47:34,155][osd1][WARNING]   File "/usr/lib/python2.7/site-packages/ceph_disk/main.py", line 5312, in main
[2021-12-10 23:47:34,155][osd1][WARNING]     args.func(args)
[2021-12-10 23:47:34,155][osd1][WARNING]   File "/usr/lib/python2.7/site-packages/ceph_disk/main.py", line 1890, in main
[2021-12-10 23:47:34,155][osd1][WARNING]     Prepare.factory(args).prepare()
[2021-12-10 23:47:34,155][osd1][WARNING]   File "/usr/lib/python2.7/site-packages/ceph_disk/main.py", line 1879, in prepare
[2021-12-10 23:47:34,155][osd1][WARNING]     self.prepare_locked()
[2021-12-10 23:47:34,155][osd1][WARNING]   File "/usr/lib/python2.7/site-packages/ceph_disk/main.py", line 1910, in prepare_locked
[2021-12-10 23:47:34,155][osd1][WARNING]     self.data.prepare(self.journal)
[2021-12-10 23:47:34,155][osd1][WARNING]   File "/usr/lib/python2.7/site-packages/ceph_disk/main.py", line 2578, in prepare
[2021-12-10 23:47:34,155][osd1][WARNING]     self.prepare_device(*to_prepare_list)
[2021-12-10 23:47:34,156][osd1][WARNING]   File "/usr/lib/python2.7/site-packages/ceph_disk/main.py", line 2738, in prepare_device
[2021-12-10 23:47:34,156][osd1][WARNING]     super(PrepareFilestoreData, self).prepare_device(*to_prepare_list)
[2021-12-10 23:47:34,156][osd1][WARNING]   File "/usr/lib/python2.7/site-packages/ceph_disk/main.py", line 2640, in prepare_device
[2021-12-10 23:47:34,156][osd1][WARNING]     self.sanity_checks()
[2021-12-10 23:47:34,156][osd1][WARNING]   File "/usr/lib/python2.7/site-packages/ceph_disk/main.py", line 2603, in sanity_checks
[2021-12-10 23:47:34,156][osd1][WARNING]     check_partitions=not self.args.dmcrypt)
[2021-12-10 23:47:34,156][osd1][WARNING]   File "/usr/lib/python2.7/site-packages/ceph_disk/main.py", line 887, in verify_not_in_use
[2021-12-10 23:47:34,156][osd1][WARNING]     raise Error('Device is mounted', partition)
[2021-12-10 23:47:34,156][osd1][WARNING] ceph_disk.main.Error: Error: Device is mounted: /dev/sdb1
[2021-12-10 23:47:34,157][osd1][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2021-12-10 23:47:34,157][ceph_deploy.osd][ERROR ] Failed to execute command: /usr/sbin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdb
[2021-12-10 23:47:34,314][osd2][DEBUG ] connection detected need for sudo
[2021-12-10 23:47:34,486][osd2][DEBUG ] connected to host: osd2 
[2021-12-10 23:47:34,486][osd2][DEBUG ] detect platform information from remote host
[2021-12-10 23:47:34,505][osd2][DEBUG ] detect machine type
[2021-12-10 23:47:34,510][osd2][DEBUG ] find the location of an executable
[2021-12-10 23:47:34,511][ceph_deploy.osd][INFO  ] Distro info: CentOS Linux 7.9.2009 Core
[2021-12-10 23:47:34,512][ceph_deploy.osd][DEBUG ] Deploying osd to osd2
[2021-12-10 23:47:34,512][osd2][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2021-12-10 23:47:34,514][ceph_deploy.osd][DEBUG ] Preparing host osd2 disk /dev/sdb journal None activate False
[2021-12-10 23:47:34,514][osd2][DEBUG ] find the location of an executable
[2021-12-10 23:47:34,517][osd2][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdb
[2021-12-10 23:47:34,634][osd2][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2021-12-10 23:47:34,634][osd2][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --log-file $run_dir/$cluster-osd-check.log --cluster ceph --setuser ceph --setgroup ceph
[2021-12-10 23:47:34,635][osd2][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --log-file $run_dir/$cluster-osd-check.log --cluster ceph --setuser ceph --setgroup ceph
[2021-12-10 23:47:34,635][osd2][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --log-file $run_dir/$cluster-osd-check.log --cluster ceph --setuser ceph --setgroup ceph
[2021-12-10 23:47:34,650][osd2][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-10 23:47:34,650][osd2][WARNING] set_type: Will colocate journal with data on /dev/sdb
[2021-12-10 23:47:34,651][osd2][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2021-12-10 23:47:34,666][osd2][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-10 23:47:34,666][osd2][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-10 23:47:34,667][osd2][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-10 23:47:34,667][osd2][WARNING] Traceback (most recent call last):
[2021-12-10 23:47:34,667][osd2][WARNING]   File "/usr/sbin/ceph-disk", line 9, in <module>
[2021-12-10 23:47:34,667][osd2][WARNING]     load_entry_point('ceph-disk==1.0.0', 'console_scripts', 'ceph-disk')()
[2021-12-10 23:47:34,667][osd2][WARNING]   File "/usr/lib/python2.7/site-packages/ceph_disk/main.py", line 5361, in run
[2021-12-10 23:47:34,699][osd2][WARNING]     main(sys.argv[1:])
[2021-12-10 23:47:34,699][osd2][WARNING]   File "/usr/lib/python2.7/site-packages/ceph_disk/main.py", line 5312, in main
[2021-12-10 23:47:34,699][osd2][WARNING]     args.func(args)
[2021-12-10 23:47:34,699][osd2][WARNING]   File "/usr/lib/python2.7/site-packages/ceph_disk/main.py", line 1890, in main
[2021-12-10 23:47:34,699][osd2][WARNING]     Prepare.factory(args).prepare()
[2021-12-10 23:47:34,699][osd2][WARNING]   File "/usr/lib/python2.7/site-packages/ceph_disk/main.py", line 1879, in prepare
[2021-12-10 23:47:34,699][osd2][WARNING]     self.prepare_locked()
[2021-12-10 23:47:34,699][osd2][WARNING]   File "/usr/lib/python2.7/site-packages/ceph_disk/main.py", line 1910, in prepare_locked
[2021-12-10 23:47:34,699][osd2][WARNING]     self.data.prepare(self.journal)
[2021-12-10 23:47:34,699][osd2][WARNING]   File "/usr/lib/python2.7/site-packages/ceph_disk/main.py", line 2578, in prepare
[2021-12-10 23:47:34,699][osd2][WARNING]     self.prepare_device(*to_prepare_list)
[2021-12-10 23:47:34,699][osd2][WARNING]   File "/usr/lib/python2.7/site-packages/ceph_disk/main.py", line 2738, in prepare_device
[2021-12-10 23:47:34,700][osd2][WARNING]     super(PrepareFilestoreData, self).prepare_device(*to_prepare_list)
[2021-12-10 23:47:34,700][osd2][WARNING]   File "/usr/lib/python2.7/site-packages/ceph_disk/main.py", line 2640, in prepare_device
[2021-12-10 23:47:34,700][osd2][WARNING]     self.sanity_checks()
[2021-12-10 23:47:34,700][osd2][WARNING]   File "/usr/lib/python2.7/site-packages/ceph_disk/main.py", line 2603, in sanity_checks
[2021-12-10 23:47:34,700][osd2][WARNING]     check_partitions=not self.args.dmcrypt)
[2021-12-10 23:47:34,700][osd2][WARNING]   File "/usr/lib/python2.7/site-packages/ceph_disk/main.py", line 887, in verify_not_in_use
[2021-12-10 23:47:34,700][osd2][WARNING]     raise Error('Device is mounted', partition)
[2021-12-10 23:47:34,700][osd2][WARNING] ceph_disk.main.Error: Error: Device is mounted: /dev/sdb1
[2021-12-10 23:47:34,700][osd2][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2021-12-10 23:47:34,700][ceph_deploy.osd][ERROR ] Failed to execute command: /usr/sbin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdb
[2021-12-10 23:47:34,856][osd3][DEBUG ] connection detected need for sudo
[2021-12-10 23:47:35,026][osd3][DEBUG ] connected to host: osd3 
[2021-12-10 23:47:35,027][osd3][DEBUG ] detect platform information from remote host
[2021-12-10 23:47:35,044][osd3][DEBUG ] detect machine type
[2021-12-10 23:47:35,049][osd3][DEBUG ] find the location of an executable
[2021-12-10 23:47:35,050][ceph_deploy.osd][INFO  ] Distro info: CentOS Linux 7.9.2009 Core
[2021-12-10 23:47:35,050][ceph_deploy.osd][DEBUG ] Deploying osd to osd3
[2021-12-10 23:47:35,050][osd3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2021-12-10 23:47:35,053][ceph_deploy.osd][DEBUG ] Preparing host osd3 disk /dev/sdb journal None activate False
[2021-12-10 23:47:35,053][osd3][DEBUG ] find the location of an executable
[2021-12-10 23:47:35,055][osd3][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdb
[2021-12-10 23:47:35,122][osd3][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2021-12-10 23:47:35,138][osd3][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --log-file $run_dir/$cluster-osd-check.log --cluster ceph --setuser ceph --setgroup ceph
[2021-12-10 23:47:35,154][osd3][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --log-file $run_dir/$cluster-osd-check.log --cluster ceph --setuser ceph --setgroup ceph
[2021-12-10 23:47:35,169][osd3][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --log-file $run_dir/$cluster-osd-check.log --cluster ceph --setuser ceph --setgroup ceph
[2021-12-10 23:47:35,185][osd3][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-10 23:47:35,185][osd3][WARNING] set_type: Will colocate journal with data on /dev/sdb
[2021-12-10 23:47:35,185][osd3][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2021-12-10 23:47:35,201][osd3][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-10 23:47:35,201][osd3][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-10 23:47:35,201][osd3][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-10 23:47:35,201][osd3][WARNING] Traceback (most recent call last):
[2021-12-10 23:47:35,201][osd3][WARNING]   File "/usr/sbin/ceph-disk", line 9, in <module>
[2021-12-10 23:47:35,201][osd3][WARNING]     load_entry_point('ceph-disk==1.0.0', 'console_scripts', 'ceph-disk')()
[2021-12-10 23:47:35,201][osd3][WARNING]   File "/usr/lib/python2.7/site-packages/ceph_disk/main.py", line 5361, in run
[2021-12-10 23:47:35,233][osd3][WARNING]     main(sys.argv[1:])
[2021-12-10 23:47:35,233][osd3][WARNING]   File "/usr/lib/python2.7/site-packages/ceph_disk/main.py", line 5312, in main
[2021-12-10 23:47:35,233][osd3][WARNING]     args.func(args)
[2021-12-10 23:47:35,233][osd3][WARNING]   File "/usr/lib/python2.7/site-packages/ceph_disk/main.py", line 1890, in main
[2021-12-10 23:47:35,233][osd3][WARNING]     Prepare.factory(args).prepare()
[2021-12-10 23:47:35,233][osd3][WARNING]   File "/usr/lib/python2.7/site-packages/ceph_disk/main.py", line 1879, in prepare
[2021-12-10 23:47:35,233][osd3][WARNING]     self.prepare_locked()
[2021-12-10 23:47:35,234][osd3][WARNING]   File "/usr/lib/python2.7/site-packages/ceph_disk/main.py", line 1910, in prepare_locked
[2021-12-10 23:47:35,234][osd3][WARNING]     self.data.prepare(self.journal)
[2021-12-10 23:47:35,234][osd3][WARNING]   File "/usr/lib/python2.7/site-packages/ceph_disk/main.py", line 2578, in prepare
[2021-12-10 23:47:35,234][osd3][WARNING]     self.prepare_device(*to_prepare_list)
[2021-12-10 23:47:35,234][osd3][WARNING]   File "/usr/lib/python2.7/site-packages/ceph_disk/main.py", line 2738, in prepare_device
[2021-12-10 23:47:35,234][osd3][WARNING]     super(PrepareFilestoreData, self).prepare_device(*to_prepare_list)
[2021-12-10 23:47:35,234][osd3][WARNING]   File "/usr/lib/python2.7/site-packages/ceph_disk/main.py", line 2640, in prepare_device
[2021-12-10 23:47:35,234][osd3][WARNING]     self.sanity_checks()
[2021-12-10 23:47:35,234][osd3][WARNING]   File "/usr/lib/python2.7/site-packages/ceph_disk/main.py", line 2603, in sanity_checks
[2021-12-10 23:47:35,234][osd3][WARNING]     check_partitions=not self.args.dmcrypt)
[2021-12-10 23:47:35,234][osd3][WARNING]   File "/usr/lib/python2.7/site-packages/ceph_disk/main.py", line 887, in verify_not_in_use
[2021-12-10 23:47:35,234][osd3][WARNING]     raise Error('Device is mounted', partition)
[2021-12-10 23:47:35,234][osd3][WARNING] ceph_disk.main.Error: Error: Device is mounted: /dev/sdb1
[2021-12-10 23:47:35,234][osd3][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2021-12-10 23:47:35,235][ceph_deploy.osd][ERROR ] Failed to execute command: /usr/sbin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdb
[2021-12-10 23:47:35,389][osd4][DEBUG ] connection detected need for sudo
[2021-12-10 23:47:35,558][osd4][DEBUG ] connected to host: osd4 
[2021-12-10 23:47:35,558][osd4][DEBUG ] detect platform information from remote host
[2021-12-10 23:47:35,576][osd4][DEBUG ] detect machine type
[2021-12-10 23:47:35,580][osd4][DEBUG ] find the location of an executable
[2021-12-10 23:47:35,582][ceph_deploy.osd][INFO  ] Distro info: CentOS Linux 7.9.2009 Core
[2021-12-10 23:47:35,582][ceph_deploy.osd][DEBUG ] Deploying osd to osd4
[2021-12-10 23:47:35,582][osd4][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2021-12-10 23:47:35,584][ceph_deploy.osd][DEBUG ] Preparing host osd4 disk /dev/sdb journal None activate False
[2021-12-10 23:47:35,584][osd4][DEBUG ] find the location of an executable
[2021-12-10 23:47:35,587][osd4][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdb
[2021-12-10 23:47:35,654][osd4][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2021-12-10 23:47:35,669][osd4][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --log-file $run_dir/$cluster-osd-check.log --cluster ceph --setuser ceph --setgroup ceph
[2021-12-10 23:47:35,685][osd4][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --log-file $run_dir/$cluster-osd-check.log --cluster ceph --setuser ceph --setgroup ceph
[2021-12-10 23:47:35,717][osd4][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --log-file $run_dir/$cluster-osd-check.log --cluster ceph --setuser ceph --setgroup ceph
[2021-12-10 23:47:35,720][osd4][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-10 23:47:35,721][osd4][WARNING] set_type: Will colocate journal with data on /dev/sdb
[2021-12-10 23:47:35,721][osd4][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2021-12-10 23:47:35,752][osd4][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-10 23:47:35,753][osd4][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-10 23:47:35,753][osd4][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-10 23:47:35,753][osd4][WARNING] Traceback (most recent call last):
[2021-12-10 23:47:35,753][osd4][WARNING]   File "/usr/sbin/ceph-disk", line 9, in <module>
[2021-12-10 23:47:35,753][osd4][WARNING]     load_entry_point('ceph-disk==1.0.0', 'console_scripts', 'ceph-disk')()
[2021-12-10 23:47:35,753][osd4][WARNING]   File "/usr/lib/python2.7/site-packages/ceph_disk/main.py", line 5361, in run
[2021-12-10 23:47:35,757][osd4][WARNING]     main(sys.argv[1:])
[2021-12-10 23:47:35,757][osd4][WARNING]   File "/usr/lib/python2.7/site-packages/ceph_disk/main.py", line 5312, in main
[2021-12-10 23:47:35,757][osd4][WARNING]     args.func(args)
[2021-12-10 23:47:35,757][osd4][WARNING]   File "/usr/lib/python2.7/site-packages/ceph_disk/main.py", line 1890, in main
[2021-12-10 23:47:35,758][osd4][WARNING]     Prepare.factory(args).prepare()
[2021-12-10 23:47:35,758][osd4][WARNING]   File "/usr/lib/python2.7/site-packages/ceph_disk/main.py", line 1879, in prepare
[2021-12-10 23:47:35,758][osd4][WARNING]     self.prepare_locked()
[2021-12-10 23:47:35,758][osd4][WARNING]   File "/usr/lib/python2.7/site-packages/ceph_disk/main.py", line 1910, in prepare_locked
[2021-12-10 23:47:35,758][osd4][WARNING]     self.data.prepare(self.journal)
[2021-12-10 23:47:35,758][osd4][WARNING]   File "/usr/lib/python2.7/site-packages/ceph_disk/main.py", line 2578, in prepare
[2021-12-10 23:47:35,758][osd4][WARNING]     self.prepare_device(*to_prepare_list)
[2021-12-10 23:47:35,758][osd4][WARNING]   File "/usr/lib/python2.7/site-packages/ceph_disk/main.py", line 2738, in prepare_device
[2021-12-10 23:47:35,758][osd4][WARNING]     super(PrepareFilestoreData, self).prepare_device(*to_prepare_list)
[2021-12-10 23:47:35,758][osd4][WARNING]   File "/usr/lib/python2.7/site-packages/ceph_disk/main.py", line 2640, in prepare_device
[2021-12-10 23:47:35,758][osd4][WARNING]     self.sanity_checks()
[2021-12-10 23:47:35,758][osd4][WARNING]   File "/usr/lib/python2.7/site-packages/ceph_disk/main.py", line 2603, in sanity_checks
[2021-12-10 23:47:35,758][osd4][WARNING]     check_partitions=not self.args.dmcrypt)
[2021-12-10 23:47:35,758][osd4][WARNING]   File "/usr/lib/python2.7/site-packages/ceph_disk/main.py", line 887, in verify_not_in_use
[2021-12-10 23:47:35,758][osd4][WARNING]     raise Error('Device is mounted', partition)
[2021-12-10 23:47:35,758][osd4][WARNING] ceph_disk.main.Error: Error: Device is mounted: /dev/sdb1
[2021-12-10 23:47:35,759][osd4][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2021-12-10 23:47:35,759][ceph_deploy.osd][ERROR ] Failed to execute command: /usr/sbin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdb
[2021-12-10 23:47:35,917][osd5][DEBUG ] connection detected need for sudo
[2021-12-10 23:47:36,091][osd5][DEBUG ] connected to host: osd5 
[2021-12-10 23:47:36,092][osd5][DEBUG ] detect platform information from remote host
[2021-12-10 23:47:36,112][osd5][DEBUG ] detect machine type
[2021-12-10 23:47:36,117][osd5][DEBUG ] find the location of an executable
[2021-12-10 23:47:36,118][ceph_deploy.osd][INFO  ] Distro info: CentOS Linux 7.9.2009 Core
[2021-12-10 23:47:36,119][ceph_deploy.osd][DEBUG ] Deploying osd to osd5
[2021-12-10 23:47:36,119][osd5][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2021-12-10 23:47:36,123][ceph_deploy.osd][DEBUG ] Preparing host osd5 disk /dev/sdb journal None activate False
[2021-12-10 23:47:36,123][osd5][DEBUG ] find the location of an executable
[2021-12-10 23:47:36,126][osd5][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdb
[2021-12-10 23:47:36,243][osd5][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2021-12-10 23:47:36,243][osd5][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --log-file $run_dir/$cluster-osd-check.log --cluster ceph --setuser ceph --setgroup ceph
[2021-12-10 23:47:36,243][osd5][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --log-file $run_dir/$cluster-osd-check.log --cluster ceph --setuser ceph --setgroup ceph
[2021-12-10 23:47:36,251][osd5][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --log-file $run_dir/$cluster-osd-check.log --cluster ceph --setuser ceph --setgroup ceph
[2021-12-10 23:47:36,283][osd5][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-10 23:47:36,283][osd5][WARNING] set_type: Will colocate journal with data on /dev/sdb
[2021-12-10 23:47:36,283][osd5][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2021-12-10 23:47:36,286][osd5][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-10 23:47:36,287][osd5][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-10 23:47:36,287][osd5][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-10 23:47:36,287][osd5][WARNING] Traceback (most recent call last):
[2021-12-10 23:47:36,287][osd5][WARNING]   File "/usr/sbin/ceph-disk", line 9, in <module>
[2021-12-10 23:47:36,287][osd5][WARNING]     load_entry_point('ceph-disk==1.0.0', 'console_scripts', 'ceph-disk')()
[2021-12-10 23:47:36,287][osd5][WARNING]   File "/usr/lib/python2.7/site-packages/ceph_disk/main.py", line 5361, in run
[2021-12-10 23:47:36,319][osd5][WARNING]     main(sys.argv[1:])
[2021-12-10 23:47:36,319][osd5][WARNING]   File "/usr/lib/python2.7/site-packages/ceph_disk/main.py", line 5312, in main
[2021-12-10 23:47:36,319][osd5][WARNING]     args.func(args)
[2021-12-10 23:47:36,319][osd5][WARNING]   File "/usr/lib/python2.7/site-packages/ceph_disk/main.py", line 1890, in main
[2021-12-10 23:47:36,319][osd5][WARNING]     Prepare.factory(args).prepare()
[2021-12-10 23:47:36,319][osd5][WARNING]   File "/usr/lib/python2.7/site-packages/ceph_disk/main.py", line 1879, in prepare
[2021-12-10 23:47:36,319][osd5][WARNING]     self.prepare_locked()
[2021-12-10 23:47:36,319][osd5][WARNING]   File "/usr/lib/python2.7/site-packages/ceph_disk/main.py", line 1910, in prepare_locked
[2021-12-10 23:47:36,319][osd5][WARNING]     self.data.prepare(self.journal)
[2021-12-10 23:47:36,319][osd5][WARNING]   File "/usr/lib/python2.7/site-packages/ceph_disk/main.py", line 2578, in prepare
[2021-12-10 23:47:36,320][osd5][WARNING]     self.prepare_device(*to_prepare_list)
[2021-12-10 23:47:36,320][osd5][WARNING]   File "/usr/lib/python2.7/site-packages/ceph_disk/main.py", line 2738, in prepare_device
[2021-12-10 23:47:36,320][osd5][WARNING]     super(PrepareFilestoreData, self).prepare_device(*to_prepare_list)
[2021-12-10 23:47:36,320][osd5][WARNING]   File "/usr/lib/python2.7/site-packages/ceph_disk/main.py", line 2640, in prepare_device
[2021-12-10 23:47:36,320][osd5][WARNING]     self.sanity_checks()
[2021-12-10 23:47:36,320][osd5][WARNING]   File "/usr/lib/python2.7/site-packages/ceph_disk/main.py", line 2603, in sanity_checks
[2021-12-10 23:47:36,320][osd5][WARNING]     check_partitions=not self.args.dmcrypt)
[2021-12-10 23:47:36,320][osd5][WARNING]   File "/usr/lib/python2.7/site-packages/ceph_disk/main.py", line 887, in verify_not_in_use
[2021-12-10 23:47:36,320][osd5][WARNING]     raise Error('Device is mounted', partition)
[2021-12-10 23:47:36,320][osd5][WARNING] ceph_disk.main.Error: Error: Device is mounted: /dev/sdb1
[2021-12-10 23:47:36,320][osd5][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2021-12-10 23:47:36,321][ceph_deploy.osd][ERROR ] Failed to execute command: /usr/sbin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdb
[2021-12-10 23:47:36,321][ceph_deploy][ERROR ] GenericError: Failed to create 5 OSDs

[2021-12-10 23:47:45,557][ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephuser/.cephdeploy.conf
[2021-12-10 23:47:45,557][ceph_deploy.cli][INFO  ] Invoked (1.5.39): /usr/bin/ceph-deploy --overwrite-conf osd prepare osd1:/dev/sdb1 osd2:/dev/sdb1 osd3:/dev/sdb1 osd4:/dev/sdb1 osd5:/dev/sdb1
[2021-12-10 23:47:45,557][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2021-12-10 23:47:45,557][ceph_deploy.cli][INFO  ]  username                      : None
[2021-12-10 23:47:45,557][ceph_deploy.cli][INFO  ]  block_db                      : None
[2021-12-10 23:47:45,557][ceph_deploy.cli][INFO  ]  disk                          : [('osd1', '/dev/sdb1', None), ('osd2', '/dev/sdb1', None), ('osd3', '/dev/sdb1', None), ('osd4', '/dev/sdb1', None), ('osd5', '/dev/sdb1', None)]
[2021-12-10 23:47:45,557][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2021-12-10 23:47:45,557][ceph_deploy.cli][INFO  ]  verbose                       : False
[2021-12-10 23:47:45,557][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2021-12-10 23:47:45,557][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2021-12-10 23:47:45,557][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2021-12-10 23:47:45,558][ceph_deploy.cli][INFO  ]  subcommand                    : prepare
[2021-12-10 23:47:45,558][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2021-12-10 23:47:45,558][ceph_deploy.cli][INFO  ]  quiet                         : False
[2021-12-10 23:47:45,558][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f6dcd185170>
[2021-12-10 23:47:45,558][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2021-12-10 23:47:45,558][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2021-12-10 23:47:45,558][ceph_deploy.cli][INFO  ]  filestore                     : None
[2021-12-10 23:47:45,558][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f6dcd1d2050>
[2021-12-10 23:47:45,558][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2021-12-10 23:47:45,558][ceph_deploy.cli][INFO  ]  default_release               : False
[2021-12-10 23:47:45,558][ceph_deploy.cli][INFO  ]  zap_disk                      : False
[2021-12-10 23:47:45,559][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks osd1:/dev/sdb1: osd2:/dev/sdb1: osd3:/dev/sdb1: osd4:/dev/sdb1: osd5:/dev/sdb1:
[2021-12-10 23:47:45,728][osd1][DEBUG ] connection detected need for sudo
[2021-12-10 23:47:45,901][osd1][DEBUG ] connected to host: osd1 
[2021-12-10 23:47:45,901][osd1][DEBUG ] detect platform information from remote host
[2021-12-10 23:47:45,919][osd1][DEBUG ] detect machine type
[2021-12-10 23:47:45,924][osd1][DEBUG ] find the location of an executable
[2021-12-10 23:47:45,925][ceph_deploy.osd][INFO  ] Distro info: CentOS Linux 7.9.2009 Core
[2021-12-10 23:47:45,925][ceph_deploy.osd][DEBUG ] Deploying osd to osd1
[2021-12-10 23:47:45,925][osd1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2021-12-10 23:47:45,927][ceph_deploy.osd][DEBUG ] Preparing host osd1 disk /dev/sdb1 journal None activate False
[2021-12-10 23:47:45,928][osd1][DEBUG ] find the location of an executable
[2021-12-10 23:47:45,930][osd1][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdb1
[2021-12-10 23:47:46,000][osd1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2021-12-10 23:47:46,016][osd1][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --log-file $run_dir/$cluster-osd-check.log --cluster ceph --setuser ceph --setgroup ceph
[2021-12-10 23:47:46,048][osd1][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --log-file $run_dir/$cluster-osd-check.log --cluster ceph --setuser ceph --setgroup ceph
[2021-12-10 23:47:46,051][osd1][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --log-file $run_dir/$cluster-osd-check.log --cluster ceph --setuser ceph --setgroup ceph
[2021-12-10 23:47:46,084][osd1][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb1 uuid path is /sys/dev/block/8:17/dm/uuid
[2021-12-10 23:47:46,084][osd1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2021-12-10 23:47:46,100][osd1][WARNING] Traceback (most recent call last):
[2021-12-10 23:47:46,100][osd1][WARNING]   File "/usr/sbin/ceph-disk", line 9, in <module>
[2021-12-10 23:47:46,100][osd1][WARNING]     load_entry_point('ceph-disk==1.0.0', 'console_scripts', 'ceph-disk')()
[2021-12-10 23:47:46,100][osd1][WARNING]   File "/usr/lib/python2.7/site-packages/ceph_disk/main.py", line 5361, in run
[2021-12-10 23:47:46,100][osd1][WARNING]     main(sys.argv[1:])
[2021-12-10 23:47:46,100][osd1][WARNING]   File "/usr/lib/python2.7/site-packages/ceph_disk/main.py", line 5312, in main
[2021-12-10 23:47:46,100][osd1][WARNING]     args.func(args)
[2021-12-10 23:47:46,100][osd1][WARNING]   File "/usr/lib/python2.7/site-packages/ceph_disk/main.py", line 1890, in main
[2021-12-10 23:47:46,100][osd1][WARNING]     Prepare.factory(args).prepare()
[2021-12-10 23:47:46,100][osd1][WARNING]   File "/usr/lib/python2.7/site-packages/ceph_disk/main.py", line 1879, in prepare
[2021-12-10 23:47:46,100][osd1][WARNING]     self.prepare_locked()
[2021-12-10 23:47:46,100][osd1][WARNING]   File "/usr/lib/python2.7/site-packages/ceph_disk/main.py", line 1910, in prepare_locked
[2021-12-10 23:47:46,100][osd1][WARNING]     self.data.prepare(self.journal)
[2021-12-10 23:47:46,101][osd1][WARNING]   File "/usr/lib/python2.7/site-packages/ceph_disk/main.py", line 2578, in prepare
[2021-12-10 23:47:46,101][osd1][WARNING]     self.prepare_device(*to_prepare_list)
[2021-12-10 23:47:46,101][osd1][WARNING]   File "/usr/lib/python2.7/site-packages/ceph_disk/main.py", line 2738, in prepare_device
[2021-12-10 23:47:46,101][osd1][WARNING]     super(PrepareFilestoreData, self).prepare_device(*to_prepare_list)
[2021-12-10 23:47:46,101][osd1][WARNING]   File "/usr/lib/python2.7/site-packages/ceph_disk/main.py", line 2640, in prepare_device
[2021-12-10 23:47:46,101][osd1][WARNING]     self.sanity_checks()
[2021-12-10 23:47:46,101][osd1][WARNING]   File "/usr/lib/python2.7/site-packages/ceph_disk/main.py", line 2603, in sanity_checks
[2021-12-10 23:47:46,101][osd1][WARNING]     check_partitions=not self.args.dmcrypt)
[2021-12-10 23:47:46,101][osd1][WARNING]   File "/usr/lib/python2.7/site-packages/ceph_disk/main.py", line 877, in verify_not_in_use
[2021-12-10 23:47:46,101][osd1][WARNING]     raise Error('Device is mounted', dev)
[2021-12-10 23:47:46,101][osd1][WARNING] ceph_disk.main.Error: Error: Device is mounted: /dev/sdb1
[2021-12-10 23:47:46,101][osd1][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2021-12-10 23:47:46,101][ceph_deploy.osd][ERROR ] Failed to execute command: /usr/sbin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdb1
[2021-12-10 23:47:46,257][osd2][DEBUG ] connection detected need for sudo
[2021-12-10 23:47:46,430][osd2][DEBUG ] connected to host: osd2 
[2021-12-10 23:47:46,430][osd2][DEBUG ] detect platform information from remote host
[2021-12-10 23:47:46,448][osd2][DEBUG ] detect machine type
[2021-12-10 23:47:46,453][osd2][DEBUG ] find the location of an executable
[2021-12-10 23:47:46,455][ceph_deploy.osd][INFO  ] Distro info: CentOS Linux 7.9.2009 Core
[2021-12-10 23:47:46,455][ceph_deploy.osd][DEBUG ] Deploying osd to osd2
[2021-12-10 23:47:46,455][osd2][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2021-12-10 23:47:46,457][ceph_deploy.osd][DEBUG ] Preparing host osd2 disk /dev/sdb1 journal None activate False
[2021-12-10 23:47:46,457][osd2][DEBUG ] find the location of an executable
[2021-12-10 23:47:46,460][osd2][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdb1
[2021-12-10 23:47:46,576][osd2][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2021-12-10 23:47:46,577][osd2][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --log-file $run_dir/$cluster-osd-check.log --cluster ceph --setuser ceph --setgroup ceph
[2021-12-10 23:47:46,577][osd2][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --log-file $run_dir/$cluster-osd-check.log --cluster ceph --setuser ceph --setgroup ceph
[2021-12-10 23:47:46,580][osd2][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --log-file $run_dir/$cluster-osd-check.log --cluster ceph --setuser ceph --setgroup ceph
[2021-12-10 23:47:46,612][osd2][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb1 uuid path is /sys/dev/block/8:17/dm/uuid
[2021-12-10 23:47:46,612][osd2][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2021-12-10 23:47:46,628][osd2][WARNING] Traceback (most recent call last):
[2021-12-10 23:47:46,628][osd2][WARNING]   File "/usr/sbin/ceph-disk", line 9, in <module>
[2021-12-10 23:47:46,628][osd2][WARNING]     load_entry_point('ceph-disk==1.0.0', 'console_scripts', 'ceph-disk')()
[2021-12-10 23:47:46,628][osd2][WARNING]   File "/usr/lib/python2.7/site-packages/ceph_disk/main.py", line 5361, in run
[2021-12-10 23:47:46,628][osd2][WARNING]     main(sys.argv[1:])
[2021-12-10 23:47:46,628][osd2][WARNING]   File "/usr/lib/python2.7/site-packages/ceph_disk/main.py", line 5312, in main
[2021-12-10 23:47:46,628][osd2][WARNING]     args.func(args)
[2021-12-10 23:47:46,629][osd2][WARNING]   File "/usr/lib/python2.7/site-packages/ceph_disk/main.py", line 1890, in main
[2021-12-10 23:47:46,629][osd2][WARNING]     Prepare.factory(args).prepare()
[2021-12-10 23:47:46,629][osd2][WARNING]   File "/usr/lib/python2.7/site-packages/ceph_disk/main.py", line 1879, in prepare
[2021-12-10 23:47:46,629][osd2][WARNING]     self.prepare_locked()
[2021-12-10 23:47:46,629][osd2][WARNING]   File "/usr/lib/python2.7/site-packages/ceph_disk/main.py", line 1910, in prepare_locked
[2021-12-10 23:47:46,629][osd2][WARNING]     self.data.prepare(self.journal)
[2021-12-10 23:47:46,629][osd2][WARNING]   File "/usr/lib/python2.7/site-packages/ceph_disk/main.py", line 2578, in prepare
[2021-12-10 23:47:46,629][osd2][WARNING]     self.prepare_device(*to_prepare_list)
[2021-12-10 23:47:46,629][osd2][WARNING]   File "/usr/lib/python2.7/site-packages/ceph_disk/main.py", line 2738, in prepare_device
[2021-12-10 23:47:46,629][osd2][WARNING]     super(PrepareFilestoreData, self).prepare_device(*to_prepare_list)
[2021-12-10 23:47:46,629][osd2][WARNING]   File "/usr/lib/python2.7/site-packages/ceph_disk/main.py", line 2640, in prepare_device
[2021-12-10 23:47:46,629][osd2][WARNING]     self.sanity_checks()
[2021-12-10 23:47:46,629][osd2][WARNING]   File "/usr/lib/python2.7/site-packages/ceph_disk/main.py", line 2603, in sanity_checks
[2021-12-10 23:47:46,629][osd2][WARNING]     check_partitions=not self.args.dmcrypt)
[2021-12-10 23:47:46,629][osd2][WARNING]   File "/usr/lib/python2.7/site-packages/ceph_disk/main.py", line 877, in verify_not_in_use
[2021-12-10 23:47:46,629][osd2][WARNING]     raise Error('Device is mounted', dev)
[2021-12-10 23:47:46,629][osd2][WARNING] ceph_disk.main.Error: Error: Device is mounted: /dev/sdb1
[2021-12-10 23:47:46,630][osd2][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2021-12-10 23:47:46,630][ceph_deploy.osd][ERROR ] Failed to execute command: /usr/sbin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdb1
[2021-12-10 23:47:46,785][osd3][DEBUG ] connection detected need for sudo
[2021-12-10 23:47:46,954][osd3][DEBUG ] connected to host: osd3 
[2021-12-10 23:47:46,955][osd3][DEBUG ] detect platform information from remote host
[2021-12-10 23:47:46,972][osd3][DEBUG ] detect machine type
[2021-12-10 23:47:46,977][osd3][DEBUG ] find the location of an executable
[2021-12-10 23:47:46,978][ceph_deploy.osd][INFO  ] Distro info: CentOS Linux 7.9.2009 Core
[2021-12-10 23:47:46,978][ceph_deploy.osd][DEBUG ] Deploying osd to osd3
[2021-12-10 23:47:46,978][osd3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2021-12-10 23:47:46,981][ceph_deploy.osd][DEBUG ] Preparing host osd3 disk /dev/sdb1 journal None activate False
[2021-12-10 23:47:46,981][osd3][DEBUG ] find the location of an executable
[2021-12-10 23:47:46,984][osd3][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdb1
[2021-12-10 23:47:47,051][osd3][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2021-12-10 23:47:47,066][osd3][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --log-file $run_dir/$cluster-osd-check.log --cluster ceph --setuser ceph --setgroup ceph
[2021-12-10 23:47:47,082][osd3][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --log-file $run_dir/$cluster-osd-check.log --cluster ceph --setuser ceph --setgroup ceph
[2021-12-10 23:47:47,098][osd3][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --log-file $run_dir/$cluster-osd-check.log --cluster ceph --setuser ceph --setgroup ceph
[2021-12-10 23:47:47,113][osd3][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb1 uuid path is /sys/dev/block/8:17/dm/uuid
[2021-12-10 23:47:47,114][osd3][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2021-12-10 23:47:47,145][osd3][WARNING] Traceback (most recent call last):
[2021-12-10 23:47:47,146][osd3][WARNING]   File "/usr/sbin/ceph-disk", line 9, in <module>
[2021-12-10 23:47:47,146][osd3][WARNING]     load_entry_point('ceph-disk==1.0.0', 'console_scripts', 'ceph-disk')()
[2021-12-10 23:47:47,146][osd3][WARNING]   File "/usr/lib/python2.7/site-packages/ceph_disk/main.py", line 5361, in run
[2021-12-10 23:47:47,146][osd3][WARNING]     main(sys.argv[1:])
[2021-12-10 23:47:47,146][osd3][WARNING]   File "/usr/lib/python2.7/site-packages/ceph_disk/main.py", line 5312, in main
[2021-12-10 23:47:47,146][osd3][WARNING]     args.func(args)
[2021-12-10 23:47:47,146][osd3][WARNING]   File "/usr/lib/python2.7/site-packages/ceph_disk/main.py", line 1890, in main
[2021-12-10 23:47:47,146][osd3][WARNING]     Prepare.factory(args).prepare()
[2021-12-10 23:47:47,146][osd3][WARNING]   File "/usr/lib/python2.7/site-packages/ceph_disk/main.py", line 1879, in prepare
[2021-12-10 23:47:47,146][osd3][WARNING]     self.prepare_locked()
[2021-12-10 23:47:47,146][osd3][WARNING]   File "/usr/lib/python2.7/site-packages/ceph_disk/main.py", line 1910, in prepare_locked
[2021-12-10 23:47:47,147][osd3][WARNING]     self.data.prepare(self.journal)
[2021-12-10 23:47:47,147][osd3][WARNING]   File "/usr/lib/python2.7/site-packages/ceph_disk/main.py", line 2578, in prepare
[2021-12-10 23:47:47,147][osd3][WARNING]     self.prepare_device(*to_prepare_list)
[2021-12-10 23:47:47,147][osd3][WARNING]   File "/usr/lib/python2.7/site-packages/ceph_disk/main.py", line 2738, in prepare_device
[2021-12-10 23:47:47,147][osd3][WARNING]     super(PrepareFilestoreData, self).prepare_device(*to_prepare_list)
[2021-12-10 23:47:47,147][osd3][WARNING]   File "/usr/lib/python2.7/site-packages/ceph_disk/main.py", line 2640, in prepare_device
[2021-12-10 23:47:47,147][osd3][WARNING]     self.sanity_checks()
[2021-12-10 23:47:47,147][osd3][WARNING]   File "/usr/lib/python2.7/site-packages/ceph_disk/main.py", line 2603, in sanity_checks
[2021-12-10 23:47:47,147][osd3][WARNING]     check_partitions=not self.args.dmcrypt)
[2021-12-10 23:47:47,147][osd3][WARNING]   File "/usr/lib/python2.7/site-packages/ceph_disk/main.py", line 877, in verify_not_in_use
[2021-12-10 23:47:47,147][osd3][WARNING]     raise Error('Device is mounted', dev)
[2021-12-10 23:47:47,147][osd3][WARNING] ceph_disk.main.Error: Error: Device is mounted: /dev/sdb1
[2021-12-10 23:47:47,147][osd3][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2021-12-10 23:47:47,147][ceph_deploy.osd][ERROR ] Failed to execute command: /usr/sbin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdb1
[2021-12-10 23:47:47,303][osd4][DEBUG ] connection detected need for sudo
[2021-12-10 23:47:47,473][osd4][DEBUG ] connected to host: osd4 
[2021-12-10 23:47:47,474][osd4][DEBUG ] detect platform information from remote host
[2021-12-10 23:47:47,490][osd4][DEBUG ] detect machine type
[2021-12-10 23:47:47,495][osd4][DEBUG ] find the location of an executable
[2021-12-10 23:47:47,496][ceph_deploy.osd][INFO  ] Distro info: CentOS Linux 7.9.2009 Core
[2021-12-10 23:47:47,496][ceph_deploy.osd][DEBUG ] Deploying osd to osd4
[2021-12-10 23:47:47,497][osd4][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2021-12-10 23:47:47,499][ceph_deploy.osd][DEBUG ] Preparing host osd4 disk /dev/sdb1 journal None activate False
[2021-12-10 23:47:47,499][osd4][DEBUG ] find the location of an executable
[2021-12-10 23:47:47,503][osd4][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdb1
[2021-12-10 23:47:47,569][osd4][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2021-12-10 23:47:47,585][osd4][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --log-file $run_dir/$cluster-osd-check.log --cluster ceph --setuser ceph --setgroup ceph
[2021-12-10 23:47:47,601][osd4][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --log-file $run_dir/$cluster-osd-check.log --cluster ceph --setuser ceph --setgroup ceph
[2021-12-10 23:47:47,617][osd4][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --log-file $run_dir/$cluster-osd-check.log --cluster ceph --setuser ceph --setgroup ceph
[2021-12-10 23:47:47,632][osd4][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb1 uuid path is /sys/dev/block/8:17/dm/uuid
[2021-12-10 23:47:47,632][osd4][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2021-12-10 23:47:47,664][osd4][WARNING] Traceback (most recent call last):
[2021-12-10 23:47:47,664][osd4][WARNING]   File "/usr/sbin/ceph-disk", line 9, in <module>
[2021-12-10 23:47:47,664][osd4][WARNING]     load_entry_point('ceph-disk==1.0.0', 'console_scripts', 'ceph-disk')()
[2021-12-10 23:47:47,665][osd4][WARNING]   File "/usr/lib/python2.7/site-packages/ceph_disk/main.py", line 5361, in run
[2021-12-10 23:47:47,665][osd4][WARNING]     main(sys.argv[1:])
[2021-12-10 23:47:47,665][osd4][WARNING]   File "/usr/lib/python2.7/site-packages/ceph_disk/main.py", line 5312, in main
[2021-12-10 23:47:47,665][osd4][WARNING]     args.func(args)
[2021-12-10 23:47:47,665][osd4][WARNING]   File "/usr/lib/python2.7/site-packages/ceph_disk/main.py", line 1890, in main
[2021-12-10 23:47:47,665][osd4][WARNING]     Prepare.factory(args).prepare()
[2021-12-10 23:47:47,665][osd4][WARNING]   File "/usr/lib/python2.7/site-packages/ceph_disk/main.py", line 1879, in prepare
[2021-12-10 23:47:47,665][osd4][WARNING]     self.prepare_locked()
[2021-12-10 23:47:47,665][osd4][WARNING]   File "/usr/lib/python2.7/site-packages/ceph_disk/main.py", line 1910, in prepare_locked
[2021-12-10 23:47:47,665][osd4][WARNING]     self.data.prepare(self.journal)
[2021-12-10 23:47:47,665][osd4][WARNING]   File "/usr/lib/python2.7/site-packages/ceph_disk/main.py", line 2578, in prepare
[2021-12-10 23:47:47,665][osd4][WARNING]     self.prepare_device(*to_prepare_list)
[2021-12-10 23:47:47,665][osd4][WARNING]   File "/usr/lib/python2.7/site-packages/ceph_disk/main.py", line 2738, in prepare_device
[2021-12-10 23:47:47,665][osd4][WARNING]     super(PrepareFilestoreData, self).prepare_device(*to_prepare_list)
[2021-12-10 23:47:47,665][osd4][WARNING]   File "/usr/lib/python2.7/site-packages/ceph_disk/main.py", line 2640, in prepare_device
[2021-12-10 23:47:47,665][osd4][WARNING]     self.sanity_checks()
[2021-12-10 23:47:47,665][osd4][WARNING]   File "/usr/lib/python2.7/site-packages/ceph_disk/main.py", line 2603, in sanity_checks
[2021-12-10 23:47:47,666][osd4][WARNING]     check_partitions=not self.args.dmcrypt)
[2021-12-10 23:47:47,666][osd4][WARNING]   File "/usr/lib/python2.7/site-packages/ceph_disk/main.py", line 877, in verify_not_in_use
[2021-12-10 23:47:47,666][osd4][WARNING]     raise Error('Device is mounted', dev)
[2021-12-10 23:47:47,666][osd4][WARNING] ceph_disk.main.Error: Error: Device is mounted: /dev/sdb1
[2021-12-10 23:47:47,666][osd4][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2021-12-10 23:47:47,666][ceph_deploy.osd][ERROR ] Failed to execute command: /usr/sbin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdb1
[2021-12-10 23:47:47,821][osd5][DEBUG ] connection detected need for sudo
[2021-12-10 23:47:47,993][osd5][DEBUG ] connected to host: osd5 
[2021-12-10 23:47:47,994][osd5][DEBUG ] detect platform information from remote host
[2021-12-10 23:47:48,013][osd5][DEBUG ] detect machine type
[2021-12-10 23:47:48,018][osd5][DEBUG ] find the location of an executable
[2021-12-10 23:47:48,019][ceph_deploy.osd][INFO  ] Distro info: CentOS Linux 7.9.2009 Core
[2021-12-10 23:47:48,019][ceph_deploy.osd][DEBUG ] Deploying osd to osd5
[2021-12-10 23:47:48,020][osd5][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2021-12-10 23:47:48,022][ceph_deploy.osd][DEBUG ] Preparing host osd5 disk /dev/sdb1 journal None activate False
[2021-12-10 23:47:48,023][osd5][DEBUG ] find the location of an executable
[2021-12-10 23:47:48,025][osd5][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdb1
[2021-12-10 23:47:48,142][osd5][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2021-12-10 23:47:48,142][osd5][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --log-file $run_dir/$cluster-osd-check.log --cluster ceph --setuser ceph --setgroup ceph
[2021-12-10 23:47:48,142][osd5][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --log-file $run_dir/$cluster-osd-check.log --cluster ceph --setuser ceph --setgroup ceph
[2021-12-10 23:47:48,146][osd5][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --log-file $run_dir/$cluster-osd-check.log --cluster ceph --setuser ceph --setgroup ceph
[2021-12-10 23:47:48,178][osd5][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb1 uuid path is /sys/dev/block/8:17/dm/uuid
[2021-12-10 23:47:48,178][osd5][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2021-12-10 23:47:48,193][osd5][WARNING] Traceback (most recent call last):
[2021-12-10 23:47:48,194][osd5][WARNING]   File "/usr/sbin/ceph-disk", line 9, in <module>
[2021-12-10 23:47:48,194][osd5][WARNING]     load_entry_point('ceph-disk==1.0.0', 'console_scripts', 'ceph-disk')()
[2021-12-10 23:47:48,194][osd5][WARNING]   File "/usr/lib/python2.7/site-packages/ceph_disk/main.py", line 5361, in run
[2021-12-10 23:47:48,194][osd5][WARNING]     main(sys.argv[1:])
[2021-12-10 23:47:48,194][osd5][WARNING]   File "/usr/lib/python2.7/site-packages/ceph_disk/main.py", line 5312, in main
[2021-12-10 23:47:48,194][osd5][WARNING]     args.func(args)
[2021-12-10 23:47:48,194][osd5][WARNING]   File "/usr/lib/python2.7/site-packages/ceph_disk/main.py", line 1890, in main
[2021-12-10 23:47:48,194][osd5][WARNING]     Prepare.factory(args).prepare()
[2021-12-10 23:47:48,194][osd5][WARNING]   File "/usr/lib/python2.7/site-packages/ceph_disk/main.py", line 1879, in prepare
[2021-12-10 23:47:48,194][osd5][WARNING]     self.prepare_locked()
[2021-12-10 23:47:48,194][osd5][WARNING]   File "/usr/lib/python2.7/site-packages/ceph_disk/main.py", line 1910, in prepare_locked
[2021-12-10 23:47:48,194][osd5][WARNING]     self.data.prepare(self.journal)
[2021-12-10 23:47:48,194][osd5][WARNING]   File "/usr/lib/python2.7/site-packages/ceph_disk/main.py", line 2578, in prepare
[2021-12-10 23:47:48,194][osd5][WARNING]     self.prepare_device(*to_prepare_list)
[2021-12-10 23:47:48,194][osd5][WARNING]   File "/usr/lib/python2.7/site-packages/ceph_disk/main.py", line 2738, in prepare_device
[2021-12-10 23:47:48,194][osd5][WARNING]     super(PrepareFilestoreData, self).prepare_device(*to_prepare_list)
[2021-12-10 23:47:48,194][osd5][WARNING]   File "/usr/lib/python2.7/site-packages/ceph_disk/main.py", line 2640, in prepare_device
[2021-12-10 23:47:48,195][osd5][WARNING]     self.sanity_checks()
[2021-12-10 23:47:48,195][osd5][WARNING]   File "/usr/lib/python2.7/site-packages/ceph_disk/main.py", line 2603, in sanity_checks
[2021-12-10 23:47:48,195][osd5][WARNING]     check_partitions=not self.args.dmcrypt)
[2021-12-10 23:47:48,195][osd5][WARNING]   File "/usr/lib/python2.7/site-packages/ceph_disk/main.py", line 877, in verify_not_in_use
[2021-12-10 23:47:48,195][osd5][WARNING]     raise Error('Device is mounted', dev)
[2021-12-10 23:47:48,195][osd5][WARNING] ceph_disk.main.Error: Error: Device is mounted: /dev/sdb1
[2021-12-10 23:47:48,195][osd5][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2021-12-10 23:47:48,195][ceph_deploy.osd][ERROR ] Failed to execute command: /usr/sbin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdb1
[2021-12-10 23:47:48,195][ceph_deploy][ERROR ] GenericError: Failed to create 5 OSDs

[2021-12-11 00:20:35,873][ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephuser/.cephdeploy.conf
[2021-12-11 00:20:35,874][ceph_deploy.cli][INFO  ] Invoked (1.5.39): /usr/bin/ceph-deploy disk list osd1
[2021-12-11 00:20:35,874][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2021-12-11 00:20:35,874][ceph_deploy.cli][INFO  ]  username                      : None
[2021-12-11 00:20:35,874][ceph_deploy.cli][INFO  ]  verbose                       : False
[2021-12-11 00:20:35,874][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2021-12-11 00:20:35,874][ceph_deploy.cli][INFO  ]  subcommand                    : list
[2021-12-11 00:20:35,874][ceph_deploy.cli][INFO  ]  quiet                         : False
[2021-12-11 00:20:35,874][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fcb4a72ef38>
[2021-12-11 00:20:35,874][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2021-12-11 00:20:35,874][ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7fcb4a70a0c8>
[2021-12-11 00:20:35,874][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2021-12-11 00:20:35,875][ceph_deploy.cli][INFO  ]  default_release               : False
[2021-12-11 00:20:35,875][ceph_deploy.cli][INFO  ]  disk                          : [('osd1', None, None)]
[2021-12-11 00:20:36,045][osd1][DEBUG ] connection detected need for sudo
[2021-12-11 00:20:36,211][osd1][DEBUG ] connected to host: osd1 
[2021-12-11 00:20:36,211][osd1][DEBUG ] detect platform information from remote host
[2021-12-11 00:20:36,229][osd1][DEBUG ] detect machine type
[2021-12-11 00:20:36,234][osd1][DEBUG ] find the location of an executable
[2021-12-11 00:20:36,236][ceph_deploy.osd][INFO  ] Distro info: CentOS Linux 7.9.2009 Core
[2021-12-11 00:20:36,236][ceph_deploy.osd][DEBUG ] Listing disks on osd1...
[2021-12-11 00:20:36,236][osd1][DEBUG ] find the location of an executable
[2021-12-11 00:20:36,238][osd1][INFO  ] Running command: sudo /usr/sbin/ceph-disk list
[2021-12-11 00:20:36,407][osd1][DEBUG ] /dev/dm-0 other, xfs, mounted on /
[2021-12-11 00:20:36,407][osd1][DEBUG ] /dev/dm-1 swap, swap
[2021-12-11 00:20:36,408][osd1][DEBUG ] /dev/sda :
[2021-12-11 00:20:36,408][osd1][DEBUG ]  /dev/sda2 other, LVM2_member
[2021-12-11 00:20:36,408][osd1][DEBUG ]  /dev/sda1 other, xfs, mounted on /boot
[2021-12-11 00:20:36,408][osd1][DEBUG ] /dev/sdb :
[2021-12-11 00:20:36,408][osd1][DEBUG ]  /dev/sdb2 other
[2021-12-11 00:20:36,408][osd1][DEBUG ]  /dev/sdb1 other, xfs, mounted on /var/lib/ceph/osd/ceph-0
[2021-12-11 00:20:36,408][osd1][DEBUG ] /dev/sr0 other, unknown
[2021-12-11 00:20:40,072][ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephuser/.cephdeploy.conf
[2021-12-11 00:20:40,072][ceph_deploy.cli][INFO  ] Invoked (1.5.39): /usr/bin/ceph-deploy disk list osd1 2
[2021-12-11 00:20:40,073][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2021-12-11 00:20:40,073][ceph_deploy.cli][INFO  ]  username                      : None
[2021-12-11 00:20:40,073][ceph_deploy.cli][INFO  ]  verbose                       : False
[2021-12-11 00:20:40,073][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2021-12-11 00:20:40,073][ceph_deploy.cli][INFO  ]  subcommand                    : list
[2021-12-11 00:20:40,073][ceph_deploy.cli][INFO  ]  quiet                         : False
[2021-12-11 00:20:40,073][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fe9c9149f38>
[2021-12-11 00:20:40,073][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2021-12-11 00:20:40,073][ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7fe9c91250c8>
[2021-12-11 00:20:40,073][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2021-12-11 00:20:40,073][ceph_deploy.cli][INFO  ]  default_release               : False
[2021-12-11 00:20:40,073][ceph_deploy.cli][INFO  ]  disk                          : [('osd1', None, None), ('2', None, None)]
[2021-12-11 00:20:40,246][osd1][DEBUG ] connection detected need for sudo
[2021-12-11 00:20:40,417][osd1][DEBUG ] connected to host: osd1 
[2021-12-11 00:20:40,418][osd1][DEBUG ] detect platform information from remote host
[2021-12-11 00:20:40,437][osd1][DEBUG ] detect machine type
[2021-12-11 00:20:40,442][osd1][DEBUG ] find the location of an executable
[2021-12-11 00:20:40,444][ceph_deploy.osd][INFO  ] Distro info: CentOS Linux 7.9.2009 Core
[2021-12-11 00:20:40,444][ceph_deploy.osd][DEBUG ] Listing disks on osd1...
[2021-12-11 00:20:40,444][osd1][DEBUG ] find the location of an executable
[2021-12-11 00:20:40,446][osd1][INFO  ] Running command: sudo /usr/sbin/ceph-disk list
[2021-12-11 00:20:40,668][osd1][DEBUG ] /dev/dm-0 other, xfs, mounted on /
[2021-12-11 00:20:40,668][osd1][DEBUG ] /dev/dm-1 swap, swap
[2021-12-11 00:20:40,668][osd1][DEBUG ] /dev/sda :
[2021-12-11 00:20:40,668][osd1][DEBUG ]  /dev/sda2 other, LVM2_member
[2021-12-11 00:20:40,668][osd1][DEBUG ]  /dev/sda1 other, xfs, mounted on /boot
[2021-12-11 00:20:40,668][osd1][DEBUG ] /dev/sdb :
[2021-12-11 00:20:40,668][osd1][DEBUG ]  /dev/sdb2 other
[2021-12-11 00:20:40,668][osd1][DEBUG ]  /dev/sdb1 other, xfs, mounted on /var/lib/ceph/osd/ceph-0
[2021-12-11 00:20:40,668][osd1][DEBUG ] /dev/sr0 other, unknown
[2021-12-11 00:20:40,676][ceph_deploy][ERROR ] RuntimeError: connecting to host: 2 resulted in errors: IOError [Errno 32] Broken pipe

[2021-12-11 00:20:42,309][ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephuser/.cephdeploy.conf
[2021-12-11 00:20:42,309][ceph_deploy.cli][INFO  ] Invoked (1.5.39): /usr/bin/ceph-deploy disk list osd2
[2021-12-11 00:20:42,309][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2021-12-11 00:20:42,309][ceph_deploy.cli][INFO  ]  username                      : None
[2021-12-11 00:20:42,309][ceph_deploy.cli][INFO  ]  verbose                       : False
[2021-12-11 00:20:42,309][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2021-12-11 00:20:42,309][ceph_deploy.cli][INFO  ]  subcommand                    : list
[2021-12-11 00:20:42,309][ceph_deploy.cli][INFO  ]  quiet                         : False
[2021-12-11 00:20:42,309][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fd2c7a67f38>
[2021-12-11 00:20:42,309][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2021-12-11 00:20:42,309][ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7fd2c7a430c8>
[2021-12-11 00:20:42,309][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2021-12-11 00:20:42,309][ceph_deploy.cli][INFO  ]  default_release               : False
[2021-12-11 00:20:42,309][ceph_deploy.cli][INFO  ]  disk                          : [('osd2', None, None)]
[2021-12-11 00:20:42,480][osd2][DEBUG ] connection detected need for sudo
[2021-12-11 00:20:42,655][osd2][DEBUG ] connected to host: osd2 
[2021-12-11 00:20:42,655][osd2][DEBUG ] detect platform information from remote host
[2021-12-11 00:20:42,673][osd2][DEBUG ] detect machine type
[2021-12-11 00:20:42,677][osd2][DEBUG ] find the location of an executable
[2021-12-11 00:20:42,678][ceph_deploy.osd][INFO  ] Distro info: CentOS Linux 7.9.2009 Core
[2021-12-11 00:20:42,678][ceph_deploy.osd][DEBUG ] Listing disks on osd2...
[2021-12-11 00:20:42,678][osd2][DEBUG ] find the location of an executable
[2021-12-11 00:20:42,680][osd2][INFO  ] Running command: sudo /usr/sbin/ceph-disk list
[2021-12-11 00:20:42,950][osd2][DEBUG ] /dev/dm-0 other, xfs, mounted on /
[2021-12-11 00:20:42,950][osd2][DEBUG ] /dev/dm-1 swap, swap
[2021-12-11 00:20:42,950][osd2][DEBUG ] /dev/sda :
[2021-12-11 00:20:42,950][osd2][DEBUG ]  /dev/sda2 other, LVM2_member
[2021-12-11 00:20:42,950][osd2][DEBUG ]  /dev/sda1 other, xfs, mounted on /boot
[2021-12-11 00:20:42,950][osd2][DEBUG ] /dev/sdb :
[2021-12-11 00:20:42,951][osd2][DEBUG ]  /dev/sdb2 ceph journal, for /dev/sdb1
[2021-12-11 00:20:42,951][osd2][DEBUG ]  /dev/sdb1 ceph data, active, unknown cluster 67d9e224-a6f7-43d3-ae36-e6100c59258e, osd.1, journal /dev/sdb2
[2021-12-11 00:20:42,951][osd2][DEBUG ] /dev/sr0 other, unknown
[2021-12-11 00:21:55,782][ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephuser/.cephdeploy.conf
[2021-12-11 00:21:55,783][ceph_deploy.cli][INFO  ] Invoked (1.5.39): /usr/bin/ceph-deploy disk zap osd1:/dev/sdb
[2021-12-11 00:21:55,783][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2021-12-11 00:21:55,783][ceph_deploy.cli][INFO  ]  username                      : None
[2021-12-11 00:21:55,783][ceph_deploy.cli][INFO  ]  verbose                       : False
[2021-12-11 00:21:55,783][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2021-12-11 00:21:55,783][ceph_deploy.cli][INFO  ]  subcommand                    : zap
[2021-12-11 00:21:55,783][ceph_deploy.cli][INFO  ]  quiet                         : False
[2021-12-11 00:21:55,783][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f4e8fadcf38>
[2021-12-11 00:21:55,783][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2021-12-11 00:21:55,783][ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7f4e8fab80c8>
[2021-12-11 00:21:55,784][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2021-12-11 00:21:55,784][ceph_deploy.cli][INFO  ]  default_release               : False
[2021-12-11 00:21:55,784][ceph_deploy.cli][INFO  ]  disk                          : [('osd1', '/dev/sdb', None)]
[2021-12-11 00:21:55,784][ceph_deploy.osd][DEBUG ] zapping /dev/sdb on osd1
[2021-12-11 00:21:55,952][osd1][DEBUG ] connection detected need for sudo
[2021-12-11 00:21:56,120][osd1][DEBUG ] connected to host: osd1 
[2021-12-11 00:21:56,121][osd1][DEBUG ] detect platform information from remote host
[2021-12-11 00:21:56,139][osd1][DEBUG ] detect machine type
[2021-12-11 00:21:56,149][osd1][DEBUG ] find the location of an executable
[2021-12-11 00:21:56,150][ceph_deploy.osd][INFO  ] Distro info: CentOS Linux 7.9.2009 Core
[2021-12-11 00:21:56,151][osd1][DEBUG ] zeroing last few blocks of device
[2021-12-11 00:21:56,152][osd1][DEBUG ] find the location of an executable
[2021-12-11 00:21:56,156][osd1][INFO  ] Running command: sudo /usr/sbin/ceph-disk zap /dev/sdb
[2021-12-11 00:21:56,275][osd1][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[2021-12-11 00:21:56,276][osd1][WARNING] backup header from main header.
[2021-12-11 00:21:56,276][osd1][WARNING] 
[2021-12-11 00:21:57,443][osd1][DEBUG ] ****************************************************************************
[2021-12-11 00:21:57,443][osd1][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[2021-12-11 00:21:57,443][osd1][DEBUG ] verification and recovery are STRONGLY recommended.
[2021-12-11 00:21:57,443][osd1][DEBUG ] ****************************************************************************
[2021-12-11 00:21:57,443][osd1][DEBUG ] Warning: The kernel is still using the old partition table.
[2021-12-11 00:21:57,444][osd1][DEBUG ] The new table will be used at the next reboot.
[2021-12-11 00:21:57,444][osd1][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[2021-12-11 00:21:57,444][osd1][DEBUG ] other utilities.
[2021-12-11 00:21:58,511][osd1][DEBUG ] Creating new GPT entries.
[2021-12-11 00:21:58,511][osd1][DEBUG ] Warning: The kernel is still using the old partition table.
[2021-12-11 00:21:58,511][osd1][DEBUG ] The new table will be used at the next reboot.
[2021-12-11 00:21:58,511][osd1][DEBUG ] The operation has completed successfully.
[2021-12-11 00:23:54,696][ceph_deploy][ERROR ] KeyboardInterrupt

[2021-12-11 18:38:58,691][ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephuser/.cephdeploy.conf
[2021-12-11 18:38:58,691][ceph_deploy.cli][INFO  ] Invoked (1.5.39): /usr/bin/ceph-deploy admin osd3 osd4
[2021-12-11 18:38:58,691][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2021-12-11 18:38:58,691][ceph_deploy.cli][INFO  ]  username                      : None
[2021-12-11 18:38:58,691][ceph_deploy.cli][INFO  ]  verbose                       : False
[2021-12-11 18:38:58,692][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2021-12-11 18:38:58,692][ceph_deploy.cli][INFO  ]  quiet                         : False
[2021-12-11 18:38:58,692][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fade398bb48>
[2021-12-11 18:38:58,692][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2021-12-11 18:38:58,692][ceph_deploy.cli][INFO  ]  client                        : ['osd3', 'osd4']
[2021-12-11 18:38:58,692][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7fade46a4a28>
[2021-12-11 18:38:58,692][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2021-12-11 18:38:58,692][ceph_deploy.cli][INFO  ]  default_release               : False
[2021-12-11 18:38:58,692][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to osd3
[2021-12-11 18:38:58,862][osd3][DEBUG ] connection detected need for sudo
[2021-12-11 18:38:59,034][osd3][DEBUG ] connected to host: osd3 
[2021-12-11 18:38:59,034][osd3][DEBUG ] detect platform information from remote host
[2021-12-11 18:38:59,062][osd3][DEBUG ] detect machine type
[2021-12-11 18:38:59,067][osd3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2021-12-11 18:38:59,070][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to osd4
[2021-12-11 18:38:59,228][osd4][DEBUG ] connection detected need for sudo
[2021-12-11 18:38:59,401][osd4][DEBUG ] connected to host: osd4 
[2021-12-11 18:38:59,401][osd4][DEBUG ] detect platform information from remote host
[2021-12-11 18:38:59,420][osd4][DEBUG ] detect machine type
[2021-12-11 18:38:59,425][osd4][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2021-12-11 18:42:10,023][ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephuser/.cephdeploy.conf
[2021-12-11 18:42:10,024][ceph_deploy.cli][INFO  ] Invoked (1.5.39): /usr/bin/ceph-deploy admin ceph-admin
[2021-12-11 18:42:10,024][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2021-12-11 18:42:10,024][ceph_deploy.cli][INFO  ]  username                      : None
[2021-12-11 18:42:10,024][ceph_deploy.cli][INFO  ]  verbose                       : False
[2021-12-11 18:42:10,024][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2021-12-11 18:42:10,024][ceph_deploy.cli][INFO  ]  quiet                         : False
[2021-12-11 18:42:10,024][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fcad1a55b48>
[2021-12-11 18:42:10,024][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2021-12-11 18:42:10,024][ceph_deploy.cli][INFO  ]  client                        : ['ceph-admin']
[2021-12-11 18:42:10,024][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7fcad276ea28>
[2021-12-11 18:42:10,024][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2021-12-11 18:42:10,025][ceph_deploy.cli][INFO  ]  default_release               : False
[2021-12-11 18:42:10,025][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to ceph-admin
[2021-12-11 18:42:10,057][ceph-admin][DEBUG ] connection detected need for sudo
[2021-12-11 18:42:10,084][ceph-admin][DEBUG ] connected to host: ceph-admin 
[2021-12-11 18:42:10,084][ceph-admin][DEBUG ] detect platform information from remote host
[2021-12-11 18:42:10,101][ceph-admin][DEBUG ] detect machine type
[2021-12-11 18:42:10,106][ceph-admin][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2021-12-11 18:42:10,107][ceph_deploy.admin][ERROR ] RuntimeError: config file /etc/ceph/ceph.conf exists with different content; use --overwrite-conf to overwrite
[2021-12-11 18:42:10,107][ceph_deploy][ERROR ] GenericError: Failed to configure 1 admin hosts

[2021-12-11 18:42:20,001][ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephuser/.cephdeploy.conf
[2021-12-11 18:42:20,001][ceph_deploy.cli][INFO  ] Invoked (1.5.39): /usr/bin/ceph-deploy --overwrite-conf admin ceph-admin
[2021-12-11 18:42:20,001][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2021-12-11 18:42:20,001][ceph_deploy.cli][INFO  ]  username                      : None
[2021-12-11 18:42:20,001][ceph_deploy.cli][INFO  ]  verbose                       : False
[2021-12-11 18:42:20,001][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2021-12-11 18:42:20,002][ceph_deploy.cli][INFO  ]  quiet                         : False
[2021-12-11 18:42:20,002][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7ff587334b48>
[2021-12-11 18:42:20,002][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2021-12-11 18:42:20,002][ceph_deploy.cli][INFO  ]  client                        : ['ceph-admin']
[2021-12-11 18:42:20,002][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7ff58804da28>
[2021-12-11 18:42:20,002][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2021-12-11 18:42:20,002][ceph_deploy.cli][INFO  ]  default_release               : False
[2021-12-11 18:42:20,002][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to ceph-admin
[2021-12-11 18:42:20,034][ceph-admin][DEBUG ] connection detected need for sudo
[2021-12-11 18:42:20,062][ceph-admin][DEBUG ] connected to host: ceph-admin 
[2021-12-11 18:42:20,063][ceph-admin][DEBUG ] detect platform information from remote host
[2021-12-11 18:42:20,079][ceph-admin][DEBUG ] detect machine type
[2021-12-11 18:42:20,084][ceph-admin][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2021-12-11 18:47:27,654][ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephuser/.cephdeploy.conf
[2021-12-11 18:47:27,655][ceph_deploy.cli][INFO  ] Invoked (1.5.39): /usr/bin/ceph-deploy admin osd1 osd2
[2021-12-11 18:47:27,655][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2021-12-11 18:47:27,655][ceph_deploy.cli][INFO  ]  username                      : None
[2021-12-11 18:47:27,655][ceph_deploy.cli][INFO  ]  verbose                       : False
[2021-12-11 18:47:27,655][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2021-12-11 18:47:27,655][ceph_deploy.cli][INFO  ]  quiet                         : False
[2021-12-11 18:47:27,655][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f5d8aebeb48>
[2021-12-11 18:47:27,655][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2021-12-11 18:47:27,655][ceph_deploy.cli][INFO  ]  client                        : ['osd1', 'osd2']
[2021-12-11 18:47:27,655][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7f5d8bbd7a28>
[2021-12-11 18:47:27,655][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2021-12-11 18:47:27,655][ceph_deploy.cli][INFO  ]  default_release               : False
[2021-12-11 18:47:27,656][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to osd1
[2021-12-11 18:47:27,856][osd1][DEBUG ] connection detected need for sudo
[2021-12-11 18:47:28,067][osd1][DEBUG ] connected to host: osd1 
[2021-12-11 18:47:28,068][osd1][DEBUG ] detect platform information from remote host
[2021-12-11 18:47:28,098][osd1][DEBUG ] detect machine type
[2021-12-11 18:47:28,102][osd1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2021-12-11 18:47:28,105][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to osd2
[2021-12-11 18:47:28,271][osd2][DEBUG ] connection detected need for sudo
[2021-12-11 18:47:28,481][osd2][DEBUG ] connected to host: osd2 
[2021-12-11 18:47:28,482][osd2][DEBUG ] detect platform information from remote host
[2021-12-11 18:47:28,523][osd2][DEBUG ] detect machine type
[2021-12-11 18:47:28,528][osd2][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2021-12-11 19:00:32,667][ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephuser/.cephdeploy.conf
[2021-12-11 19:00:32,667][ceph_deploy.cli][INFO  ] Invoked (1.5.39): /usr/bin/ceph-deploy --overwrite-conf mon create-initial
[2021-12-11 19:00:32,667][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2021-12-11 19:00:32,667][ceph_deploy.cli][INFO  ]  username                      : None
[2021-12-11 19:00:32,667][ceph_deploy.cli][INFO  ]  verbose                       : False
[2021-12-11 19:00:32,667][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2021-12-11 19:00:32,667][ceph_deploy.cli][INFO  ]  subcommand                    : create-initial
[2021-12-11 19:00:32,667][ceph_deploy.cli][INFO  ]  quiet                         : False
[2021-12-11 19:00:32,668][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f321aeddf38>
[2021-12-11 19:00:32,668][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2021-12-11 19:00:32,668][ceph_deploy.cli][INFO  ]  func                          : <function mon at 0x7f321aec4758>
[2021-12-11 19:00:32,668][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2021-12-11 19:00:32,668][ceph_deploy.cli][INFO  ]  default_release               : False
[2021-12-11 19:00:32,668][ceph_deploy.cli][INFO  ]  keyrings                      : None
[2021-12-11 19:00:32,669][ceph_deploy.mon][DEBUG ] Deploying mon, cluster ceph hosts mon1
[2021-12-11 19:00:32,669][ceph_deploy.mon][DEBUG ] detecting platform for host mon1 ...
[2021-12-11 19:00:32,848][mon1][DEBUG ] connection detected need for sudo
[2021-12-11 19:00:33,063][mon1][DEBUG ] connected to host: mon1 
[2021-12-11 19:00:33,064][mon1][DEBUG ] detect platform information from remote host
[2021-12-11 19:00:33,081][mon1][DEBUG ] detect machine type
[2021-12-11 19:00:33,085][mon1][DEBUG ] find the location of an executable
[2021-12-11 19:00:33,087][ceph_deploy.mon][INFO  ] distro info: CentOS Linux 7.9.2009 Core
[2021-12-11 19:00:33,087][mon1][DEBUG ] determining if provided host has same hostname in remote
[2021-12-11 19:00:33,087][mon1][DEBUG ] get remote short hostname
[2021-12-11 19:00:33,088][mon1][DEBUG ] deploying mon to mon1
[2021-12-11 19:00:33,088][mon1][DEBUG ] get remote short hostname
[2021-12-11 19:00:33,089][mon1][DEBUG ] remote hostname: mon1
[2021-12-11 19:00:33,091][mon1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2021-12-11 19:00:33,092][mon1][DEBUG ] create the mon path if it does not exist
[2021-12-11 19:00:33,093][mon1][DEBUG ] checking for done path: /var/lib/ceph/mon/ceph-mon1/done
[2021-12-11 19:00:33,094][mon1][DEBUG ] create a done file to avoid re-doing the mon deployment
[2021-12-11 19:00:33,095][mon1][DEBUG ] create the init path if it does not exist
[2021-12-11 19:00:33,097][mon1][INFO  ] Running command: sudo systemctl enable ceph.target
[2021-12-11 19:00:33,169][mon1][INFO  ] Running command: sudo systemctl enable ceph-mon@mon1
[2021-12-11 19:00:33,237][mon1][INFO  ] Running command: sudo systemctl start ceph-mon@mon1
[2021-12-11 19:00:35,252][mon1][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.mon1.asok mon_status
[2021-12-11 19:00:35,317][mon1][DEBUG ] ********************************************************************************
[2021-12-11 19:00:35,317][mon1][DEBUG ] status for monitor: mon.mon1
[2021-12-11 19:00:35,317][mon1][DEBUG ] {
[2021-12-11 19:00:35,317][mon1][DEBUG ]   "election_epoch": 15, 
[2021-12-11 19:00:35,317][mon1][DEBUG ]   "extra_probe_peers": [], 
[2021-12-11 19:00:35,317][mon1][DEBUG ]   "monmap": {
[2021-12-11 19:00:35,317][mon1][DEBUG ]     "created": "2021-11-03 20:50:42.718657", 
[2021-12-11 19:00:35,317][mon1][DEBUG ]     "epoch": 1, 
[2021-12-11 19:00:35,317][mon1][DEBUG ]     "fsid": "67d9e224-a6f7-43d3-ae36-e6100c59258e", 
[2021-12-11 19:00:35,317][mon1][DEBUG ]     "modified": "2021-11-03 20:50:42.718657", 
[2021-12-11 19:00:35,317][mon1][DEBUG ]     "mons": [
[2021-12-11 19:00:35,317][mon1][DEBUG ]       {
[2021-12-11 19:00:35,317][mon1][DEBUG ]         "addr": "192.168.1.93:6789/0", 
[2021-12-11 19:00:35,317][mon1][DEBUG ]         "name": "mon1", 
[2021-12-11 19:00:35,317][mon1][DEBUG ]         "rank": 0
[2021-12-11 19:00:35,318][mon1][DEBUG ]       }
[2021-12-11 19:00:35,318][mon1][DEBUG ]     ]
[2021-12-11 19:00:35,318][mon1][DEBUG ]   }, 
[2021-12-11 19:00:35,318][mon1][DEBUG ]   "name": "mon1", 
[2021-12-11 19:00:35,318][mon1][DEBUG ]   "outside_quorum": [], 
[2021-12-11 19:00:35,318][mon1][DEBUG ]   "quorum": [
[2021-12-11 19:00:35,318][mon1][DEBUG ]     0
[2021-12-11 19:00:35,318][mon1][DEBUG ]   ], 
[2021-12-11 19:00:35,318][mon1][DEBUG ]   "rank": 0, 
[2021-12-11 19:00:35,318][mon1][DEBUG ]   "state": "leader", 
[2021-12-11 19:00:35,318][mon1][DEBUG ]   "sync_provider": []
[2021-12-11 19:00:35,318][mon1][DEBUG ] }
[2021-12-11 19:00:35,318][mon1][DEBUG ] ********************************************************************************
[2021-12-11 19:00:35,318][mon1][INFO  ] monitor: mon.mon1 is running
[2021-12-11 19:00:35,320][mon1][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.mon1.asok mon_status
[2021-12-11 19:00:35,435][ceph_deploy.mon][INFO  ] processing monitor mon.mon1
[2021-12-11 19:00:35,591][mon1][DEBUG ] connection detected need for sudo
[2021-12-11 19:00:35,753][mon1][DEBUG ] connected to host: mon1 
[2021-12-11 19:00:35,753][mon1][DEBUG ] detect platform information from remote host
[2021-12-11 19:00:35,770][mon1][DEBUG ] detect machine type
[2021-12-11 19:00:35,774][mon1][DEBUG ] find the location of an executable
[2021-12-11 19:00:35,777][mon1][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.mon1.asok mon_status
[2021-12-11 19:00:35,842][ceph_deploy.mon][INFO  ] mon.mon1 monitor has reached quorum!
[2021-12-11 19:00:35,842][ceph_deploy.mon][INFO  ] all initial monitors are running and have formed quorum
[2021-12-11 19:00:35,842][ceph_deploy.mon][INFO  ] Running gatherkeys...
[2021-12-11 19:00:35,842][ceph_deploy.gatherkeys][INFO  ] Storing keys in temp directory /tmp/tmp5PO2hh
[2021-12-11 19:00:35,999][mon1][DEBUG ] connection detected need for sudo
[2021-12-11 19:00:36,160][mon1][DEBUG ] connected to host: mon1 
[2021-12-11 19:00:36,160][mon1][DEBUG ] detect platform information from remote host
[2021-12-11 19:00:36,176][mon1][DEBUG ] detect machine type
[2021-12-11 19:00:36,181][mon1][DEBUG ] get remote short hostname
[2021-12-11 19:00:36,182][mon1][DEBUG ] fetch remote file
[2021-12-11 19:00:36,185][mon1][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --admin-daemon=/var/run/ceph/ceph-mon.mon1.asok mon_status
[2021-12-11 19:00:36,251][mon1][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-mon1/keyring auth get client.admin
[2021-12-11 19:00:36,418][mon1][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-mon1/keyring auth get client.bootstrap-mds
[2021-12-11 19:00:36,586][mon1][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-mon1/keyring auth get client.bootstrap-mgr
[2021-12-11 19:00:36,753][mon1][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-mon1/keyring auth get client.bootstrap-osd
[2021-12-11 19:00:36,919][mon1][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-mon1/keyring auth get client.bootstrap-rgw
[2021-12-11 19:00:37,085][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.client.admin.keyring' already exists
[2021-12-11 19:00:37,085][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.bootstrap-mds.keyring' already exists
[2021-12-11 19:00:37,086][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.bootstrap-mgr.keyring' already exists
[2021-12-11 19:00:37,086][ceph_deploy.gatherkeys][INFO  ] Replacing 'ceph.mon.keyring' and backing up old key as 'ceph.mon.keyring-20211211190037'
[2021-12-11 19:00:37,087][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.bootstrap-osd.keyring' already exists
[2021-12-11 19:00:37,087][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.bootstrap-rgw.keyring' already exists
[2021-12-11 19:00:37,088][ceph_deploy.gatherkeys][INFO  ] Destroy temp directory /tmp/tmp5PO2hh
[2021-12-11 19:00:44,539][ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephuser/.cephdeploy.conf
[2021-12-11 19:00:44,539][ceph_deploy.cli][INFO  ] Invoked (1.5.39): /usr/bin/ceph-deploy gatherkeys mon1
[2021-12-11 19:00:44,539][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2021-12-11 19:00:44,539][ceph_deploy.cli][INFO  ]  username                      : None
[2021-12-11 19:00:44,539][ceph_deploy.cli][INFO  ]  verbose                       : False
[2021-12-11 19:00:44,539][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2021-12-11 19:00:44,540][ceph_deploy.cli][INFO  ]  quiet                         : False
[2021-12-11 19:00:44,540][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f0e08037878>
[2021-12-11 19:00:44,540][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2021-12-11 19:00:44,540][ceph_deploy.cli][INFO  ]  mon                           : ['mon1']
[2021-12-11 19:00:44,540][ceph_deploy.cli][INFO  ]  func                          : <function gatherkeys at 0x7f0e07ffb230>
[2021-12-11 19:00:44,540][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2021-12-11 19:00:44,540][ceph_deploy.cli][INFO  ]  default_release               : False
[2021-12-11 19:00:44,540][ceph_deploy.gatherkeys][INFO  ] Storing keys in temp directory /tmp/tmpI6CSMA
[2021-12-11 19:00:44,704][mon1][DEBUG ] connection detected need for sudo
[2021-12-11 19:00:44,866][mon1][DEBUG ] connected to host: mon1 
[2021-12-11 19:00:44,866][mon1][DEBUG ] detect platform information from remote host
[2021-12-11 19:00:44,883][mon1][DEBUG ] detect machine type
[2021-12-11 19:00:44,888][mon1][DEBUG ] get remote short hostname
[2021-12-11 19:00:44,889][mon1][DEBUG ] fetch remote file
[2021-12-11 19:00:44,892][mon1][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --admin-daemon=/var/run/ceph/ceph-mon.mon1.asok mon_status
[2021-12-11 19:00:44,960][mon1][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-mon1/keyring auth get client.admin
[2021-12-11 19:00:45,127][mon1][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-mon1/keyring auth get client.bootstrap-mds
[2021-12-11 19:00:45,294][mon1][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-mon1/keyring auth get client.bootstrap-mgr
[2021-12-11 19:00:45,461][mon1][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-mon1/keyring auth get client.bootstrap-osd
[2021-12-11 19:00:45,628][mon1][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-mon1/keyring auth get client.bootstrap-rgw
[2021-12-11 19:00:45,793][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.client.admin.keyring' already exists
[2021-12-11 19:00:45,794][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.bootstrap-mds.keyring' already exists
[2021-12-11 19:00:45,794][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.bootstrap-mgr.keyring' already exists
[2021-12-11 19:00:45,794][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.mon.keyring' already exists
[2021-12-11 19:00:45,794][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.bootstrap-osd.keyring' already exists
[2021-12-11 19:00:45,794][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.bootstrap-rgw.keyring' already exists
[2021-12-11 19:00:45,794][ceph_deploy.gatherkeys][INFO  ] Destroy temp directory /tmp/tmpI6CSMA
[2021-12-11 19:01:13,609][ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephuser/.cephdeploy.conf
[2021-12-11 19:01:13,610][ceph_deploy.cli][INFO  ] Invoked (1.5.39): /usr/bin/ceph-deploy admin ceph-admin mon1 osd1 osd2 osd3 osd4 osd5 client client2 client3
[2021-12-11 19:01:13,610][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2021-12-11 19:01:13,610][ceph_deploy.cli][INFO  ]  username                      : None
[2021-12-11 19:01:13,610][ceph_deploy.cli][INFO  ]  verbose                       : False
[2021-12-11 19:01:13,610][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2021-12-11 19:01:13,610][ceph_deploy.cli][INFO  ]  quiet                         : False
[2021-12-11 19:01:13,610][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f1fb826db48>
[2021-12-11 19:01:13,610][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2021-12-11 19:01:13,610][ceph_deploy.cli][INFO  ]  client                        : ['ceph-admin', 'mon1', 'osd1', 'osd2', 'osd3', 'osd4', 'osd5', 'client', 'client2', 'client3']
[2021-12-11 19:01:13,610][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7f1fb8f86a28>
[2021-12-11 19:01:13,610][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2021-12-11 19:01:13,610][ceph_deploy.cli][INFO  ]  default_release               : False
[2021-12-11 19:01:13,610][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to ceph-admin
[2021-12-11 19:01:13,644][ceph-admin][DEBUG ] connection detected need for sudo
[2021-12-11 19:01:13,676][ceph-admin][DEBUG ] connected to host: ceph-admin 
[2021-12-11 19:01:13,677][ceph-admin][DEBUG ] detect platform information from remote host
[2021-12-11 19:01:13,694][ceph-admin][DEBUG ] detect machine type
[2021-12-11 19:01:13,698][ceph-admin][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2021-12-11 19:01:13,699][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to mon1
[2021-12-11 19:01:13,848][mon1][DEBUG ] connection detected need for sudo
[2021-12-11 19:01:14,011][mon1][DEBUG ] connected to host: mon1 
[2021-12-11 19:01:14,011][mon1][DEBUG ] detect platform information from remote host
[2021-12-11 19:01:14,028][mon1][DEBUG ] detect machine type
[2021-12-11 19:01:14,032][mon1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2021-12-11 19:01:14,035][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to osd1
[2021-12-11 19:01:14,188][osd1][DEBUG ] connection detected need for sudo
[2021-12-11 19:01:14,358][osd1][DEBUG ] connected to host: osd1 
[2021-12-11 19:01:14,358][osd1][DEBUG ] detect platform information from remote host
[2021-12-11 19:01:14,377][osd1][DEBUG ] detect machine type
[2021-12-11 19:01:14,381][osd1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2021-12-11 19:01:14,384][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to osd2
[2021-12-11 19:01:14,538][osd2][DEBUG ] connection detected need for sudo
[2021-12-11 19:01:14,703][osd2][DEBUG ] connected to host: osd2 
[2021-12-11 19:01:14,704][osd2][DEBUG ] detect platform information from remote host
[2021-12-11 19:01:14,720][osd2][DEBUG ] detect machine type
[2021-12-11 19:01:14,725][osd2][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2021-12-11 19:01:14,728][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to osd3
[2021-12-11 19:01:14,884][osd3][DEBUG ] connection detected need for sudo
[2021-12-11 19:01:15,057][osd3][DEBUG ] connected to host: osd3 
[2021-12-11 19:01:15,058][osd3][DEBUG ] detect platform information from remote host
[2021-12-11 19:01:15,076][osd3][DEBUG ] detect machine type
[2021-12-11 19:01:15,081][osd3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2021-12-11 19:01:15,084][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to osd4
[2021-12-11 19:01:15,242][osd4][DEBUG ] connection detected need for sudo
[2021-12-11 19:01:15,413][osd4][DEBUG ] connected to host: osd4 
[2021-12-11 19:01:15,413][osd4][DEBUG ] detect platform information from remote host
[2021-12-11 19:01:15,431][osd4][DEBUG ] detect machine type
[2021-12-11 19:01:15,436][osd4][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2021-12-11 19:01:15,439][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to osd5
[2021-12-11 19:01:15,603][osd5][DEBUG ] connection detected need for sudo
[2021-12-11 19:01:15,824][osd5][DEBUG ] connected to host: osd5 
[2021-12-11 19:01:15,824][osd5][DEBUG ] detect platform information from remote host
[2021-12-11 19:01:15,843][osd5][DEBUG ] detect machine type
[2021-12-11 19:01:15,847][osd5][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2021-12-11 19:01:15,849][ceph_deploy.admin][ERROR ] RuntimeError: config file /etc/ceph/ceph.conf exists with different content; use --overwrite-conf to overwrite
[2021-12-11 19:01:15,849][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to client
[2021-12-11 19:01:16,006][client][DEBUG ] connection detected need for sudo
[2021-12-11 19:01:16,181][client][DEBUG ] connected to host: client 
[2021-12-11 19:01:16,181][client][DEBUG ] detect platform information from remote host
[2021-12-11 19:01:16,199][client][DEBUG ] detect machine type
[2021-12-11 19:01:16,204][client][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2021-12-11 19:01:16,207][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to client2
[2021-12-11 19:01:16,478][client2][DEBUG ] connection detected need for sudo
[2021-12-11 19:01:16,712][client2][DEBUG ] connected to host: client2 
[2021-12-11 19:01:16,712][client2][DEBUG ] detect platform information from remote host
[2021-12-11 19:01:16,745][client2][DEBUG ] detect machine type
[2021-12-11 19:01:16,750][client2][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2021-12-11 19:01:16,752][ceph_deploy.admin][ERROR ] RuntimeError: config file /etc/ceph/ceph.conf exists with different content; use --overwrite-conf to overwrite
[2021-12-11 19:01:16,752][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to client3
[2021-12-11 19:01:16,941][client3][DEBUG ] connection detected need for sudo
[2021-12-11 19:01:17,176][client3][DEBUG ] connected to host: client3 
[2021-12-11 19:01:17,176][client3][DEBUG ] detect platform information from remote host
[2021-12-11 19:01:17,209][client3][DEBUG ] detect machine type
[2021-12-11 19:01:17,215][client3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2021-12-11 19:01:17,216][ceph_deploy.admin][ERROR ] RuntimeError: config file /etc/ceph/ceph.conf exists with different content; use --overwrite-conf to overwrite
[2021-12-11 19:01:17,216][ceph_deploy][ERROR ] GenericError: Failed to configure 3 admin hosts

[2021-12-11 19:01:25,394][ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephuser/.cephdeploy.conf
[2021-12-11 19:01:25,395][ceph_deploy.cli][INFO  ] Invoked (1.5.39): /usr/bin/ceph-deploy --overwrite-conf admin ceph-admin mon1 osd1 osd2 osd3 osd4 osd5 client client2 client3
[2021-12-11 19:01:25,395][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2021-12-11 19:01:25,395][ceph_deploy.cli][INFO  ]  username                      : None
[2021-12-11 19:01:25,395][ceph_deploy.cli][INFO  ]  verbose                       : False
[2021-12-11 19:01:25,395][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2021-12-11 19:01:25,395][ceph_deploy.cli][INFO  ]  quiet                         : False
[2021-12-11 19:01:25,395][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f0b409d4b48>
[2021-12-11 19:01:25,395][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2021-12-11 19:01:25,396][ceph_deploy.cli][INFO  ]  client                        : ['ceph-admin', 'mon1', 'osd1', 'osd2', 'osd3', 'osd4', 'osd5', 'client', 'client2', 'client3']
[2021-12-11 19:01:25,396][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7f0b416eda28>
[2021-12-11 19:01:25,396][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2021-12-11 19:01:25,396][ceph_deploy.cli][INFO  ]  default_release               : False
[2021-12-11 19:01:25,396][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to ceph-admin
[2021-12-11 19:01:25,426][ceph-admin][DEBUG ] connection detected need for sudo
[2021-12-11 19:01:25,451][ceph-admin][DEBUG ] connected to host: ceph-admin 
[2021-12-11 19:01:25,452][ceph-admin][DEBUG ] detect platform information from remote host
[2021-12-11 19:01:25,469][ceph-admin][DEBUG ] detect machine type
[2021-12-11 19:01:25,473][ceph-admin][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2021-12-11 19:01:25,474][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to mon1
[2021-12-11 19:01:25,623][mon1][DEBUG ] connection detected need for sudo
[2021-12-11 19:01:25,784][mon1][DEBUG ] connected to host: mon1 
[2021-12-11 19:01:25,784][mon1][DEBUG ] detect platform information from remote host
[2021-12-11 19:01:25,801][mon1][DEBUG ] detect machine type
[2021-12-11 19:01:25,805][mon1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2021-12-11 19:01:25,808][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to osd1
[2021-12-11 19:01:25,958][osd1][DEBUG ] connection detected need for sudo
[2021-12-11 19:01:26,126][osd1][DEBUG ] connected to host: osd1 
[2021-12-11 19:01:26,126][osd1][DEBUG ] detect platform information from remote host
[2021-12-11 19:01:26,144][osd1][DEBUG ] detect machine type
[2021-12-11 19:01:26,149][osd1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2021-12-11 19:01:26,151][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to osd2
[2021-12-11 19:01:26,305][osd2][DEBUG ] connection detected need for sudo
[2021-12-11 19:01:26,474][osd2][DEBUG ] connected to host: osd2 
[2021-12-11 19:01:26,474][osd2][DEBUG ] detect platform information from remote host
[2021-12-11 19:01:26,491][osd2][DEBUG ] detect machine type
[2021-12-11 19:01:26,496][osd2][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2021-12-11 19:01:26,499][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to osd3
[2021-12-11 19:01:26,653][osd3][DEBUG ] connection detected need for sudo
[2021-12-11 19:01:26,823][osd3][DEBUG ] connected to host: osd3 
[2021-12-11 19:01:26,824][osd3][DEBUG ] detect platform information from remote host
[2021-12-11 19:01:26,840][osd3][DEBUG ] detect machine type
[2021-12-11 19:01:26,845][osd3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2021-12-11 19:01:26,848][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to osd4
[2021-12-11 19:01:27,005][osd4][DEBUG ] connection detected need for sudo
[2021-12-11 19:01:27,174][osd4][DEBUG ] connected to host: osd4 
[2021-12-11 19:01:27,174][osd4][DEBUG ] detect platform information from remote host
[2021-12-11 19:01:27,191][osd4][DEBUG ] detect machine type
[2021-12-11 19:01:27,196][osd4][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2021-12-11 19:01:27,199][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to osd5
[2021-12-11 19:01:27,351][osd5][DEBUG ] connection detected need for sudo
[2021-12-11 19:01:27,519][osd5][DEBUG ] connected to host: osd5 
[2021-12-11 19:01:27,519][osd5][DEBUG ] detect platform information from remote host
[2021-12-11 19:01:27,535][osd5][DEBUG ] detect machine type
[2021-12-11 19:01:27,540][osd5][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2021-12-11 19:01:27,542][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to client
[2021-12-11 19:01:27,696][client][DEBUG ] connection detected need for sudo
[2021-12-11 19:01:27,865][client][DEBUG ] connected to host: client 
[2021-12-11 19:01:27,865][client][DEBUG ] detect platform information from remote host
[2021-12-11 19:01:27,882][client][DEBUG ] detect machine type
[2021-12-11 19:01:27,887][client][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2021-12-11 19:01:27,890][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to client2
[2021-12-11 19:01:28,042][client2][DEBUG ] connection detected need for sudo
[2021-12-11 19:01:28,206][client2][DEBUG ] connected to host: client2 
[2021-12-11 19:01:28,207][client2][DEBUG ] detect platform information from remote host
[2021-12-11 19:01:28,223][client2][DEBUG ] detect machine type
[2021-12-11 19:01:28,227][client2][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2021-12-11 19:01:28,230][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to client3
[2021-12-11 19:01:28,379][client3][DEBUG ] connection detected need for sudo
[2021-12-11 19:01:28,544][client3][DEBUG ] connected to host: client3 
[2021-12-11 19:01:28,545][client3][DEBUG ] detect platform information from remote host
[2021-12-11 19:01:28,563][client3][DEBUG ] detect machine type
[2021-12-11 19:01:28,568][client3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2021-12-11 19:29:25,074][ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephuser/.cephdeploy.conf
[2021-12-11 19:29:25,075][ceph_deploy.cli][INFO  ] Invoked (1.5.39): /usr/bin/ceph-deploy new mon1
[2021-12-11 19:29:25,075][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2021-12-11 19:29:25,075][ceph_deploy.cli][INFO  ]  username                      : None
[2021-12-11 19:29:25,075][ceph_deploy.cli][INFO  ]  func                          : <function new at 0x7ff317464668>
[2021-12-11 19:29:25,075][ceph_deploy.cli][INFO  ]  verbose                       : False
[2021-12-11 19:29:25,075][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2021-12-11 19:29:25,075][ceph_deploy.cli][INFO  ]  quiet                         : False
[2021-12-11 19:29:25,075][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7ff316bd6f38>
[2021-12-11 19:29:25,075][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2021-12-11 19:29:25,076][ceph_deploy.cli][INFO  ]  ssh_copykey                   : True
[2021-12-11 19:29:25,076][ceph_deploy.cli][INFO  ]  mon                           : ['mon1']
[2021-12-11 19:29:25,076][ceph_deploy.cli][INFO  ]  public_network                : None
[2021-12-11 19:29:25,076][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2021-12-11 19:29:25,076][ceph_deploy.cli][INFO  ]  cluster_network               : None
[2021-12-11 19:29:25,076][ceph_deploy.cli][INFO  ]  default_release               : False
[2021-12-11 19:29:25,076][ceph_deploy.cli][INFO  ]  fsid                          : None
[2021-12-11 19:29:25,076][ceph_deploy.new][DEBUG ] Creating new cluster named ceph
[2021-12-11 19:29:25,076][ceph_deploy.new][INFO  ] making sure passwordless SSH succeeds
[2021-12-11 19:29:25,104][mon1][DEBUG ] connected to host: ceph-admin 
[2021-12-11 19:29:25,110][mon1][INFO  ] Running command: ssh -CT -o BatchMode=yes mon1
[2021-12-11 19:29:25,439][mon1][DEBUG ] connection detected need for sudo
[2021-12-11 19:29:25,610][mon1][DEBUG ] connected to host: mon1 
[2021-12-11 19:29:25,610][mon1][DEBUG ] detect platform information from remote host
[2021-12-11 19:29:25,627][mon1][DEBUG ] detect machine type
[2021-12-11 19:29:25,632][mon1][DEBUG ] find the location of an executable
[2021-12-11 19:29:25,634][mon1][INFO  ] Running command: sudo /usr/sbin/ip link show
[2021-12-11 19:29:25,644][mon1][INFO  ] Running command: sudo /usr/sbin/ip addr show
[2021-12-11 19:29:25,653][mon1][DEBUG ] IP addresses found: [u'192.168.1.93']
[2021-12-11 19:29:25,654][ceph_deploy.new][DEBUG ] Resolving host mon1
[2021-12-11 19:29:25,654][ceph_deploy.new][DEBUG ] Monitor mon1 at 192.168.1.93
[2021-12-11 19:29:25,654][ceph_deploy.new][DEBUG ] Monitor initial members are ['mon1']
[2021-12-11 19:29:25,654][ceph_deploy.new][DEBUG ] Monitor addrs are ['192.168.1.93']
[2021-12-11 19:29:25,654][ceph_deploy.new][DEBUG ] Creating a random mon key...
[2021-12-11 19:29:25,654][ceph_deploy.new][DEBUG ] Writing monitor keyring to ceph.mon.keyring...
[2021-12-11 19:29:25,654][ceph_deploy.new][DEBUG ] Writing initial config to ceph.conf...
[2021-12-11 19:29:40,474][ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephuser/.cephdeploy.conf
[2021-12-11 19:29:40,474][ceph_deploy.cli][INFO  ] Invoked (1.5.39): /usr/bin/ceph-deploy mon create-initial
[2021-12-11 19:29:40,474][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2021-12-11 19:29:40,474][ceph_deploy.cli][INFO  ]  username                      : None
[2021-12-11 19:29:40,474][ceph_deploy.cli][INFO  ]  verbose                       : False
[2021-12-11 19:29:40,474][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2021-12-11 19:29:40,475][ceph_deploy.cli][INFO  ]  subcommand                    : create-initial
[2021-12-11 19:29:40,475][ceph_deploy.cli][INFO  ]  quiet                         : False
[2021-12-11 19:29:40,475][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f8e01b2ef38>
[2021-12-11 19:29:40,475][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2021-12-11 19:29:40,475][ceph_deploy.cli][INFO  ]  func                          : <function mon at 0x7f8e01b15758>
[2021-12-11 19:29:40,475][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2021-12-11 19:29:40,475][ceph_deploy.cli][INFO  ]  default_release               : False
[2021-12-11 19:29:40,475][ceph_deploy.cli][INFO  ]  keyrings                      : None
[2021-12-11 19:29:40,476][ceph_deploy.mon][DEBUG ] Deploying mon, cluster ceph hosts mon1
[2021-12-11 19:29:40,476][ceph_deploy.mon][DEBUG ] detecting platform for host mon1 ...
[2021-12-11 19:29:40,642][mon1][DEBUG ] connection detected need for sudo
[2021-12-11 19:29:40,805][mon1][DEBUG ] connected to host: mon1 
[2021-12-11 19:29:40,806][mon1][DEBUG ] detect platform information from remote host
[2021-12-11 19:29:40,822][mon1][DEBUG ] detect machine type
[2021-12-11 19:29:40,827][mon1][DEBUG ] find the location of an executable
[2021-12-11 19:29:40,828][ceph_deploy.mon][INFO  ] distro info: CentOS Linux 7.9.2009 Core
[2021-12-11 19:29:40,828][mon1][DEBUG ] determining if provided host has same hostname in remote
[2021-12-11 19:29:40,828][mon1][DEBUG ] get remote short hostname
[2021-12-11 19:29:40,830][mon1][DEBUG ] deploying mon to mon1
[2021-12-11 19:29:40,830][mon1][DEBUG ] get remote short hostname
[2021-12-11 19:29:40,831][mon1][DEBUG ] remote hostname: mon1
[2021-12-11 19:29:40,832][mon1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2021-12-11 19:29:40,834][ceph_deploy.mon][ERROR ] RuntimeError: config file /etc/ceph/ceph.conf exists with different content; use --overwrite-conf to overwrite
[2021-12-11 19:29:40,834][ceph_deploy][ERROR ] GenericError: Failed to create 1 monitors

[2021-12-11 19:29:49,527][ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephuser/.cephdeploy.conf
[2021-12-11 19:29:49,528][ceph_deploy.cli][INFO  ] Invoked (1.5.39): /usr/bin/ceph-deploy --overwrite-conf mon create-initial
[2021-12-11 19:29:49,528][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2021-12-11 19:29:49,528][ceph_deploy.cli][INFO  ]  username                      : None
[2021-12-11 19:29:49,528][ceph_deploy.cli][INFO  ]  verbose                       : False
[2021-12-11 19:29:49,528][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2021-12-11 19:29:49,528][ceph_deploy.cli][INFO  ]  subcommand                    : create-initial
[2021-12-11 19:29:49,528][ceph_deploy.cli][INFO  ]  quiet                         : False
[2021-12-11 19:29:49,528][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7ff3e6ecbf38>
[2021-12-11 19:29:49,528][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2021-12-11 19:29:49,528][ceph_deploy.cli][INFO  ]  func                          : <function mon at 0x7ff3e6eb2758>
[2021-12-11 19:29:49,528][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2021-12-11 19:29:49,528][ceph_deploy.cli][INFO  ]  default_release               : False
[2021-12-11 19:29:49,528][ceph_deploy.cli][INFO  ]  keyrings                      : None
[2021-12-11 19:29:49,529][ceph_deploy.mon][DEBUG ] Deploying mon, cluster ceph hosts mon1
[2021-12-11 19:29:49,529][ceph_deploy.mon][DEBUG ] detecting platform for host mon1 ...
[2021-12-11 19:29:49,698][mon1][DEBUG ] connection detected need for sudo
[2021-12-11 19:29:49,867][mon1][DEBUG ] connected to host: mon1 
[2021-12-11 19:29:49,867][mon1][DEBUG ] detect platform information from remote host
[2021-12-11 19:29:49,884][mon1][DEBUG ] detect machine type
[2021-12-11 19:29:49,889][mon1][DEBUG ] find the location of an executable
[2021-12-11 19:29:49,890][ceph_deploy.mon][INFO  ] distro info: CentOS Linux 7.9.2009 Core
[2021-12-11 19:29:49,890][mon1][DEBUG ] determining if provided host has same hostname in remote
[2021-12-11 19:29:49,890][mon1][DEBUG ] get remote short hostname
[2021-12-11 19:29:49,892][mon1][DEBUG ] deploying mon to mon1
[2021-12-11 19:29:49,892][mon1][DEBUG ] get remote short hostname
[2021-12-11 19:29:49,893][mon1][DEBUG ] remote hostname: mon1
[2021-12-11 19:29:49,895][mon1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2021-12-11 19:29:49,897][mon1][DEBUG ] create the mon path if it does not exist
[2021-12-11 19:29:49,898][mon1][DEBUG ] checking for done path: /var/lib/ceph/mon/ceph-mon1/done
[2021-12-11 19:29:49,899][mon1][DEBUG ] create a done file to avoid re-doing the mon deployment
[2021-12-11 19:29:49,900][mon1][DEBUG ] create the init path if it does not exist
[2021-12-11 19:29:49,903][mon1][INFO  ] Running command: sudo systemctl enable ceph.target
[2021-12-11 19:29:49,973][mon1][INFO  ] Running command: sudo systemctl enable ceph-mon@mon1
[2021-12-11 19:29:50,041][mon1][INFO  ] Running command: sudo systemctl start ceph-mon@mon1
[2021-12-11 19:29:52,056][mon1][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.mon1.asok mon_status
[2021-12-11 19:29:52,120][mon1][DEBUG ] ********************************************************************************
[2021-12-11 19:29:52,121][mon1][DEBUG ] status for monitor: mon.mon1
[2021-12-11 19:29:52,121][mon1][DEBUG ] {
[2021-12-11 19:29:52,121][mon1][DEBUG ]   "election_epoch": 16, 
[2021-12-11 19:29:52,121][mon1][DEBUG ]   "extra_probe_peers": [], 
[2021-12-11 19:29:52,121][mon1][DEBUG ]   "monmap": {
[2021-12-11 19:29:52,121][mon1][DEBUG ]     "created": "2021-11-03 20:50:42.718657", 
[2021-12-11 19:29:52,121][mon1][DEBUG ]     "epoch": 1, 
[2021-12-11 19:29:52,121][mon1][DEBUG ]     "fsid": "67d9e224-a6f7-43d3-ae36-e6100c59258e", 
[2021-12-11 19:29:52,121][mon1][DEBUG ]     "modified": "2021-11-03 20:50:42.718657", 
[2021-12-11 19:29:52,121][mon1][DEBUG ]     "mons": [
[2021-12-11 19:29:52,121][mon1][DEBUG ]       {
[2021-12-11 19:29:52,121][mon1][DEBUG ]         "addr": "192.168.1.93:6789/0", 
[2021-12-11 19:29:52,121][mon1][DEBUG ]         "name": "mon1", 
[2021-12-11 19:29:52,122][mon1][DEBUG ]         "rank": 0
[2021-12-11 19:29:52,122][mon1][DEBUG ]       }
[2021-12-11 19:29:52,122][mon1][DEBUG ]     ]
[2021-12-11 19:29:52,122][mon1][DEBUG ]   }, 
[2021-12-11 19:29:52,122][mon1][DEBUG ]   "name": "mon1", 
[2021-12-11 19:29:52,122][mon1][DEBUG ]   "outside_quorum": [], 
[2021-12-11 19:29:52,122][mon1][DEBUG ]   "quorum": [
[2021-12-11 19:29:52,122][mon1][DEBUG ]     0
[2021-12-11 19:29:52,122][mon1][DEBUG ]   ], 
[2021-12-11 19:29:52,122][mon1][DEBUG ]   "rank": 0, 
[2021-12-11 19:29:52,122][mon1][DEBUG ]   "state": "leader", 
[2021-12-11 19:29:52,122][mon1][DEBUG ]   "sync_provider": []
[2021-12-11 19:29:52,122][mon1][DEBUG ] }
[2021-12-11 19:29:52,122][mon1][DEBUG ] ********************************************************************************
[2021-12-11 19:29:52,122][mon1][INFO  ] monitor: mon.mon1 is running
[2021-12-11 19:29:52,124][mon1][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.mon1.asok mon_status
[2021-12-11 19:29:52,239][ceph_deploy.mon][INFO  ] processing monitor mon.mon1
[2021-12-11 19:29:52,415][mon1][DEBUG ] connection detected need for sudo
[2021-12-11 19:29:52,582][mon1][DEBUG ] connected to host: mon1 
[2021-12-11 19:29:52,583][mon1][DEBUG ] detect platform information from remote host
[2021-12-11 19:29:52,600][mon1][DEBUG ] detect machine type
[2021-12-11 19:29:52,605][mon1][DEBUG ] find the location of an executable
[2021-12-11 19:29:52,607][mon1][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.mon1.asok mon_status
[2021-12-11 19:29:52,672][ceph_deploy.mon][INFO  ] mon.mon1 monitor has reached quorum!
[2021-12-11 19:29:52,672][ceph_deploy.mon][INFO  ] all initial monitors are running and have formed quorum
[2021-12-11 19:29:52,672][ceph_deploy.mon][INFO  ] Running gatherkeys...
[2021-12-11 19:29:52,673][ceph_deploy.gatherkeys][INFO  ] Storing keys in temp directory /tmp/tmpmcwirS
[2021-12-11 19:29:52,837][mon1][DEBUG ] connection detected need for sudo
[2021-12-11 19:29:53,006][mon1][DEBUG ] connected to host: mon1 
[2021-12-11 19:29:53,007][mon1][DEBUG ] detect platform information from remote host
[2021-12-11 19:29:53,024][mon1][DEBUG ] detect machine type
[2021-12-11 19:29:53,029][mon1][DEBUG ] get remote short hostname
[2021-12-11 19:29:53,030][mon1][DEBUG ] fetch remote file
[2021-12-11 19:29:53,033][mon1][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --admin-daemon=/var/run/ceph/ceph-mon.mon1.asok mon_status
[2021-12-11 19:29:53,099][mon1][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-mon1/keyring auth get client.admin
[2021-12-11 19:29:53,267][mon1][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-mon1/keyring auth get client.bootstrap-mds
[2021-12-11 19:29:53,434][mon1][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-mon1/keyring auth get client.bootstrap-mgr
[2021-12-11 19:29:53,601][mon1][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-mon1/keyring auth get client.bootstrap-osd
[2021-12-11 19:29:53,768][mon1][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-mon1/keyring auth get client.bootstrap-rgw
[2021-12-11 19:29:53,934][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.client.admin.keyring' already exists
[2021-12-11 19:29:53,934][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.bootstrap-mds.keyring' already exists
[2021-12-11 19:29:53,934][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.bootstrap-mgr.keyring' already exists
[2021-12-11 19:29:53,934][ceph_deploy.gatherkeys][INFO  ] Replacing 'ceph.mon.keyring' and backing up old key as 'ceph.mon.keyring-20211211192953'
[2021-12-11 19:29:53,934][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.bootstrap-osd.keyring' already exists
[2021-12-11 19:29:53,934][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.bootstrap-rgw.keyring' already exists
[2021-12-11 19:29:53,935][ceph_deploy.gatherkeys][INFO  ] Destroy temp directory /tmp/tmpmcwirS
[2021-12-11 19:29:57,763][ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephuser/.cephdeploy.conf
[2021-12-11 19:29:57,763][ceph_deploy.cli][INFO  ] Invoked (1.5.39): /usr/bin/ceph-deploy gatherkeys mon1
[2021-12-11 19:29:57,763][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2021-12-11 19:29:57,763][ceph_deploy.cli][INFO  ]  username                      : None
[2021-12-11 19:29:57,763][ceph_deploy.cli][INFO  ]  verbose                       : False
[2021-12-11 19:29:57,763][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2021-12-11 19:29:57,764][ceph_deploy.cli][INFO  ]  quiet                         : False
[2021-12-11 19:29:57,764][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f1f5a235878>
[2021-12-11 19:29:57,764][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2021-12-11 19:29:57,764][ceph_deploy.cli][INFO  ]  mon                           : ['mon1']
[2021-12-11 19:29:57,764][ceph_deploy.cli][INFO  ]  func                          : <function gatherkeys at 0x7f1f5a1f9230>
[2021-12-11 19:29:57,764][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2021-12-11 19:29:57,764][ceph_deploy.cli][INFO  ]  default_release               : False
[2021-12-11 19:29:57,764][ceph_deploy.gatherkeys][INFO  ] Storing keys in temp directory /tmp/tmpyujQZf
[2021-12-11 19:29:57,930][mon1][DEBUG ] connection detected need for sudo
[2021-12-11 19:29:58,096][mon1][DEBUG ] connected to host: mon1 
[2021-12-11 19:29:58,096][mon1][DEBUG ] detect platform information from remote host
[2021-12-11 19:29:58,112][mon1][DEBUG ] detect machine type
[2021-12-11 19:29:58,117][mon1][DEBUG ] get remote short hostname
[2021-12-11 19:29:58,118][mon1][DEBUG ] fetch remote file
[2021-12-11 19:29:58,121][mon1][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --admin-daemon=/var/run/ceph/ceph-mon.mon1.asok mon_status
[2021-12-11 19:29:58,190][mon1][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-mon1/keyring auth get client.admin
[2021-12-11 19:29:58,356][mon1][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-mon1/keyring auth get client.bootstrap-mds
[2021-12-11 19:29:58,523][mon1][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-mon1/keyring auth get client.bootstrap-mgr
[2021-12-11 19:29:58,691][mon1][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-mon1/keyring auth get client.bootstrap-osd
[2021-12-11 19:29:58,858][mon1][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-mon1/keyring auth get client.bootstrap-rgw
[2021-12-11 19:29:59,023][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.client.admin.keyring' already exists
[2021-12-11 19:29:59,023][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.bootstrap-mds.keyring' already exists
[2021-12-11 19:29:59,023][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.bootstrap-mgr.keyring' already exists
[2021-12-11 19:29:59,024][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.mon.keyring' already exists
[2021-12-11 19:29:59,024][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.bootstrap-osd.keyring' already exists
[2021-12-11 19:29:59,024][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.bootstrap-rgw.keyring' already exists
[2021-12-11 19:29:59,024][ceph_deploy.gatherkeys][INFO  ] Destroy temp directory /tmp/tmpyujQZf
[2021-12-11 19:30:13,297][ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephuser/.cephdeploy.conf
[2021-12-11 19:30:13,298][ceph_deploy.cli][INFO  ] Invoked (1.5.39): /usr/bin/ceph-deploy disk list osd1
[2021-12-11 19:30:13,298][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2021-12-11 19:30:13,298][ceph_deploy.cli][INFO  ]  username                      : None
[2021-12-11 19:30:13,298][ceph_deploy.cli][INFO  ]  verbose                       : False
[2021-12-11 19:30:13,298][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2021-12-11 19:30:13,298][ceph_deploy.cli][INFO  ]  subcommand                    : list
[2021-12-11 19:30:13,298][ceph_deploy.cli][INFO  ]  quiet                         : False
[2021-12-11 19:30:13,298][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7ff69dc29ef0>
[2021-12-11 19:30:13,298][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2021-12-11 19:30:13,298][ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7ff69dc050c8>
[2021-12-11 19:30:13,298][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2021-12-11 19:30:13,299][ceph_deploy.cli][INFO  ]  default_release               : False
[2021-12-11 19:30:13,299][ceph_deploy.cli][INFO  ]  disk                          : [('osd1', None, None)]
[2021-12-11 19:30:13,494][osd1][DEBUG ] connection detected need for sudo
[2021-12-11 19:30:13,669][osd1][DEBUG ] connected to host: osd1 
[2021-12-11 19:30:13,670][osd1][DEBUG ] detect platform information from remote host
[2021-12-11 19:30:13,689][osd1][DEBUG ] detect machine type
[2021-12-11 19:30:13,694][osd1][DEBUG ] find the location of an executable
[2021-12-11 19:30:13,696][ceph_deploy.osd][INFO  ] Distro info: CentOS Linux 7.9.2009 Core
[2021-12-11 19:30:13,696][ceph_deploy.osd][DEBUG ] Listing disks on osd1...
[2021-12-11 19:30:13,696][osd1][DEBUG ] find the location of an executable
[2021-12-11 19:30:13,708][osd1][INFO  ] Running command: sudo /usr/sbin/ceph-disk list
[2021-12-11 19:30:13,977][osd1][DEBUG ] /dev/dm-0 other, xfs, mounted on /
[2021-12-11 19:30:13,977][osd1][DEBUG ] /dev/dm-1 swap, swap
[2021-12-11 19:30:13,977][osd1][DEBUG ] /dev/sda :
[2021-12-11 19:30:13,977][osd1][DEBUG ]  /dev/sda2 other, LVM2_member
[2021-12-11 19:30:13,978][osd1][DEBUG ]  /dev/sda1 other, xfs, mounted on /boot
[2021-12-11 19:30:13,978][osd1][DEBUG ] /dev/sdb other, unknown
[2021-12-11 19:30:13,978][osd1][DEBUG ] /dev/sr0 other, unknown
[2021-12-11 19:30:25,579][ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephuser/.cephdeploy.conf
[2021-12-11 19:30:25,579][ceph_deploy.cli][INFO  ] Invoked (1.5.39): /usr/bin/ceph-deploy disk list osd2
[2021-12-11 19:30:25,579][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2021-12-11 19:30:25,579][ceph_deploy.cli][INFO  ]  username                      : None
[2021-12-11 19:30:25,579][ceph_deploy.cli][INFO  ]  verbose                       : False
[2021-12-11 19:30:25,579][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2021-12-11 19:30:25,579][ceph_deploy.cli][INFO  ]  subcommand                    : list
[2021-12-11 19:30:25,579][ceph_deploy.cli][INFO  ]  quiet                         : False
[2021-12-11 19:30:25,579][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f590e305ef0>
[2021-12-11 19:30:25,579][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2021-12-11 19:30:25,579][ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7f590e2e10c8>
[2021-12-11 19:30:25,579][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2021-12-11 19:30:25,579][ceph_deploy.cli][INFO  ]  default_release               : False
[2021-12-11 19:30:25,580][ceph_deploy.cli][INFO  ]  disk                          : [('osd2', None, None)]
[2021-12-11 19:30:25,836][osd2][DEBUG ] connection detected need for sudo
[2021-12-11 19:30:26,016][osd2][DEBUG ] connected to host: osd2 
[2021-12-11 19:30:26,017][osd2][DEBUG ] detect platform information from remote host
[2021-12-11 19:30:26,036][osd2][DEBUG ] detect machine type
[2021-12-11 19:30:26,041][osd2][DEBUG ] find the location of an executable
[2021-12-11 19:30:26,043][ceph_deploy.osd][INFO  ] Distro info: CentOS Linux 7.9.2009 Core
[2021-12-11 19:30:26,043][ceph_deploy.osd][DEBUG ] Listing disks on osd2...
[2021-12-11 19:30:26,043][osd2][DEBUG ] find the location of an executable
[2021-12-11 19:30:26,046][osd2][INFO  ] Running command: sudo /usr/sbin/ceph-disk list
[2021-12-11 19:30:26,716][osd2][DEBUG ] /dev/dm-0 other, xfs, mounted on /
[2021-12-11 19:30:26,716][osd2][DEBUG ] /dev/dm-1 swap, swap
[2021-12-11 19:30:26,716][osd2][DEBUG ] /dev/sda :
[2021-12-11 19:30:26,716][osd2][DEBUG ]  /dev/sda2 other, LVM2_member
[2021-12-11 19:30:26,716][osd2][DEBUG ]  /dev/sda1 other, xfs, mounted on /boot
[2021-12-11 19:30:26,716][osd2][DEBUG ] /dev/sdb :
[2021-12-11 19:30:26,717][osd2][DEBUG ]  /dev/sdb2 ceph journal, for /dev/sdb1
[2021-12-11 19:30:26,717][osd2][DEBUG ]  /dev/sdb1 ceph data, prepared, unknown cluster 67d9e224-a6f7-43d3-ae36-e6100c59258e, osd.1, journal /dev/sdb2
[2021-12-11 19:30:26,717][osd2][DEBUG ] /dev/sr0 other, unknown
[2021-12-11 19:30:33,253][ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephuser/.cephdeploy.conf
[2021-12-11 19:30:33,254][ceph_deploy.cli][INFO  ] Invoked (1.5.39): /usr/bin/ceph-deploy disk list osd1 osd2 osd3 osd4 osd5
[2021-12-11 19:30:33,254][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2021-12-11 19:30:33,254][ceph_deploy.cli][INFO  ]  username                      : None
[2021-12-11 19:30:33,254][ceph_deploy.cli][INFO  ]  verbose                       : False
[2021-12-11 19:30:33,254][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2021-12-11 19:30:33,254][ceph_deploy.cli][INFO  ]  subcommand                    : list
[2021-12-11 19:30:33,254][ceph_deploy.cli][INFO  ]  quiet                         : False
[2021-12-11 19:30:33,254][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f8e5d274ef0>
[2021-12-11 19:30:33,254][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2021-12-11 19:30:33,255][ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7f8e5d2500c8>
[2021-12-11 19:30:33,255][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2021-12-11 19:30:33,255][ceph_deploy.cli][INFO  ]  default_release               : False
[2021-12-11 19:30:33,255][ceph_deploy.cli][INFO  ]  disk                          : [('osd1', None, None), ('osd2', None, None), ('osd3', None, None), ('osd4', None, None), ('osd5', None, None)]
[2021-12-11 19:30:33,428][osd1][DEBUG ] connection detected need for sudo
[2021-12-11 19:30:33,600][osd1][DEBUG ] connected to host: osd1 
[2021-12-11 19:30:33,600][osd1][DEBUG ] detect platform information from remote host
[2021-12-11 19:30:33,618][osd1][DEBUG ] detect machine type
[2021-12-11 19:30:33,622][osd1][DEBUG ] find the location of an executable
[2021-12-11 19:30:33,623][ceph_deploy.osd][INFO  ] Distro info: CentOS Linux 7.9.2009 Core
[2021-12-11 19:30:33,623][ceph_deploy.osd][DEBUG ] Listing disks on osd1...
[2021-12-11 19:30:33,623][osd1][DEBUG ] find the location of an executable
[2021-12-11 19:30:33,626][osd1][INFO  ] Running command: sudo /usr/sbin/ceph-disk list
[2021-12-11 19:30:33,795][osd1][DEBUG ] /dev/dm-0 other, xfs, mounted on /
[2021-12-11 19:30:33,795][osd1][DEBUG ] /dev/dm-1 swap, swap
[2021-12-11 19:30:33,795][osd1][DEBUG ] /dev/sda :
[2021-12-11 19:30:33,796][osd1][DEBUG ]  /dev/sda2 other, LVM2_member
[2021-12-11 19:30:33,796][osd1][DEBUG ]  /dev/sda1 other, xfs, mounted on /boot
[2021-12-11 19:30:33,796][osd1][DEBUG ] /dev/sdb other, unknown
[2021-12-11 19:30:33,796][osd1][DEBUG ] /dev/sr0 other, unknown
[2021-12-11 19:30:33,951][osd2][DEBUG ] connection detected need for sudo
[2021-12-11 19:30:34,124][osd2][DEBUG ] connected to host: osd2 
[2021-12-11 19:30:34,125][osd2][DEBUG ] detect platform information from remote host
[2021-12-11 19:30:34,144][osd2][DEBUG ] detect machine type
[2021-12-11 19:30:34,149][osd2][DEBUG ] find the location of an executable
[2021-12-11 19:30:34,150][ceph_deploy.osd][INFO  ] Distro info: CentOS Linux 7.9.2009 Core
[2021-12-11 19:30:34,150][ceph_deploy.osd][DEBUG ] Listing disks on osd2...
[2021-12-11 19:30:34,150][osd2][DEBUG ] find the location of an executable
[2021-12-11 19:30:34,154][osd2][INFO  ] Running command: sudo /usr/sbin/ceph-disk list
[2021-12-11 19:30:34,671][osd2][DEBUG ] /dev/dm-0 other, xfs, mounted on /
[2021-12-11 19:30:34,672][osd2][DEBUG ] /dev/dm-1 swap, swap
[2021-12-11 19:30:34,672][osd2][DEBUG ] /dev/sda :
[2021-12-11 19:30:34,672][osd2][DEBUG ]  /dev/sda2 other, LVM2_member
[2021-12-11 19:30:34,672][osd2][DEBUG ]  /dev/sda1 other, xfs, mounted on /boot
[2021-12-11 19:30:34,672][osd2][DEBUG ] /dev/sdb :
[2021-12-11 19:30:34,672][osd2][DEBUG ]  /dev/sdb2 ceph journal, for /dev/sdb1
[2021-12-11 19:30:34,672][osd2][DEBUG ]  /dev/sdb1 ceph data, prepared, unknown cluster 67d9e224-a6f7-43d3-ae36-e6100c59258e, osd.1, journal /dev/sdb2
[2021-12-11 19:30:34,672][osd2][DEBUG ] /dev/sr0 other, unknown
[2021-12-11 19:30:34,892][osd3][DEBUG ] connection detected need for sudo
[2021-12-11 19:30:35,065][osd3][DEBUG ] connected to host: osd3 
[2021-12-11 19:30:35,065][osd3][DEBUG ] detect platform information from remote host
[2021-12-11 19:30:35,083][osd3][DEBUG ] detect machine type
[2021-12-11 19:30:35,088][osd3][DEBUG ] find the location of an executable
[2021-12-11 19:30:35,089][ceph_deploy.osd][INFO  ] Distro info: CentOS Linux 7.9.2009 Core
[2021-12-11 19:30:35,089][ceph_deploy.osd][DEBUG ] Listing disks on osd3...
[2021-12-11 19:30:35,089][osd3][DEBUG ] find the location of an executable
[2021-12-11 19:30:35,092][osd3][INFO  ] Running command: sudo /usr/sbin/ceph-disk list
[2021-12-11 19:30:36,111][osd3][DEBUG ] /dev/dm-0 other, xfs, mounted on /
[2021-12-11 19:30:36,111][osd3][DEBUG ] /dev/dm-1 swap, swap
[2021-12-11 19:30:36,111][osd3][DEBUG ] /dev/sda :
[2021-12-11 19:30:36,111][osd3][DEBUG ]  /dev/sda2 other, LVM2_member
[2021-12-11 19:30:36,112][osd3][DEBUG ]  /dev/sda1 other, xfs, mounted on /boot
[2021-12-11 19:30:36,112][osd3][DEBUG ] /dev/sdb :
[2021-12-11 19:30:36,112][osd3][DEBUG ]  /dev/sdb2 ceph journal, for /dev/sdb1
[2021-12-11 19:30:36,112][osd3][DEBUG ]  /dev/sdb1 ceph data, prepared, unknown cluster 67d9e224-a6f7-43d3-ae36-e6100c59258e, osd.2, journal /dev/sdb2
[2021-12-11 19:30:36,316][osd4][DEBUG ] connection detected need for sudo
[2021-12-11 19:30:36,491][osd4][DEBUG ] connected to host: osd4 
[2021-12-11 19:30:36,492][osd4][DEBUG ] detect platform information from remote host
[2021-12-11 19:30:36,510][osd4][DEBUG ] detect machine type
[2021-12-11 19:30:36,515][osd4][DEBUG ] find the location of an executable
[2021-12-11 19:30:36,516][ceph_deploy.osd][INFO  ] Distro info: CentOS Linux 7.9.2009 Core
[2021-12-11 19:30:36,516][ceph_deploy.osd][DEBUG ] Listing disks on osd4...
[2021-12-11 19:30:36,516][osd4][DEBUG ] find the location of an executable
[2021-12-11 19:30:36,519][osd4][INFO  ] Running command: sudo /usr/sbin/ceph-disk list
[2021-12-11 19:30:37,037][osd4][DEBUG ] /dev/dm-0 other, xfs, mounted on /
[2021-12-11 19:30:37,037][osd4][DEBUG ] /dev/dm-1 swap, swap
[2021-12-11 19:30:37,037][osd4][DEBUG ] /dev/sda :
[2021-12-11 19:30:37,037][osd4][DEBUG ]  /dev/sda2 other, LVM2_member
[2021-12-11 19:30:37,037][osd4][DEBUG ]  /dev/sda1 other, xfs, mounted on /boot
[2021-12-11 19:30:37,037][osd4][DEBUG ] /dev/sdb :
[2021-12-11 19:30:37,037][osd4][DEBUG ]  /dev/sdb2 ceph journal, for /dev/sdb1
[2021-12-11 19:30:37,037][osd4][DEBUG ]  /dev/sdb1 ceph data, prepared, unknown cluster 67d9e224-a6f7-43d3-ae36-e6100c59258e, osd.3, journal /dev/sdb2
[2021-12-11 19:30:37,250][osd5][DEBUG ] connection detected need for sudo
[2021-12-11 19:30:37,423][osd5][DEBUG ] connected to host: osd5 
[2021-12-11 19:30:37,424][osd5][DEBUG ] detect platform information from remote host
[2021-12-11 19:30:37,442][osd5][DEBUG ] detect machine type
[2021-12-11 19:30:37,447][osd5][DEBUG ] find the location of an executable
[2021-12-11 19:30:37,449][ceph_deploy.osd][INFO  ] Distro info: CentOS Linux 7.9.2009 Core
[2021-12-11 19:30:37,449][ceph_deploy.osd][DEBUG ] Listing disks on osd5...
[2021-12-11 19:30:37,449][osd5][DEBUG ] find the location of an executable
[2021-12-11 19:30:37,451][osd5][INFO  ] Running command: sudo /usr/sbin/ceph-disk list
[2021-12-11 19:30:37,969][osd5][DEBUG ] /dev/dm-0 other, xfs, mounted on /
[2021-12-11 19:30:37,970][osd5][DEBUG ] /dev/dm-1 swap, swap
[2021-12-11 19:30:37,970][osd5][DEBUG ] /dev/sda :
[2021-12-11 19:30:37,970][osd5][DEBUG ]  /dev/sda2 other, LVM2_member
[2021-12-11 19:30:37,970][osd5][DEBUG ]  /dev/sda1 other, xfs, mounted on /boot
[2021-12-11 19:30:37,970][osd5][DEBUG ] /dev/sdb :
[2021-12-11 19:30:37,970][osd5][DEBUG ]  /dev/sdb2 ceph journal, for /dev/sdb1
[2021-12-11 19:30:37,970][osd5][DEBUG ]  /dev/sdb1 ceph data, prepared, unknown cluster 67d9e224-a6f7-43d3-ae36-e6100c59258e, osd.4, journal /dev/sdb2
[2021-12-11 19:31:04,296][ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephuser/.cephdeploy.conf
[2021-12-11 19:31:04,297][ceph_deploy.cli][INFO  ] Invoked (1.5.39): /usr/bin/ceph-deploy disk zap osd1
[2021-12-11 19:31:04,297][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2021-12-11 19:31:04,297][ceph_deploy.cli][INFO  ]  username                      : None
[2021-12-11 19:31:04,297][ceph_deploy.cli][INFO  ]  verbose                       : False
[2021-12-11 19:31:04,297][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2021-12-11 19:31:04,297][ceph_deploy.cli][INFO  ]  subcommand                    : zap
[2021-12-11 19:31:04,297][ceph_deploy.cli][INFO  ]  quiet                         : False
[2021-12-11 19:31:04,297][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f9344bf6ef0>
[2021-12-11 19:31:04,297][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2021-12-11 19:31:04,297][ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7f9344bd20c8>
[2021-12-11 19:31:04,297][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2021-12-11 19:31:04,297][ceph_deploy.cli][INFO  ]  default_release               : False
[2021-12-11 19:31:04,297][ceph_deploy.cli][INFO  ]  disk                          : [('osd1', None, None)]
[2021-12-11 19:31:04,297][ceph_deploy][ERROR ] RuntimeError: zap command needs both HOSTNAME and DISK but got "osd1 None"

[2021-12-11 19:31:10,356][ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephuser/.cephdeploy.conf
[2021-12-11 19:31:10,357][ceph_deploy.cli][INFO  ] Invoked (1.5.39): /usr/bin/ceph-deploy disk zap osd1:/dev/sdb
[2021-12-11 19:31:10,357][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2021-12-11 19:31:10,357][ceph_deploy.cli][INFO  ]  username                      : None
[2021-12-11 19:31:10,357][ceph_deploy.cli][INFO  ]  verbose                       : False
[2021-12-11 19:31:10,357][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2021-12-11 19:31:10,357][ceph_deploy.cli][INFO  ]  subcommand                    : zap
[2021-12-11 19:31:10,357][ceph_deploy.cli][INFO  ]  quiet                         : False
[2021-12-11 19:31:10,357][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f6ddf667ef0>
[2021-12-11 19:31:10,357][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2021-12-11 19:31:10,357][ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7f6ddf6430c8>
[2021-12-11 19:31:10,357][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2021-12-11 19:31:10,357][ceph_deploy.cli][INFO  ]  default_release               : False
[2021-12-11 19:31:10,357][ceph_deploy.cli][INFO  ]  disk                          : [('osd1', '/dev/sdb', None)]
[2021-12-11 19:31:10,358][ceph_deploy.osd][DEBUG ] zapping /dev/sdb on osd1
[2021-12-11 19:31:10,527][osd1][DEBUG ] connection detected need for sudo
[2021-12-11 19:31:10,701][osd1][DEBUG ] connected to host: osd1 
[2021-12-11 19:31:10,701][osd1][DEBUG ] detect platform information from remote host
[2021-12-11 19:31:10,720][osd1][DEBUG ] detect machine type
[2021-12-11 19:31:10,726][osd1][DEBUG ] find the location of an executable
[2021-12-11 19:31:10,727][ceph_deploy.osd][INFO  ] Distro info: CentOS Linux 7.9.2009 Core
[2021-12-11 19:31:10,727][osd1][DEBUG ] zeroing last few blocks of device
[2021-12-11 19:31:10,728][osd1][DEBUG ] find the location of an executable
[2021-12-11 19:31:10,732][osd1][INFO  ] Running command: sudo /usr/sbin/ceph-disk zap /dev/sdb
[2021-12-11 19:31:10,851][osd1][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[2021-12-11 19:31:10,852][osd1][WARNING] backup header from main header.
[2021-12-11 19:31:10,852][osd1][WARNING] 
[2021-12-11 19:31:12,069][osd1][DEBUG ] ****************************************************************************
[2021-12-11 19:31:12,069][osd1][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[2021-12-11 19:31:12,069][osd1][DEBUG ] verification and recovery are STRONGLY recommended.
[2021-12-11 19:31:12,069][osd1][DEBUG ] ****************************************************************************
[2021-12-11 19:31:12,069][osd1][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[2021-12-11 19:31:12,070][osd1][DEBUG ] other utilities.
[2021-12-11 19:31:13,136][osd1][DEBUG ] Creating new GPT entries.
[2021-12-11 19:31:13,137][osd1][DEBUG ] The operation has completed successfully.
[2021-12-11 19:31:37,446][ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephuser/.cephdeploy.conf
[2021-12-11 19:31:37,447][ceph_deploy.cli][INFO  ] Invoked (1.5.39): /usr/bin/ceph-deploy disk zap osd2:/dev/sdb osd3:/dev/sdb osd4:/dev/sdb osd5:/dev/sdb
[2021-12-11 19:31:37,447][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2021-12-11 19:31:37,447][ceph_deploy.cli][INFO  ]  username                      : None
[2021-12-11 19:31:37,447][ceph_deploy.cli][INFO  ]  verbose                       : False
[2021-12-11 19:31:37,447][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2021-12-11 19:31:37,447][ceph_deploy.cli][INFO  ]  subcommand                    : zap
[2021-12-11 19:31:37,447][ceph_deploy.cli][INFO  ]  quiet                         : False
[2021-12-11 19:31:37,447][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fa0496abef0>
[2021-12-11 19:31:37,447][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2021-12-11 19:31:37,447][ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7fa0496870c8>
[2021-12-11 19:31:37,447][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2021-12-11 19:31:37,447][ceph_deploy.cli][INFO  ]  default_release               : False
[2021-12-11 19:31:37,447][ceph_deploy.cli][INFO  ]  disk                          : [('osd2', '/dev/sdb', None), ('osd3', '/dev/sdb', None), ('osd4', '/dev/sdb', None), ('osd5', '/dev/sdb', None)]
[2021-12-11 19:31:37,448][ceph_deploy.osd][DEBUG ] zapping /dev/sdb on osd2
[2021-12-11 19:31:37,619][osd2][DEBUG ] connection detected need for sudo
[2021-12-11 19:31:37,791][osd2][DEBUG ] connected to host: osd2 
[2021-12-11 19:31:37,792][osd2][DEBUG ] detect platform information from remote host
[2021-12-11 19:31:37,813][osd2][DEBUG ] detect machine type
[2021-12-11 19:31:37,818][osd2][DEBUG ] find the location of an executable
[2021-12-11 19:31:37,819][ceph_deploy.osd][INFO  ] Distro info: CentOS Linux 7.9.2009 Core
[2021-12-11 19:31:37,819][osd2][DEBUG ] zeroing last few blocks of device
[2021-12-11 19:31:37,820][osd2][DEBUG ] find the location of an executable
[2021-12-11 19:31:37,823][osd2][INFO  ] Running command: sudo /usr/sbin/ceph-disk zap /dev/sdb
[2021-12-11 19:31:37,942][osd2][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[2021-12-11 19:31:37,942][osd2][WARNING] backup header from main header.
[2021-12-11 19:31:37,942][osd2][WARNING] 
[2021-12-11 19:31:37,942][osd2][WARNING] Warning! Main and backup partition tables differ! Use the 'c' and 'e' options
[2021-12-11 19:31:37,942][osd2][WARNING] on the recovery & transformation menu to examine the two tables.
[2021-12-11 19:31:37,942][osd2][WARNING] 
[2021-12-11 19:31:37,942][osd2][WARNING] Warning! One or more CRCs don't match. You should repair the disk!
[2021-12-11 19:31:37,942][osd2][WARNING] 
[2021-12-11 19:31:39,109][osd2][DEBUG ] ****************************************************************************
[2021-12-11 19:31:39,110][osd2][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[2021-12-11 19:31:39,110][osd2][DEBUG ] verification and recovery are STRONGLY recommended.
[2021-12-11 19:31:39,110][osd2][DEBUG ] ****************************************************************************
[2021-12-11 19:31:39,110][osd2][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[2021-12-11 19:31:39,110][osd2][DEBUG ] other utilities.
[2021-12-11 19:31:40,227][osd2][DEBUG ] Creating new GPT entries.
[2021-12-11 19:31:40,227][osd2][DEBUG ] The operation has completed successfully.
[2021-12-11 19:31:40,230][ceph_deploy.osd][DEBUG ] zapping /dev/sdb on osd3
[2021-12-11 19:31:40,385][osd3][DEBUG ] connection detected need for sudo
[2021-12-11 19:31:40,555][osd3][DEBUG ] connected to host: osd3 
[2021-12-11 19:31:40,556][osd3][DEBUG ] detect platform information from remote host
[2021-12-11 19:31:40,573][osd3][DEBUG ] detect machine type
[2021-12-11 19:31:40,578][osd3][DEBUG ] find the location of an executable
[2021-12-11 19:31:40,580][ceph_deploy.osd][INFO  ] Distro info: CentOS Linux 7.9.2009 Core
[2021-12-11 19:31:40,580][osd3][DEBUG ] zeroing last few blocks of device
[2021-12-11 19:31:40,581][osd3][DEBUG ] find the location of an executable
[2021-12-11 19:31:40,583][osd3][INFO  ] Running command: sudo /usr/sbin/ceph-disk zap /dev/sdb
[2021-12-11 19:31:40,700][osd3][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[2021-12-11 19:31:40,701][osd3][WARNING] backup header from main header.
[2021-12-11 19:31:40,701][osd3][WARNING] 
[2021-12-11 19:31:40,701][osd3][WARNING] Warning! Main and backup partition tables differ! Use the 'c' and 'e' options
[2021-12-11 19:31:40,701][osd3][WARNING] on the recovery & transformation menu to examine the two tables.
[2021-12-11 19:31:40,701][osd3][WARNING] 
[2021-12-11 19:31:40,701][osd3][WARNING] Warning! One or more CRCs don't match. You should repair the disk!
[2021-12-11 19:31:40,701][osd3][WARNING] 
[2021-12-11 19:31:41,768][osd3][DEBUG ] ****************************************************************************
[2021-12-11 19:31:41,768][osd3][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[2021-12-11 19:31:41,768][osd3][DEBUG ] verification and recovery are STRONGLY recommended.
[2021-12-11 19:31:41,768][osd3][DEBUG ] ****************************************************************************
[2021-12-11 19:31:41,768][osd3][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[2021-12-11 19:31:41,768][osd3][DEBUG ] other utilities.
[2021-12-11 19:31:42,835][osd3][DEBUG ] Creating new GPT entries.
[2021-12-11 19:31:42,835][osd3][DEBUG ] The operation has completed successfully.
[2021-12-11 19:31:42,899][ceph_deploy.osd][DEBUG ] zapping /dev/sdb on osd4
[2021-12-11 19:31:43,053][osd4][DEBUG ] connection detected need for sudo
[2021-12-11 19:31:43,222][osd4][DEBUG ] connected to host: osd4 
[2021-12-11 19:31:43,222][osd4][DEBUG ] detect platform information from remote host
[2021-12-11 19:31:43,239][osd4][DEBUG ] detect machine type
[2021-12-11 19:31:43,244][osd4][DEBUG ] find the location of an executable
[2021-12-11 19:31:43,245][ceph_deploy.osd][INFO  ] Distro info: CentOS Linux 7.9.2009 Core
[2021-12-11 19:31:43,245][osd4][DEBUG ] zeroing last few blocks of device
[2021-12-11 19:31:43,246][osd4][DEBUG ] find the location of an executable
[2021-12-11 19:31:43,249][osd4][INFO  ] Running command: sudo /usr/sbin/ceph-disk zap /dev/sdb
[2021-12-11 19:31:43,366][osd4][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[2021-12-11 19:31:43,366][osd4][WARNING] backup header from main header.
[2021-12-11 19:31:43,366][osd4][WARNING] 
[2021-12-11 19:31:43,367][osd4][WARNING] Warning! Main and backup partition tables differ! Use the 'c' and 'e' options
[2021-12-11 19:31:43,367][osd4][WARNING] on the recovery & transformation menu to examine the two tables.
[2021-12-11 19:31:43,367][osd4][WARNING] 
[2021-12-11 19:31:43,367][osd4][WARNING] Warning! One or more CRCs don't match. You should repair the disk!
[2021-12-11 19:31:43,367][osd4][WARNING] 
[2021-12-11 19:31:44,484][osd4][DEBUG ] ****************************************************************************
[2021-12-11 19:31:44,484][osd4][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[2021-12-11 19:31:44,484][osd4][DEBUG ] verification and recovery are STRONGLY recommended.
[2021-12-11 19:31:44,484][osd4][DEBUG ] ****************************************************************************
[2021-12-11 19:31:44,484][osd4][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[2021-12-11 19:31:44,484][osd4][DEBUG ] other utilities.
[2021-12-11 19:31:45,551][osd4][DEBUG ] Creating new GPT entries.
[2021-12-11 19:31:45,551][osd4][DEBUG ] The operation has completed successfully.
[2021-12-11 19:31:45,559][ceph_deploy.osd][DEBUG ] zapping /dev/sdb on osd5
[2021-12-11 19:31:45,712][osd5][DEBUG ] connection detected need for sudo
[2021-12-11 19:31:45,879][osd5][DEBUG ] connected to host: osd5 
[2021-12-11 19:31:45,880][osd5][DEBUG ] detect platform information from remote host
[2021-12-11 19:31:45,897][osd5][DEBUG ] detect machine type
[2021-12-11 19:31:45,902][osd5][DEBUG ] find the location of an executable
[2021-12-11 19:31:45,904][ceph_deploy.osd][INFO  ] Distro info: CentOS Linux 7.9.2009 Core
[2021-12-11 19:31:45,904][osd5][DEBUG ] zeroing last few blocks of device
[2021-12-11 19:31:45,905][osd5][DEBUG ] find the location of an executable
[2021-12-11 19:31:45,908][osd5][INFO  ] Running command: sudo /usr/sbin/ceph-disk zap /dev/sdb
[2021-12-11 19:31:46,025][osd5][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[2021-12-11 19:31:46,025][osd5][WARNING] backup header from main header.
[2021-12-11 19:31:46,026][osd5][WARNING] 
[2021-12-11 19:31:46,026][osd5][WARNING] Warning! Main and backup partition tables differ! Use the 'c' and 'e' options
[2021-12-11 19:31:46,026][osd5][WARNING] on the recovery & transformation menu to examine the two tables.
[2021-12-11 19:31:46,026][osd5][WARNING] 
[2021-12-11 19:31:46,026][osd5][WARNING] Warning! One or more CRCs don't match. You should repair the disk!
[2021-12-11 19:31:46,026][osd5][WARNING] 
[2021-12-11 19:31:47,093][osd5][DEBUG ] ****************************************************************************
[2021-12-11 19:31:47,093][osd5][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[2021-12-11 19:31:47,093][osd5][DEBUG ] verification and recovery are STRONGLY recommended.
[2021-12-11 19:31:47,093][osd5][DEBUG ] ****************************************************************************
[2021-12-11 19:31:47,093][osd5][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[2021-12-11 19:31:47,093][osd5][DEBUG ] other utilities.
[2021-12-11 19:31:48,160][osd5][DEBUG ] Creating new GPT entries.
[2021-12-11 19:31:48,161][osd5][DEBUG ] The operation has completed successfully.
[2021-12-11 19:32:10,503][ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephuser/.cephdeploy.conf
[2021-12-11 19:32:10,503][ceph_deploy.cli][INFO  ] Invoked (1.5.39): /usr/bin/ceph-deploy osd prepare osd1:/dev/sdb osd2:/dev/sdb osd3:/dev/sdb osd4:/dev/sdb osd5:/dev/sdb
[2021-12-11 19:32:10,503][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2021-12-11 19:32:10,504][ceph_deploy.cli][INFO  ]  username                      : None
[2021-12-11 19:32:10,504][ceph_deploy.cli][INFO  ]  block_db                      : None
[2021-12-11 19:32:10,504][ceph_deploy.cli][INFO  ]  disk                          : [('osd1', '/dev/sdb', None), ('osd2', '/dev/sdb', None), ('osd3', '/dev/sdb', None), ('osd4', '/dev/sdb', None), ('osd5', '/dev/sdb', None)]
[2021-12-11 19:32:10,504][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2021-12-11 19:32:10,504][ceph_deploy.cli][INFO  ]  verbose                       : False
[2021-12-11 19:32:10,504][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2021-12-11 19:32:10,504][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2021-12-11 19:32:10,504][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2021-12-11 19:32:10,504][ceph_deploy.cli][INFO  ]  subcommand                    : prepare
[2021-12-11 19:32:10,504][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2021-12-11 19:32:10,504][ceph_deploy.cli][INFO  ]  quiet                         : False
[2021-12-11 19:32:10,504][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f0b6f8b1128>
[2021-12-11 19:32:10,504][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2021-12-11 19:32:10,504][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2021-12-11 19:32:10,504][ceph_deploy.cli][INFO  ]  filestore                     : None
[2021-12-11 19:32:10,504][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f0b6f8fe050>
[2021-12-11 19:32:10,504][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2021-12-11 19:32:10,504][ceph_deploy.cli][INFO  ]  default_release               : False
[2021-12-11 19:32:10,504][ceph_deploy.cli][INFO  ]  zap_disk                      : False
[2021-12-11 19:32:10,505][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks osd1:/dev/sdb: osd2:/dev/sdb: osd3:/dev/sdb: osd4:/dev/sdb: osd5:/dev/sdb:
[2021-12-11 19:32:10,677][osd1][DEBUG ] connection detected need for sudo
[2021-12-11 19:32:10,849][osd1][DEBUG ] connected to host: osd1 
[2021-12-11 19:32:10,849][osd1][DEBUG ] detect platform information from remote host
[2021-12-11 19:32:10,867][osd1][DEBUG ] detect machine type
[2021-12-11 19:32:10,873][osd1][DEBUG ] find the location of an executable
[2021-12-11 19:32:10,874][ceph_deploy.osd][INFO  ] Distro info: CentOS Linux 7.9.2009 Core
[2021-12-11 19:32:10,874][ceph_deploy.osd][DEBUG ] Deploying osd to osd1
[2021-12-11 19:32:10,875][osd1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2021-12-11 19:32:10,877][ceph_deploy.osd][ERROR ] RuntimeError: config file /etc/ceph/ceph.conf exists with different content; use --overwrite-conf to overwrite
[2021-12-11 19:32:11,035][osd2][DEBUG ] connection detected need for sudo
[2021-12-11 19:32:11,208][osd2][DEBUG ] connected to host: osd2 
[2021-12-11 19:32:11,209][osd2][DEBUG ] detect platform information from remote host
[2021-12-11 19:32:11,227][osd2][DEBUG ] detect machine type
[2021-12-11 19:32:11,232][osd2][DEBUG ] find the location of an executable
[2021-12-11 19:32:11,233][ceph_deploy.osd][INFO  ] Distro info: CentOS Linux 7.9.2009 Core
[2021-12-11 19:32:11,233][ceph_deploy.osd][DEBUG ] Deploying osd to osd2
[2021-12-11 19:32:11,234][osd2][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2021-12-11 19:32:11,235][ceph_deploy.osd][ERROR ] RuntimeError: config file /etc/ceph/ceph.conf exists with different content; use --overwrite-conf to overwrite
[2021-12-11 19:32:11,391][osd3][DEBUG ] connection detected need for sudo
[2021-12-11 19:32:11,560][osd3][DEBUG ] connected to host: osd3 
[2021-12-11 19:32:11,560][osd3][DEBUG ] detect platform information from remote host
[2021-12-11 19:32:11,577][osd3][DEBUG ] detect machine type
[2021-12-11 19:32:11,582][osd3][DEBUG ] find the location of an executable
[2021-12-11 19:32:11,583][ceph_deploy.osd][INFO  ] Distro info: CentOS Linux 7.9.2009 Core
[2021-12-11 19:32:11,583][ceph_deploy.osd][DEBUG ] Deploying osd to osd3
[2021-12-11 19:32:11,583][osd3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2021-12-11 19:32:11,585][ceph_deploy.osd][ERROR ] RuntimeError: config file /etc/ceph/ceph.conf exists with different content; use --overwrite-conf to overwrite
[2021-12-11 19:32:11,742][osd4][DEBUG ] connection detected need for sudo
[2021-12-11 19:32:11,911][osd4][DEBUG ] connected to host: osd4 
[2021-12-11 19:32:11,912][osd4][DEBUG ] detect platform information from remote host
[2021-12-11 19:32:11,928][osd4][DEBUG ] detect machine type
[2021-12-11 19:32:11,933][osd4][DEBUG ] find the location of an executable
[2021-12-11 19:32:11,934][ceph_deploy.osd][INFO  ] Distro info: CentOS Linux 7.9.2009 Core
[2021-12-11 19:32:11,934][ceph_deploy.osd][DEBUG ] Deploying osd to osd4
[2021-12-11 19:32:11,934][osd4][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2021-12-11 19:32:11,936][ceph_deploy.osd][ERROR ] RuntimeError: config file /etc/ceph/ceph.conf exists with different content; use --overwrite-conf to overwrite
[2021-12-11 19:32:12,089][osd5][DEBUG ] connection detected need for sudo
[2021-12-11 19:32:12,259][osd5][DEBUG ] connected to host: osd5 
[2021-12-11 19:32:12,260][osd5][DEBUG ] detect platform information from remote host
[2021-12-11 19:32:12,276][osd5][DEBUG ] detect machine type
[2021-12-11 19:32:12,281][osd5][DEBUG ] find the location of an executable
[2021-12-11 19:32:12,282][ceph_deploy.osd][INFO  ] Distro info: CentOS Linux 7.9.2009 Core
[2021-12-11 19:32:12,282][ceph_deploy.osd][DEBUG ] Deploying osd to osd5
[2021-12-11 19:32:12,282][osd5][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2021-12-11 19:32:12,284][ceph_deploy.osd][ERROR ] RuntimeError: config file /etc/ceph/ceph.conf exists with different content; use --overwrite-conf to overwrite
[2021-12-11 19:32:12,284][ceph_deploy][ERROR ] GenericError: Failed to create 5 OSDs

[2021-12-11 19:32:49,893][ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephuser/.cephdeploy.conf
[2021-12-11 19:32:49,893][ceph_deploy.cli][INFO  ] Invoked (1.5.39): /usr/bin/ceph-deploy --overwrite-conf osd prepare osd1:/dev/sdb osd2:/dev/sdb osd3:/dev/sdb osd4:/dev/sdb osd5:/dev/sdb
[2021-12-11 19:32:49,894][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2021-12-11 19:32:49,894][ceph_deploy.cli][INFO  ]  username                      : None
[2021-12-11 19:32:49,894][ceph_deploy.cli][INFO  ]  block_db                      : None
[2021-12-11 19:32:49,894][ceph_deploy.cli][INFO  ]  disk                          : [('osd1', '/dev/sdb', None), ('osd2', '/dev/sdb', None), ('osd3', '/dev/sdb', None), ('osd4', '/dev/sdb', None), ('osd5', '/dev/sdb', None)]
[2021-12-11 19:32:49,894][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2021-12-11 19:32:49,894][ceph_deploy.cli][INFO  ]  verbose                       : False
[2021-12-11 19:32:49,894][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2021-12-11 19:32:49,894][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2021-12-11 19:32:49,894][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2021-12-11 19:32:49,894][ceph_deploy.cli][INFO  ]  subcommand                    : prepare
[2021-12-11 19:32:49,894][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2021-12-11 19:32:49,894][ceph_deploy.cli][INFO  ]  quiet                         : False
[2021-12-11 19:32:49,894][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7efe63f67128>
[2021-12-11 19:32:49,894][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2021-12-11 19:32:49,894][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2021-12-11 19:32:49,894][ceph_deploy.cli][INFO  ]  filestore                     : None
[2021-12-11 19:32:49,894][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7efe63fb4050>
[2021-12-11 19:32:49,894][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2021-12-11 19:32:49,894][ceph_deploy.cli][INFO  ]  default_release               : False
[2021-12-11 19:32:49,894][ceph_deploy.cli][INFO  ]  zap_disk                      : False
[2021-12-11 19:32:49,895][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks osd1:/dev/sdb: osd2:/dev/sdb: osd3:/dev/sdb: osd4:/dev/sdb: osd5:/dev/sdb:
[2021-12-11 19:32:50,064][osd1][DEBUG ] connection detected need for sudo
[2021-12-11 19:32:50,238][osd1][DEBUG ] connected to host: osd1 
[2021-12-11 19:32:50,239][osd1][DEBUG ] detect platform information from remote host
[2021-12-11 19:32:50,258][osd1][DEBUG ] detect machine type
[2021-12-11 19:32:50,263][osd1][DEBUG ] find the location of an executable
[2021-12-11 19:32:50,264][ceph_deploy.osd][INFO  ] Distro info: CentOS Linux 7.9.2009 Core
[2021-12-11 19:32:50,264][ceph_deploy.osd][DEBUG ] Deploying osd to osd1
[2021-12-11 19:32:50,264][osd1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2021-12-11 19:32:50,266][ceph_deploy.osd][DEBUG ] Preparing host osd1 disk /dev/sdb journal None activate False
[2021-12-11 19:32:50,266][osd1][DEBUG ] find the location of an executable
[2021-12-11 19:32:50,270][osd1][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdb
[2021-12-11 19:32:50,389][osd1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2021-12-11 19:32:50,653][osd1][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --log-file $run_dir/$cluster-osd-check.log --cluster ceph --setuser ceph --setgroup ceph
[2021-12-11 19:32:50,654][osd1][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --log-file $run_dir/$cluster-osd-check.log --cluster ceph --setuser ceph --setgroup ceph
[2021-12-11 19:32:50,670][osd1][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --log-file $run_dir/$cluster-osd-check.log --cluster ceph --setuser ceph --setgroup ceph
[2021-12-11 19:32:50,702][osd1][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-11 19:32:50,702][osd1][WARNING] set_type: Will colocate journal with data on /dev/sdb
[2021-12-11 19:32:50,702][osd1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2021-12-11 19:32:50,705][osd1][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-11 19:32:50,706][osd1][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-11 19:32:50,706][osd1][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-11 19:32:50,706][osd1][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[2021-12-11 19:32:50,820][osd1][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[2021-12-11 19:32:50,820][osd1][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[2021-12-11 19:32:50,820][osd1][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[2021-12-11 19:32:50,820][osd1][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-11 19:32:50,820][osd1][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-11 19:32:50,820][osd1][WARNING] ptype_tobe_for_name: name = journal
[2021-12-11 19:32:50,820][osd1][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-11 19:32:50,821][osd1][WARNING] create_partition: Creating journal partition num 2 size 5120 on /dev/sdb
[2021-12-11 19:32:50,821][osd1][WARNING] command_check_call: Running command: /sbin/sgdisk --new=2:0:+5120M --change-name=2:ceph journal --partition-guid=2:6168df0d-94cb-4049-8048-76941299e2ab --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sdb
[2021-12-11 19:32:51,938][osd1][DEBUG ] Setting name!
[2021-12-11 19:32:51,938][osd1][DEBUG ] partNum is 1
[2021-12-11 19:32:51,938][osd1][DEBUG ] REALLY setting name!
[2021-12-11 19:32:51,938][osd1][DEBUG ] The operation has completed successfully.
[2021-12-11 19:32:51,938][osd1][WARNING] update_partition: Calling partprobe on created device /dev/sdb
[2021-12-11 19:32:51,938][osd1][WARNING] command_check_call: Running command: /usr/bin/udevadm settle --timeout=600
[2021-12-11 19:32:52,102][osd1][WARNING] command: Running command: /usr/bin/flock -s /dev/sdb /sbin/partprobe /dev/sdb
[2021-12-11 19:32:52,118][osd1][WARNING] command_check_call: Running command: /usr/bin/udevadm settle --timeout=600
[2021-12-11 19:32:52,283][osd1][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-11 19:32:52,283][osd1][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-11 19:32:52,283][osd1][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb2 uuid path is /sys/dev/block/8:18/dm/uuid
[2021-12-11 19:32:52,283][osd1][WARNING] prepare_device: Journal is GPT partition /dev/disk/by-partuuid/6168df0d-94cb-4049-8048-76941299e2ab
[2021-12-11 19:32:52,283][osd1][WARNING] prepare_device: Journal is GPT partition /dev/disk/by-partuuid/6168df0d-94cb-4049-8048-76941299e2ab
[2021-12-11 19:32:52,283][osd1][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-11 19:32:52,283][osd1][WARNING] set_data_partition: Creating osd partition on /dev/sdb
[2021-12-11 19:32:52,283][osd1][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-11 19:32:52,283][osd1][WARNING] ptype_tobe_for_name: name = data
[2021-12-11 19:32:52,283][osd1][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-11 19:32:52,284][osd1][WARNING] create_partition: Creating data partition num 1 size 0 on /dev/sdb
[2021-12-11 19:32:52,284][osd1][WARNING] command_check_call: Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:f3d18985-9ec5-4740-bee3-29048120b366 --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be --mbrtogpt -- /dev/sdb
[2021-12-11 19:32:53,451][osd1][DEBUG ] Setting name!
[2021-12-11 19:32:53,451][osd1][DEBUG ] partNum is 0
[2021-12-11 19:32:53,451][osd1][DEBUG ] REALLY setting name!
[2021-12-11 19:32:53,451][osd1][DEBUG ] The operation has completed successfully.
[2021-12-11 19:32:53,451][osd1][WARNING] update_partition: Calling partprobe on created device /dev/sdb
[2021-12-11 19:32:53,451][osd1][WARNING] command_check_call: Running command: /usr/bin/udevadm settle --timeout=600
[2021-12-11 19:32:53,616][osd1][WARNING] command: Running command: /usr/bin/flock -s /dev/sdb /sbin/partprobe /dev/sdb
[2021-12-11 19:32:53,680][osd1][WARNING] command_check_call: Running command: /usr/bin/udevadm settle --timeout=600
[2021-12-11 19:32:53,794][osd1][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-11 19:32:53,794][osd1][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-11 19:32:53,794][osd1][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb1 uuid path is /sys/dev/block/8:17/dm/uuid
[2021-12-11 19:32:53,794][osd1][WARNING] populate_data_path_device: Creating xfs fs on /dev/sdb1
[2021-12-11 19:32:53,794][osd1][WARNING] command_check_call: Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/sdb1
[2021-12-11 19:32:54,059][osd1][DEBUG ] Discarding blocks...Done.
[2021-12-11 19:32:54,059][osd1][DEBUG ] meta-data=/dev/sdb1              isize=2048   agcount=4, agsize=196543 blks
[2021-12-11 19:32:54,059][osd1][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=1
[2021-12-11 19:32:54,059][osd1][DEBUG ]          =                       crc=1        finobt=0, sparse=0
[2021-12-11 19:32:54,059][osd1][DEBUG ] data     =                       bsize=4096   blocks=786171, imaxpct=25
[2021-12-11 19:32:54,059][osd1][DEBUG ]          =                       sunit=0      swidth=0 blks
[2021-12-11 19:32:54,060][osd1][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0 ftype=1
[2021-12-11 19:32:54,060][osd1][DEBUG ] log      =internal log           bsize=4096   blocks=2560, version=2
[2021-12-11 19:32:54,060][osd1][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[2021-12-11 19:32:54,060][osd1][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[2021-12-11 19:32:54,060][osd1][WARNING] mount: Mounting /dev/sdb1 on /var/lib/ceph/tmp/mnt.5GDaTr with options noatime,inode64
[2021-12-11 19:32:54,060][osd1][WARNING] command_check_call: Running command: /usr/bin/mount -t xfs -o noatime,inode64 -- /dev/sdb1 /var/lib/ceph/tmp/mnt.5GDaTr
[2021-12-11 19:32:54,060][osd1][WARNING] command: Running command: /sbin/restorecon /var/lib/ceph/tmp/mnt.5GDaTr
[2021-12-11 19:32:54,060][osd1][WARNING] populate_data_path: Preparing osd data dir /var/lib/ceph/tmp/mnt.5GDaTr
[2021-12-11 19:32:54,174][osd1][WARNING] command: Running command: /sbin/restorecon -R /var/lib/ceph/tmp/mnt.5GDaTr/ceph_fsid.1581.tmp
[2021-12-11 19:32:54,174][osd1][WARNING] command: Running command: /usr/bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.5GDaTr/ceph_fsid.1581.tmp
[2021-12-11 19:32:54,190][osd1][WARNING] command: Running command: /sbin/restorecon -R /var/lib/ceph/tmp/mnt.5GDaTr/fsid.1581.tmp
[2021-12-11 19:32:54,190][osd1][WARNING] command: Running command: /usr/bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.5GDaTr/fsid.1581.tmp
[2021-12-11 19:32:54,222][osd1][WARNING] command: Running command: /sbin/restorecon -R /var/lib/ceph/tmp/mnt.5GDaTr/magic.1581.tmp
[2021-12-11 19:32:54,225][osd1][WARNING] command: Running command: /usr/bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.5GDaTr/magic.1581.tmp
[2021-12-11 19:32:54,257][osd1][WARNING] command: Running command: /sbin/restorecon -R /var/lib/ceph/tmp/mnt.5GDaTr/journal_uuid.1581.tmp
[2021-12-11 19:32:54,257][osd1][WARNING] command: Running command: /usr/bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.5GDaTr/journal_uuid.1581.tmp
[2021-12-11 19:32:54,257][osd1][WARNING] adjust_symlink: Creating symlink /var/lib/ceph/tmp/mnt.5GDaTr/journal -> /dev/disk/by-partuuid/6168df0d-94cb-4049-8048-76941299e2ab
[2021-12-11 19:32:54,257][osd1][WARNING] command: Running command: /sbin/restorecon -R /var/lib/ceph/tmp/mnt.5GDaTr
[2021-12-11 19:32:54,258][osd1][WARNING] command: Running command: /usr/bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.5GDaTr
[2021-12-11 19:32:54,259][osd1][WARNING] unmount: Unmounting /var/lib/ceph/tmp/mnt.5GDaTr
[2021-12-11 19:32:54,259][osd1][WARNING] command_check_call: Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.5GDaTr
[2021-12-11 19:32:54,373][osd1][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-11 19:32:54,373][osd1][WARNING] command_check_call: Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/sdb
[2021-12-11 19:32:55,491][osd1][DEBUG ] Warning: The kernel is still using the old partition table.
[2021-12-11 19:32:55,491][osd1][DEBUG ] The new table will be used at the next reboot.
[2021-12-11 19:32:55,491][osd1][DEBUG ] The operation has completed successfully.
[2021-12-11 19:32:55,492][osd1][WARNING] update_partition: Calling partprobe on prepared device /dev/sdb
[2021-12-11 19:32:55,492][osd1][WARNING] command_check_call: Running command: /usr/bin/udevadm settle --timeout=600
[2021-12-11 19:32:55,500][osd1][WARNING] command: Running command: /usr/bin/flock -s /dev/sdb /sbin/partprobe /dev/sdb
[2021-12-11 19:32:55,614][osd1][WARNING] command_check_call: Running command: /usr/bin/udevadm settle --timeout=600
[2021-12-11 19:32:55,728][osd1][WARNING] command_check_call: Running command: /usr/bin/udevadm trigger --action=add --sysname-match sdb1
[2021-12-11 19:33:00,734][osd1][INFO  ] checking OSD status...
[2021-12-11 19:33:00,734][osd1][DEBUG ] find the location of an executable
[2021-12-11 19:33:00,737][osd1][INFO  ] Running command: sudo /bin/ceph --cluster=ceph osd stat --format=json
[2021-12-11 19:33:00,952][osd1][WARNING] there are 4 OSDs down
[2021-12-11 19:33:00,952][osd1][WARNING] there are 2 OSDs out
[2021-12-11 19:33:00,952][ceph_deploy.osd][DEBUG ] Host osd1 is now ready for osd use.
[2021-12-11 19:33:01,109][osd2][DEBUG ] connection detected need for sudo
[2021-12-11 19:33:01,283][osd2][DEBUG ] connected to host: osd2 
[2021-12-11 19:33:01,284][osd2][DEBUG ] detect platform information from remote host
[2021-12-11 19:33:01,303][osd2][DEBUG ] detect machine type
[2021-12-11 19:33:01,309][osd2][DEBUG ] find the location of an executable
[2021-12-11 19:33:01,310][ceph_deploy.osd][INFO  ] Distro info: CentOS Linux 7.9.2009 Core
[2021-12-11 19:33:01,310][ceph_deploy.osd][DEBUG ] Deploying osd to osd2
[2021-12-11 19:33:01,310][osd2][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2021-12-11 19:33:01,314][ceph_deploy.osd][DEBUG ] Preparing host osd2 disk /dev/sdb journal None activate False
[2021-12-11 19:33:01,314][osd2][DEBUG ] find the location of an executable
[2021-12-11 19:33:01,317][osd2][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdb
[2021-12-11 19:33:01,434][osd2][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2021-12-11 19:33:01,434][osd2][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --log-file $run_dir/$cluster-osd-check.log --cluster ceph --setuser ceph --setgroup ceph
[2021-12-11 19:33:01,434][osd2][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --log-file $run_dir/$cluster-osd-check.log --cluster ceph --setuser ceph --setgroup ceph
[2021-12-11 19:33:01,442][osd2][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --log-file $run_dir/$cluster-osd-check.log --cluster ceph --setuser ceph --setgroup ceph
[2021-12-11 19:33:01,458][osd2][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-11 19:33:01,458][osd2][WARNING] set_type: Will colocate journal with data on /dev/sdb
[2021-12-11 19:33:01,458][osd2][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2021-12-11 19:33:01,490][osd2][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-11 19:33:01,490][osd2][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-11 19:33:01,490][osd2][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-11 19:33:01,491][osd2][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[2021-12-11 19:33:01,491][osd2][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[2021-12-11 19:33:01,494][osd2][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[2021-12-11 19:33:01,510][osd2][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[2021-12-11 19:33:01,517][osd2][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-11 19:33:01,518][osd2][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-11 19:33:01,518][osd2][WARNING] ptype_tobe_for_name: name = journal
[2021-12-11 19:33:01,518][osd2][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-11 19:33:01,518][osd2][WARNING] create_partition: Creating journal partition num 2 size 5120 on /dev/sdb
[2021-12-11 19:33:01,518][osd2][WARNING] command_check_call: Running command: /sbin/sgdisk --new=2:0:+5120M --change-name=2:ceph journal --partition-guid=2:89429740-05bb-4b12-9b96-378e64f0f214 --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sdb
[2021-12-11 19:33:02,685][osd2][DEBUG ] Setting name!
[2021-12-11 19:33:02,685][osd2][DEBUG ] partNum is 1
[2021-12-11 19:33:02,685][osd2][DEBUG ] REALLY setting name!
[2021-12-11 19:33:02,685][osd2][DEBUG ] The operation has completed successfully.
[2021-12-11 19:33:02,685][osd2][WARNING] update_partition: Calling partprobe on created device /dev/sdb
[2021-12-11 19:33:02,686][osd2][WARNING] command_check_call: Running command: /usr/bin/udevadm settle --timeout=600
[2021-12-11 19:33:02,800][osd2][WARNING] command: Running command: /usr/bin/flock -s /dev/sdb /sbin/partprobe /dev/sdb
[2021-12-11 19:33:02,864][osd2][WARNING] command_check_call: Running command: /usr/bin/udevadm settle --timeout=600
[2021-12-11 19:33:02,978][osd2][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-11 19:33:02,978][osd2][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-11 19:33:02,978][osd2][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb2 uuid path is /sys/dev/block/8:18/dm/uuid
[2021-12-11 19:33:02,978][osd2][WARNING] prepare_device: Journal is GPT partition /dev/disk/by-partuuid/89429740-05bb-4b12-9b96-378e64f0f214
[2021-12-11 19:33:02,978][osd2][WARNING] prepare_device: Journal is GPT partition /dev/disk/by-partuuid/89429740-05bb-4b12-9b96-378e64f0f214
[2021-12-11 19:33:02,978][osd2][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-11 19:33:02,978][osd2][WARNING] set_data_partition: Creating osd partition on /dev/sdb
[2021-12-11 19:33:02,979][osd2][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-11 19:33:02,979][osd2][WARNING] ptype_tobe_for_name: name = data
[2021-12-11 19:33:02,979][osd2][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-11 19:33:02,979][osd2][WARNING] create_partition: Creating data partition num 1 size 0 on /dev/sdb
[2021-12-11 19:33:02,979][osd2][WARNING] command_check_call: Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:7376de08-6f49-4729-810d-3ed75594d70f --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be --mbrtogpt -- /dev/sdb
[2021-12-11 19:33:04,096][osd2][DEBUG ] Setting name!
[2021-12-11 19:33:04,096][osd2][DEBUG ] partNum is 0
[2021-12-11 19:33:04,096][osd2][DEBUG ] REALLY setting name!
[2021-12-11 19:33:04,096][osd2][DEBUG ] The operation has completed successfully.
[2021-12-11 19:33:04,096][osd2][WARNING] update_partition: Calling partprobe on created device /dev/sdb
[2021-12-11 19:33:04,096][osd2][WARNING] command_check_call: Running command: /usr/bin/udevadm settle --timeout=600
[2021-12-11 19:33:04,260][osd2][WARNING] command: Running command: /usr/bin/flock -s /dev/sdb /sbin/partprobe /dev/sdb
[2021-12-11 19:33:04,292][osd2][WARNING] command_check_call: Running command: /usr/bin/udevadm settle --timeout=600
[2021-12-11 19:33:04,457][osd2][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-11 19:33:04,457][osd2][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-11 19:33:04,457][osd2][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb1 uuid path is /sys/dev/block/8:17/dm/uuid
[2021-12-11 19:33:04,457][osd2][WARNING] populate_data_path_device: Creating xfs fs on /dev/sdb1
[2021-12-11 19:33:04,457][osd2][WARNING] command_check_call: Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/sdb1
[2021-12-11 19:33:04,672][osd2][DEBUG ] Discarding blocks...Done.
[2021-12-11 19:33:04,672][osd2][DEBUG ] meta-data=/dev/sdb1              isize=2048   agcount=4, agsize=196543 blks
[2021-12-11 19:33:04,672][osd2][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=1
[2021-12-11 19:33:04,672][osd2][DEBUG ]          =                       crc=1        finobt=0, sparse=0
[2021-12-11 19:33:04,672][osd2][DEBUG ] data     =                       bsize=4096   blocks=786171, imaxpct=25
[2021-12-11 19:33:04,672][osd2][DEBUG ]          =                       sunit=0      swidth=0 blks
[2021-12-11 19:33:04,672][osd2][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0 ftype=1
[2021-12-11 19:33:04,672][osd2][DEBUG ] log      =internal log           bsize=4096   blocks=2560, version=2
[2021-12-11 19:33:04,673][osd2][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[2021-12-11 19:33:04,673][osd2][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[2021-12-11 19:33:04,673][osd2][WARNING] mount: Mounting /dev/sdb1 on /var/lib/ceph/tmp/mnt.q7PfFL with options noatime,inode64
[2021-12-11 19:33:04,673][osd2][WARNING] command_check_call: Running command: /usr/bin/mount -t xfs -o noatime,inode64 -- /dev/sdb1 /var/lib/ceph/tmp/mnt.q7PfFL
[2021-12-11 19:33:04,676][osd2][WARNING] command: Running command: /sbin/restorecon /var/lib/ceph/tmp/mnt.q7PfFL
[2021-12-11 19:33:04,684][osd2][WARNING] populate_data_path: Preparing osd data dir /var/lib/ceph/tmp/mnt.q7PfFL
[2021-12-11 19:33:04,798][osd2][WARNING] command: Running command: /sbin/restorecon -R /var/lib/ceph/tmp/mnt.q7PfFL/ceph_fsid.1749.tmp
[2021-12-11 19:33:04,798][osd2][WARNING] command: Running command: /usr/bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.q7PfFL/ceph_fsid.1749.tmp
[2021-12-11 19:33:04,798][osd2][WARNING] command: Running command: /sbin/restorecon -R /var/lib/ceph/tmp/mnt.q7PfFL/fsid.1749.tmp
[2021-12-11 19:33:04,798][osd2][WARNING] command: Running command: /usr/bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.q7PfFL/fsid.1749.tmp
[2021-12-11 19:33:04,798][osd2][WARNING] command: Running command: /sbin/restorecon -R /var/lib/ceph/tmp/mnt.q7PfFL/magic.1749.tmp
[2021-12-11 19:33:04,798][osd2][WARNING] command: Running command: /usr/bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.q7PfFL/magic.1749.tmp
[2021-12-11 19:33:04,814][osd2][WARNING] command: Running command: /sbin/restorecon -R /var/lib/ceph/tmp/mnt.q7PfFL/journal_uuid.1749.tmp
[2021-12-11 19:33:04,817][osd2][WARNING] command: Running command: /usr/bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.q7PfFL/journal_uuid.1749.tmp
[2021-12-11 19:33:04,821][osd2][WARNING] adjust_symlink: Creating symlink /var/lib/ceph/tmp/mnt.q7PfFL/journal -> /dev/disk/by-partuuid/89429740-05bb-4b12-9b96-378e64f0f214
[2021-12-11 19:33:04,821][osd2][WARNING] command: Running command: /sbin/restorecon -R /var/lib/ceph/tmp/mnt.q7PfFL
[2021-12-11 19:33:04,824][osd2][WARNING] command: Running command: /usr/bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.q7PfFL
[2021-12-11 19:33:04,828][osd2][WARNING] unmount: Unmounting /var/lib/ceph/tmp/mnt.q7PfFL
[2021-12-11 19:33:04,828][osd2][WARNING] command_check_call: Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.q7PfFL
[2021-12-11 19:33:04,943][osd2][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-11 19:33:04,943][osd2][WARNING] command_check_call: Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/sdb
[2021-12-11 19:33:06,060][osd2][DEBUG ] Warning: The kernel is still using the old partition table.
[2021-12-11 19:33:06,061][osd2][DEBUG ] The new table will be used at the next reboot.
[2021-12-11 19:33:06,061][osd2][DEBUG ] The operation has completed successfully.
[2021-12-11 19:33:06,061][osd2][WARNING] update_partition: Calling partprobe on prepared device /dev/sdb
[2021-12-11 19:33:06,061][osd2][WARNING] command_check_call: Running command: /usr/bin/udevadm settle --timeout=600
[2021-12-11 19:33:06,061][osd2][WARNING] command: Running command: /usr/bin/flock -s /dev/sdb /sbin/partprobe /dev/sdb
[2021-12-11 19:33:06,061][osd2][WARNING] command_check_call: Running command: /usr/bin/udevadm settle --timeout=600
[2021-12-11 19:33:06,125][osd2][WARNING] command_check_call: Running command: /usr/bin/udevadm trigger --action=add --sysname-match sdb1
[2021-12-11 19:33:11,130][osd2][INFO  ] checking OSD status...
[2021-12-11 19:33:11,131][osd2][DEBUG ] find the location of an executable
[2021-12-11 19:33:11,133][osd2][INFO  ] Running command: sudo /bin/ceph --cluster=ceph osd stat --format=json
[2021-12-11 19:33:11,348][osd2][WARNING] there are 4 OSDs down
[2021-12-11 19:33:11,348][osd2][WARNING] there are 2 OSDs out
[2021-12-11 19:33:11,349][ceph_deploy.osd][DEBUG ] Host osd2 is now ready for osd use.
[2021-12-11 19:33:11,505][osd3][DEBUG ] connection detected need for sudo
[2021-12-11 19:33:11,677][osd3][DEBUG ] connected to host: osd3 
[2021-12-11 19:33:11,677][osd3][DEBUG ] detect platform information from remote host
[2021-12-11 19:33:11,695][osd3][DEBUG ] detect machine type
[2021-12-11 19:33:11,700][osd3][DEBUG ] find the location of an executable
[2021-12-11 19:33:11,702][ceph_deploy.osd][INFO  ] Distro info: CentOS Linux 7.9.2009 Core
[2021-12-11 19:33:11,702][ceph_deploy.osd][DEBUG ] Deploying osd to osd3
[2021-12-11 19:33:11,702][osd3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2021-12-11 19:33:11,704][ceph_deploy.osd][DEBUG ] Preparing host osd3 disk /dev/sdb journal None activate False
[2021-12-11 19:33:11,704][osd3][DEBUG ] find the location of an executable
[2021-12-11 19:33:11,707][osd3][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdb
[2021-12-11 19:33:11,774][osd3][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2021-12-11 19:33:11,790][osd3][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --log-file $run_dir/$cluster-osd-check.log --cluster ceph --setuser ceph --setgroup ceph
[2021-12-11 19:33:11,806][osd3][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --log-file $run_dir/$cluster-osd-check.log --cluster ceph --setuser ceph --setgroup ceph
[2021-12-11 19:33:11,821][osd3][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --log-file $run_dir/$cluster-osd-check.log --cluster ceph --setuser ceph --setgroup ceph
[2021-12-11 19:33:11,837][osd3][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-11 19:33:11,837][osd3][WARNING] set_type: Will colocate journal with data on /dev/sdb
[2021-12-11 19:33:11,837][osd3][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2021-12-11 19:33:11,853][osd3][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-11 19:33:11,853][osd3][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-11 19:33:11,853][osd3][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-11 19:33:11,853][osd3][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[2021-12-11 19:33:11,856][osd3][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[2021-12-11 19:33:11,872][osd3][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[2021-12-11 19:33:11,873][osd3][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[2021-12-11 19:33:11,889][osd3][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-11 19:33:11,889][osd3][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-11 19:33:11,889][osd3][WARNING] ptype_tobe_for_name: name = journal
[2021-12-11 19:33:11,889][osd3][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-11 19:33:11,889][osd3][WARNING] create_partition: Creating journal partition num 2 size 5120 on /dev/sdb
[2021-12-11 19:33:11,889][osd3][WARNING] command_check_call: Running command: /sbin/sgdisk --new=2:0:+5120M --change-name=2:ceph journal --partition-guid=2:5c554a13-7606-4349-baad-6206dc6250c9 --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sdb
[2021-12-11 19:33:13,006][osd3][DEBUG ] Setting name!
[2021-12-11 19:33:13,006][osd3][DEBUG ] partNum is 1
[2021-12-11 19:33:13,006][osd3][DEBUG ] REALLY setting name!
[2021-12-11 19:33:13,006][osd3][DEBUG ] The operation has completed successfully.
[2021-12-11 19:33:13,006][osd3][WARNING] update_partition: Calling partprobe on created device /dev/sdb
[2021-12-11 19:33:13,006][osd3][WARNING] command_check_call: Running command: /usr/bin/udevadm settle --timeout=600
[2021-12-11 19:33:13,120][osd3][WARNING] command: Running command: /usr/bin/flock -s /dev/sdb /sbin/partprobe /dev/sdb
[2021-12-11 19:33:13,136][osd3][WARNING] command_check_call: Running command: /usr/bin/udevadm settle --timeout=600
[2021-12-11 19:33:13,250][osd3][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-11 19:33:13,251][osd3][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-11 19:33:13,251][osd3][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb2 uuid path is /sys/dev/block/8:18/dm/uuid
[2021-12-11 19:33:13,251][osd3][WARNING] prepare_device: Journal is GPT partition /dev/disk/by-partuuid/5c554a13-7606-4349-baad-6206dc6250c9
[2021-12-11 19:33:13,251][osd3][WARNING] prepare_device: Journal is GPT partition /dev/disk/by-partuuid/5c554a13-7606-4349-baad-6206dc6250c9
[2021-12-11 19:33:13,251][osd3][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-11 19:33:13,251][osd3][WARNING] set_data_partition: Creating osd partition on /dev/sdb
[2021-12-11 19:33:13,251][osd3][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-11 19:33:13,251][osd3][WARNING] ptype_tobe_for_name: name = data
[2021-12-11 19:33:13,251][osd3][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-11 19:33:13,251][osd3][WARNING] create_partition: Creating data partition num 1 size 0 on /dev/sdb
[2021-12-11 19:33:13,251][osd3][WARNING] command_check_call: Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:dd9bd91e-401a-4580-b83c-79cb73315452 --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be --mbrtogpt -- /dev/sdb
[2021-12-11 19:33:14,418][osd3][DEBUG ] Setting name!
[2021-12-11 19:33:14,418][osd3][DEBUG ] partNum is 0
[2021-12-11 19:33:14,418][osd3][DEBUG ] REALLY setting name!
[2021-12-11 19:33:14,418][osd3][DEBUG ] The operation has completed successfully.
[2021-12-11 19:33:14,419][osd3][WARNING] update_partition: Calling partprobe on created device /dev/sdb
[2021-12-11 19:33:14,419][osd3][WARNING] command_check_call: Running command: /usr/bin/udevadm settle --timeout=600
[2021-12-11 19:33:14,533][osd3][WARNING] command: Running command: /usr/bin/flock -s /dev/sdb /sbin/partprobe /dev/sdb
[2021-12-11 19:33:14,564][osd3][WARNING] command_check_call: Running command: /usr/bin/udevadm settle --timeout=600
[2021-12-11 19:33:14,729][osd3][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-11 19:33:14,729][osd3][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-11 19:33:14,729][osd3][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb1 uuid path is /sys/dev/block/8:17/dm/uuid
[2021-12-11 19:33:14,729][osd3][WARNING] populate_data_path_device: Creating xfs fs on /dev/sdb1
[2021-12-11 19:33:14,729][osd3][WARNING] command_check_call: Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/sdb1
[2021-12-11 19:33:14,994][osd3][DEBUG ] Discarding blocks...Done.
[2021-12-11 19:33:14,994][osd3][DEBUG ] meta-data=/dev/sdb1              isize=2048   agcount=4, agsize=196543 blks
[2021-12-11 19:33:14,994][osd3][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=1
[2021-12-11 19:33:14,994][osd3][DEBUG ]          =                       crc=1        finobt=0, sparse=0
[2021-12-11 19:33:14,994][osd3][DEBUG ] data     =                       bsize=4096   blocks=786171, imaxpct=25
[2021-12-11 19:33:14,994][osd3][DEBUG ]          =                       sunit=0      swidth=0 blks
[2021-12-11 19:33:14,994][osd3][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0 ftype=1
[2021-12-11 19:33:14,994][osd3][DEBUG ] log      =internal log           bsize=4096   blocks=2560, version=2
[2021-12-11 19:33:14,994][osd3][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[2021-12-11 19:33:14,994][osd3][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[2021-12-11 19:33:14,994][osd3][WARNING] mount: Mounting /dev/sdb1 on /var/lib/ceph/tmp/mnt.yNQa8Y with options noatime,inode64
[2021-12-11 19:33:14,994][osd3][WARNING] command_check_call: Running command: /usr/bin/mount -t xfs -o noatime,inode64 -- /dev/sdb1 /var/lib/ceph/tmp/mnt.yNQa8Y
[2021-12-11 19:33:14,995][osd3][WARNING] command: Running command: /sbin/restorecon /var/lib/ceph/tmp/mnt.yNQa8Y
[2021-12-11 19:33:14,995][osd3][WARNING] populate_data_path: Preparing osd data dir /var/lib/ceph/tmp/mnt.yNQa8Y
[2021-12-11 19:33:15,109][osd3][WARNING] command: Running command: /sbin/restorecon -R /var/lib/ceph/tmp/mnt.yNQa8Y/ceph_fsid.1442.tmp
[2021-12-11 19:33:15,109][osd3][WARNING] command: Running command: /usr/bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.yNQa8Y/ceph_fsid.1442.tmp
[2021-12-11 19:33:15,109][osd3][WARNING] command: Running command: /sbin/restorecon -R /var/lib/ceph/tmp/mnt.yNQa8Y/fsid.1442.tmp
[2021-12-11 19:33:15,109][osd3][WARNING] command: Running command: /usr/bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.yNQa8Y/fsid.1442.tmp
[2021-12-11 19:33:15,116][osd3][WARNING] command: Running command: /sbin/restorecon -R /var/lib/ceph/tmp/mnt.yNQa8Y/magic.1442.tmp
[2021-12-11 19:33:15,120][osd3][WARNING] command: Running command: /usr/bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.yNQa8Y/magic.1442.tmp
[2021-12-11 19:33:15,152][osd3][WARNING] command: Running command: /sbin/restorecon -R /var/lib/ceph/tmp/mnt.yNQa8Y/journal_uuid.1442.tmp
[2021-12-11 19:33:15,152][osd3][WARNING] command: Running command: /usr/bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.yNQa8Y/journal_uuid.1442.tmp
[2021-12-11 19:33:15,152][osd3][WARNING] adjust_symlink: Creating symlink /var/lib/ceph/tmp/mnt.yNQa8Y/journal -> /dev/disk/by-partuuid/5c554a13-7606-4349-baad-6206dc6250c9
[2021-12-11 19:33:15,152][osd3][WARNING] command: Running command: /sbin/restorecon -R /var/lib/ceph/tmp/mnt.yNQa8Y
[2021-12-11 19:33:15,152][osd3][WARNING] command: Running command: /usr/bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.yNQa8Y
[2021-12-11 19:33:15,152][osd3][WARNING] unmount: Unmounting /var/lib/ceph/tmp/mnt.yNQa8Y
[2021-12-11 19:33:15,152][osd3][WARNING] command_check_call: Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.yNQa8Y
[2021-12-11 19:33:15,266][osd3][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-11 19:33:15,266][osd3][WARNING] command_check_call: Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/sdb
[2021-12-11 19:33:16,434][osd3][DEBUG ] Warning: The kernel is still using the old partition table.
[2021-12-11 19:33:16,434][osd3][DEBUG ] The new table will be used at the next reboot.
[2021-12-11 19:33:16,434][osd3][DEBUG ] The operation has completed successfully.
[2021-12-11 19:33:16,434][osd3][WARNING] update_partition: Calling partprobe on prepared device /dev/sdb
[2021-12-11 19:33:16,434][osd3][WARNING] command_check_call: Running command: /usr/bin/udevadm settle --timeout=600
[2021-12-11 19:33:16,442][osd3][WARNING] command: Running command: /usr/bin/flock -s /dev/sdb /sbin/partprobe /dev/sdb
[2021-12-11 19:33:16,457][osd3][WARNING] command_check_call: Running command: /usr/bin/udevadm settle --timeout=600
[2021-12-11 19:33:16,571][osd3][WARNING] command_check_call: Running command: /usr/bin/udevadm trigger --action=add --sysname-match sdb1
[2021-12-11 19:33:21,577][osd3][INFO  ] checking OSD status...
[2021-12-11 19:33:21,577][osd3][DEBUG ] find the location of an executable
[2021-12-11 19:33:21,580][osd3][INFO  ] Running command: sudo /bin/ceph --cluster=ceph osd stat --format=json
[2021-12-11 19:33:21,745][osd3][WARNING] there are 4 OSDs down
[2021-12-11 19:33:21,745][osd3][WARNING] there are 3 OSDs out
[2021-12-11 19:33:21,745][ceph_deploy.osd][DEBUG ] Host osd3 is now ready for osd use.
[2021-12-11 19:33:21,902][osd4][DEBUG ] connection detected need for sudo
[2021-12-11 19:33:22,073][osd4][DEBUG ] connected to host: osd4 
[2021-12-11 19:33:22,074][osd4][DEBUG ] detect platform information from remote host
[2021-12-11 19:33:22,091][osd4][DEBUG ] detect machine type
[2021-12-11 19:33:22,096][osd4][DEBUG ] find the location of an executable
[2021-12-11 19:33:22,097][ceph_deploy.osd][INFO  ] Distro info: CentOS Linux 7.9.2009 Core
[2021-12-11 19:33:22,097][ceph_deploy.osd][DEBUG ] Deploying osd to osd4
[2021-12-11 19:33:22,097][osd4][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2021-12-11 19:33:22,099][ceph_deploy.osd][DEBUG ] Preparing host osd4 disk /dev/sdb journal None activate False
[2021-12-11 19:33:22,099][osd4][DEBUG ] find the location of an executable
[2021-12-11 19:33:22,102][osd4][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdb
[2021-12-11 19:33:22,168][osd4][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2021-12-11 19:33:22,200][osd4][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --log-file $run_dir/$cluster-osd-check.log --cluster ceph --setuser ceph --setgroup ceph
[2021-12-11 19:33:22,203][osd4][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --log-file $run_dir/$cluster-osd-check.log --cluster ceph --setuser ceph --setgroup ceph
[2021-12-11 19:33:22,235][osd4][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --log-file $run_dir/$cluster-osd-check.log --cluster ceph --setuser ceph --setgroup ceph
[2021-12-11 19:33:22,238][osd4][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-11 19:33:22,239][osd4][WARNING] set_type: Will colocate journal with data on /dev/sdb
[2021-12-11 19:33:22,239][osd4][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2021-12-11 19:33:22,270][osd4][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-11 19:33:22,270][osd4][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-11 19:33:22,271][osd4][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-11 19:33:22,271][osd4][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[2021-12-11 19:33:22,271][osd4][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[2021-12-11 19:33:22,274][osd4][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[2021-12-11 19:33:22,281][osd4][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[2021-12-11 19:33:22,297][osd4][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-11 19:33:22,297][osd4][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-11 19:33:22,297][osd4][WARNING] ptype_tobe_for_name: name = journal
[2021-12-11 19:33:22,297][osd4][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-11 19:33:22,297][osd4][WARNING] create_partition: Creating journal partition num 2 size 5120 on /dev/sdb
[2021-12-11 19:33:22,297][osd4][WARNING] command_check_call: Running command: /sbin/sgdisk --new=2:0:+5120M --change-name=2:ceph journal --partition-guid=2:4ae7435c-49e5-4025-8df2-451e9228d760 --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sdb
[2021-12-11 19:33:23,415][osd4][DEBUG ] Setting name!
[2021-12-11 19:33:23,415][osd4][DEBUG ] partNum is 1
[2021-12-11 19:33:23,415][osd4][DEBUG ] REALLY setting name!
[2021-12-11 19:33:23,415][osd4][DEBUG ] The operation has completed successfully.
[2021-12-11 19:33:23,415][osd4][WARNING] update_partition: Calling partprobe on created device /dev/sdb
[2021-12-11 19:33:23,416][osd4][WARNING] command_check_call: Running command: /usr/bin/udevadm settle --timeout=600
[2021-12-11 19:33:23,530][osd4][WARNING] command: Running command: /usr/bin/flock -s /dev/sdb /sbin/partprobe /dev/sdb
[2021-12-11 19:33:23,644][osd4][WARNING] command_check_call: Running command: /usr/bin/udevadm settle --timeout=600
[2021-12-11 19:33:23,708][osd4][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-11 19:33:23,708][osd4][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-11 19:33:23,708][osd4][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb2 uuid path is /sys/dev/block/8:18/dm/uuid
[2021-12-11 19:33:23,708][osd4][WARNING] prepare_device: Journal is GPT partition /dev/disk/by-partuuid/4ae7435c-49e5-4025-8df2-451e9228d760
[2021-12-11 19:33:23,708][osd4][WARNING] prepare_device: Journal is GPT partition /dev/disk/by-partuuid/4ae7435c-49e5-4025-8df2-451e9228d760
[2021-12-11 19:33:23,708][osd4][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-11 19:33:23,708][osd4][WARNING] set_data_partition: Creating osd partition on /dev/sdb
[2021-12-11 19:33:23,708][osd4][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-11 19:33:23,709][osd4][WARNING] ptype_tobe_for_name: name = data
[2021-12-11 19:33:23,709][osd4][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-11 19:33:23,709][osd4][WARNING] create_partition: Creating data partition num 1 size 0 on /dev/sdb
[2021-12-11 19:33:23,709][osd4][WARNING] command_check_call: Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:45e55bba-e7b0-469a-b36d-b40f044a0dd4 --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be --mbrtogpt -- /dev/sdb
[2021-12-11 19:33:24,876][osd4][DEBUG ] Setting name!
[2021-12-11 19:33:24,876][osd4][DEBUG ] partNum is 0
[2021-12-11 19:33:24,876][osd4][DEBUG ] REALLY setting name!
[2021-12-11 19:33:24,876][osd4][DEBUG ] The operation has completed successfully.
[2021-12-11 19:33:24,876][osd4][WARNING] update_partition: Calling partprobe on created device /dev/sdb
[2021-12-11 19:33:24,876][osd4][WARNING] command_check_call: Running command: /usr/bin/udevadm settle --timeout=600
[2021-12-11 19:33:24,990][osd4][WARNING] command: Running command: /usr/bin/flock -s /dev/sdb /sbin/partprobe /dev/sdb
[2021-12-11 19:33:25,054][osd4][WARNING] command_check_call: Running command: /usr/bin/udevadm settle --timeout=600
[2021-12-11 19:33:25,219][osd4][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-11 19:33:25,219][osd4][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-11 19:33:25,219][osd4][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb1 uuid path is /sys/dev/block/8:17/dm/uuid
[2021-12-11 19:33:25,219][osd4][WARNING] populate_data_path_device: Creating xfs fs on /dev/sdb1
[2021-12-11 19:33:25,219][osd4][WARNING] command_check_call: Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/sdb1
[2021-12-11 19:33:25,484][osd4][DEBUG ] Discarding blocks...Done.
[2021-12-11 19:33:25,484][osd4][DEBUG ] meta-data=/dev/sdb1              isize=2048   agcount=4, agsize=196543 blks
[2021-12-11 19:33:25,484][osd4][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=1
[2021-12-11 19:33:25,484][osd4][DEBUG ]          =                       crc=1        finobt=0, sparse=0
[2021-12-11 19:33:25,484][osd4][DEBUG ] data     =                       bsize=4096   blocks=786171, imaxpct=25
[2021-12-11 19:33:25,484][osd4][DEBUG ]          =                       sunit=0      swidth=0 blks
[2021-12-11 19:33:25,484][osd4][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0 ftype=1
[2021-12-11 19:33:25,484][osd4][DEBUG ] log      =internal log           bsize=4096   blocks=2560, version=2
[2021-12-11 19:33:25,484][osd4][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[2021-12-11 19:33:25,485][osd4][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[2021-12-11 19:33:25,485][osd4][WARNING] mount: Mounting /dev/sdb1 on /var/lib/ceph/tmp/mnt.7HF_XQ with options noatime,inode64
[2021-12-11 19:33:25,485][osd4][WARNING] command_check_call: Running command: /usr/bin/mount -t xfs -o noatime,inode64 -- /dev/sdb1 /var/lib/ceph/tmp/mnt.7HF_XQ
[2021-12-11 19:33:25,485][osd4][WARNING] command: Running command: /sbin/restorecon /var/lib/ceph/tmp/mnt.7HF_XQ
[2021-12-11 19:33:25,486][osd4][WARNING] populate_data_path: Preparing osd data dir /var/lib/ceph/tmp/mnt.7HF_XQ
[2021-12-11 19:33:25,600][osd4][WARNING] command: Running command: /sbin/restorecon -R /var/lib/ceph/tmp/mnt.7HF_XQ/ceph_fsid.1440.tmp
[2021-12-11 19:33:25,600][osd4][WARNING] command: Running command: /usr/bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.7HF_XQ/ceph_fsid.1440.tmp
[2021-12-11 19:33:25,600][osd4][WARNING] command: Running command: /sbin/restorecon -R /var/lib/ceph/tmp/mnt.7HF_XQ/fsid.1440.tmp
[2021-12-11 19:33:25,600][osd4][WARNING] command: Running command: /usr/bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.7HF_XQ/fsid.1440.tmp
[2021-12-11 19:33:25,616][osd4][WARNING] command: Running command: /sbin/restorecon -R /var/lib/ceph/tmp/mnt.7HF_XQ/magic.1440.tmp
[2021-12-11 19:33:25,616][osd4][WARNING] command: Running command: /usr/bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.7HF_XQ/magic.1440.tmp
[2021-12-11 19:33:25,631][osd4][WARNING] command: Running command: /sbin/restorecon -R /var/lib/ceph/tmp/mnt.7HF_XQ/journal_uuid.1440.tmp
[2021-12-11 19:33:25,635][osd4][WARNING] command: Running command: /usr/bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.7HF_XQ/journal_uuid.1440.tmp
[2021-12-11 19:33:25,638][osd4][WARNING] adjust_symlink: Creating symlink /var/lib/ceph/tmp/mnt.7HF_XQ/journal -> /dev/disk/by-partuuid/4ae7435c-49e5-4025-8df2-451e9228d760
[2021-12-11 19:33:25,638][osd4][WARNING] command: Running command: /sbin/restorecon -R /var/lib/ceph/tmp/mnt.7HF_XQ
[2021-12-11 19:33:25,642][osd4][WARNING] command: Running command: /usr/bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.7HF_XQ
[2021-12-11 19:33:25,643][osd4][WARNING] unmount: Unmounting /var/lib/ceph/tmp/mnt.7HF_XQ
[2021-12-11 19:33:25,643][osd4][WARNING] command_check_call: Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.7HF_XQ
[2021-12-11 19:33:25,757][osd4][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-11 19:33:25,757][osd4][WARNING] command_check_call: Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/sdb
[2021-12-11 19:33:26,924][osd4][DEBUG ] Warning: The kernel is still using the old partition table.
[2021-12-11 19:33:26,924][osd4][DEBUG ] The new table will be used at the next reboot.
[2021-12-11 19:33:26,925][osd4][DEBUG ] The operation has completed successfully.
[2021-12-11 19:33:26,925][osd4][WARNING] update_partition: Calling partprobe on prepared device /dev/sdb
[2021-12-11 19:33:26,925][osd4][WARNING] command_check_call: Running command: /usr/bin/udevadm settle --timeout=600
[2021-12-11 19:33:26,925][osd4][WARNING] command: Running command: /usr/bin/flock -s /dev/sdb /sbin/partprobe /dev/sdb
[2021-12-11 19:33:27,039][osd4][WARNING] command_check_call: Running command: /usr/bin/udevadm settle --timeout=600
[2021-12-11 19:33:27,071][osd4][WARNING] command_check_call: Running command: /usr/bin/udevadm trigger --action=add --sysname-match sdb1
[2021-12-11 19:33:32,076][osd4][INFO  ] checking OSD status...
[2021-12-11 19:33:32,076][osd4][DEBUG ] find the location of an executable
[2021-12-11 19:33:32,079][osd4][INFO  ] Running command: sudo /bin/ceph --cluster=ceph osd stat --format=json
[2021-12-11 19:33:32,295][osd4][WARNING] there are 4 OSDs down
[2021-12-11 19:33:32,295][osd4][WARNING] there are 3 OSDs out
[2021-12-11 19:33:32,295][ceph_deploy.osd][DEBUG ] Host osd4 is now ready for osd use.
[2021-12-11 19:33:32,446][osd5][DEBUG ] connection detected need for sudo
[2021-12-11 19:33:32,612][osd5][DEBUG ] connected to host: osd5 
[2021-12-11 19:33:32,613][osd5][DEBUG ] detect platform information from remote host
[2021-12-11 19:33:32,630][osd5][DEBUG ] detect machine type
[2021-12-11 19:33:32,635][osd5][DEBUG ] find the location of an executable
[2021-12-11 19:33:32,636][ceph_deploy.osd][INFO  ] Distro info: CentOS Linux 7.9.2009 Core
[2021-12-11 19:33:32,636][ceph_deploy.osd][DEBUG ] Deploying osd to osd5
[2021-12-11 19:33:32,636][osd5][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2021-12-11 19:33:32,653][ceph_deploy.osd][DEBUG ] Preparing host osd5 disk /dev/sdb journal None activate False
[2021-12-11 19:33:32,653][osd5][DEBUG ] find the location of an executable
[2021-12-11 19:33:32,656][osd5][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdb
[2021-12-11 19:33:32,722][osd5][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2021-12-11 19:33:32,738][osd5][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --log-file $run_dir/$cluster-osd-check.log --cluster ceph --setuser ceph --setgroup ceph
[2021-12-11 19:33:32,753][osd5][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --log-file $run_dir/$cluster-osd-check.log --cluster ceph --setuser ceph --setgroup ceph
[2021-12-11 19:33:32,769][osd5][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --log-file $run_dir/$cluster-osd-check.log --cluster ceph --setuser ceph --setgroup ceph
[2021-12-11 19:33:32,785][osd5][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-11 19:33:32,785][osd5][WARNING] set_type: Will colocate journal with data on /dev/sdb
[2021-12-11 19:33:32,785][osd5][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2021-12-11 19:33:32,800][osd5][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-11 19:33:32,800][osd5][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-11 19:33:32,800][osd5][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-11 19:33:32,801][osd5][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[2021-12-11 19:33:32,804][osd5][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[2021-12-11 19:33:32,811][osd5][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[2021-12-11 19:33:32,819][osd5][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[2021-12-11 19:33:32,834][osd5][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-11 19:33:32,834][osd5][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-11 19:33:32,834][osd5][WARNING] ptype_tobe_for_name: name = journal
[2021-12-11 19:33:32,835][osd5][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-11 19:33:32,835][osd5][WARNING] create_partition: Creating journal partition num 2 size 5120 on /dev/sdb
[2021-12-11 19:33:32,835][osd5][WARNING] command_check_call: Running command: /sbin/sgdisk --new=2:0:+5120M --change-name=2:ceph journal --partition-guid=2:8c103072-9984-4210-a4fb-648013c22398 --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sdb
[2021-12-11 19:33:33,902][osd5][DEBUG ] Setting name!
[2021-12-11 19:33:33,902][osd5][DEBUG ] partNum is 1
[2021-12-11 19:33:33,902][osd5][DEBUG ] REALLY setting name!
[2021-12-11 19:33:33,902][osd5][DEBUG ] The operation has completed successfully.
[2021-12-11 19:33:33,905][osd5][WARNING] update_partition: Calling partprobe on created device /dev/sdb
[2021-12-11 19:33:33,906][osd5][WARNING] command_check_call: Running command: /usr/bin/udevadm settle --timeout=600
[2021-12-11 19:33:34,019][osd5][WARNING] command: Running command: /usr/bin/flock -s /dev/sdb /sbin/partprobe /dev/sdb
[2021-12-11 19:33:34,083][osd5][WARNING] command_check_call: Running command: /usr/bin/udevadm settle --timeout=600
[2021-12-11 19:33:34,147][osd5][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-11 19:33:34,147][osd5][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-11 19:33:34,148][osd5][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb2 uuid path is /sys/dev/block/8:18/dm/uuid
[2021-12-11 19:33:34,148][osd5][WARNING] prepare_device: Journal is GPT partition /dev/disk/by-partuuid/8c103072-9984-4210-a4fb-648013c22398
[2021-12-11 19:33:34,148][osd5][WARNING] prepare_device: Journal is GPT partition /dev/disk/by-partuuid/8c103072-9984-4210-a4fb-648013c22398
[2021-12-11 19:33:34,148][osd5][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-11 19:33:34,148][osd5][WARNING] set_data_partition: Creating osd partition on /dev/sdb
[2021-12-11 19:33:34,148][osd5][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-11 19:33:34,148][osd5][WARNING] ptype_tobe_for_name: name = data
[2021-12-11 19:33:34,148][osd5][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-11 19:33:34,148][osd5][WARNING] create_partition: Creating data partition num 1 size 0 on /dev/sdb
[2021-12-11 19:33:34,148][osd5][WARNING] command_check_call: Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:bc4934bf-ec83-4e1b-9873-680e933986ff --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be --mbrtogpt -- /dev/sdb
[2021-12-11 19:33:35,465][osd5][DEBUG ] Setting name!
[2021-12-11 19:33:35,466][osd5][DEBUG ] partNum is 0
[2021-12-11 19:33:35,466][osd5][DEBUG ] REALLY setting name!
[2021-12-11 19:33:35,466][osd5][DEBUG ] The operation has completed successfully.
[2021-12-11 19:33:35,466][osd5][WARNING] update_partition: Calling partprobe on created device /dev/sdb
[2021-12-11 19:33:35,466][osd5][WARNING] command_check_call: Running command: /usr/bin/udevadm settle --timeout=600
[2021-12-11 19:33:35,580][osd5][WARNING] command: Running command: /usr/bin/flock -s /dev/sdb /sbin/partprobe /dev/sdb
[2021-12-11 19:33:35,612][osd5][WARNING] command_check_call: Running command: /usr/bin/udevadm settle --timeout=600
[2021-12-11 19:33:35,776][osd5][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-11 19:33:35,777][osd5][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-11 19:33:35,777][osd5][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb1 uuid path is /sys/dev/block/8:17/dm/uuid
[2021-12-11 19:33:35,777][osd5][WARNING] populate_data_path_device: Creating xfs fs on /dev/sdb1
[2021-12-11 19:33:35,777][osd5][WARNING] command_check_call: Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/sdb1
[2021-12-11 19:33:35,991][osd5][DEBUG ] Discarding blocks...Done.
[2021-12-11 19:33:35,991][osd5][DEBUG ] meta-data=/dev/sdb1              isize=2048   agcount=4, agsize=196543 blks
[2021-12-11 19:33:35,991][osd5][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=1
[2021-12-11 19:33:35,991][osd5][DEBUG ]          =                       crc=1        finobt=0, sparse=0
[2021-12-11 19:33:35,991][osd5][DEBUG ] data     =                       bsize=4096   blocks=786171, imaxpct=25
[2021-12-11 19:33:35,991][osd5][DEBUG ]          =                       sunit=0      swidth=0 blks
[2021-12-11 19:33:35,991][osd5][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0 ftype=1
[2021-12-11 19:33:35,992][osd5][DEBUG ] log      =internal log           bsize=4096   blocks=2560, version=2
[2021-12-11 19:33:35,992][osd5][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[2021-12-11 19:33:35,992][osd5][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[2021-12-11 19:33:35,992][osd5][WARNING] mount: Mounting /dev/sdb1 on /var/lib/ceph/tmp/mnt.Lc7Y95 with options noatime,inode64
[2021-12-11 19:33:35,992][osd5][WARNING] command_check_call: Running command: /usr/bin/mount -t xfs -o noatime,inode64 -- /dev/sdb1 /var/lib/ceph/tmp/mnt.Lc7Y95
[2021-12-11 19:33:35,992][osd5][WARNING] command: Running command: /sbin/restorecon /var/lib/ceph/tmp/mnt.Lc7Y95
[2021-12-11 19:33:35,992][osd5][WARNING] populate_data_path: Preparing osd data dir /var/lib/ceph/tmp/mnt.Lc7Y95
[2021-12-11 19:33:36,056][osd5][WARNING] command: Running command: /sbin/restorecon -R /var/lib/ceph/tmp/mnt.Lc7Y95/ceph_fsid.1658.tmp
[2021-12-11 19:33:36,056][osd5][WARNING] command: Running command: /usr/bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.Lc7Y95/ceph_fsid.1658.tmp
[2021-12-11 19:33:36,063][osd5][WARNING] command: Running command: /sbin/restorecon -R /var/lib/ceph/tmp/mnt.Lc7Y95/fsid.1658.tmp
[2021-12-11 19:33:36,065][osd5][WARNING] command: Running command: /usr/bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.Lc7Y95/fsid.1658.tmp
[2021-12-11 19:33:36,096][osd5][WARNING] command: Running command: /sbin/restorecon -R /var/lib/ceph/tmp/mnt.Lc7Y95/magic.1658.tmp
[2021-12-11 19:33:36,096][osd5][WARNING] command: Running command: /usr/bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.Lc7Y95/magic.1658.tmp
[2021-12-11 19:33:36,104][osd5][WARNING] command: Running command: /sbin/restorecon -R /var/lib/ceph/tmp/mnt.Lc7Y95/journal_uuid.1658.tmp
[2021-12-11 19:33:36,107][osd5][WARNING] command: Running command: /usr/bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.Lc7Y95/journal_uuid.1658.tmp
[2021-12-11 19:33:36,108][osd5][WARNING] adjust_symlink: Creating symlink /var/lib/ceph/tmp/mnt.Lc7Y95/journal -> /dev/disk/by-partuuid/8c103072-9984-4210-a4fb-648013c22398
[2021-12-11 19:33:36,108][osd5][WARNING] command: Running command: /sbin/restorecon -R /var/lib/ceph/tmp/mnt.Lc7Y95
[2021-12-11 19:33:36,112][osd5][WARNING] command: Running command: /usr/bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.Lc7Y95
[2021-12-11 19:33:36,113][osd5][WARNING] unmount: Unmounting /var/lib/ceph/tmp/mnt.Lc7Y95
[2021-12-11 19:33:36,113][osd5][WARNING] command_check_call: Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.Lc7Y95
[2021-12-11 19:33:36,227][osd5][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-11 19:33:36,227][osd5][WARNING] command_check_call: Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/sdb
[2021-12-11 19:33:37,394][osd5][DEBUG ] Warning: The kernel is still using the old partition table.
[2021-12-11 19:33:37,394][osd5][DEBUG ] The new table will be used at the next reboot.
[2021-12-11 19:33:37,394][osd5][DEBUG ] The operation has completed successfully.
[2021-12-11 19:33:37,394][osd5][WARNING] update_partition: Calling partprobe on prepared device /dev/sdb
[2021-12-11 19:33:37,394][osd5][WARNING] command_check_call: Running command: /usr/bin/udevadm settle --timeout=600
[2021-12-11 19:33:37,398][osd5][WARNING] command: Running command: /usr/bin/flock -s /dev/sdb /sbin/partprobe /dev/sdb
[2021-12-11 19:33:37,414][osd5][WARNING] command_check_call: Running command: /usr/bin/udevadm settle --timeout=600
[2021-12-11 19:33:37,478][osd5][WARNING] command_check_call: Running command: /usr/bin/udevadm trigger --action=add --sysname-match sdb1
[2021-12-11 19:33:42,483][osd5][INFO  ] checking OSD status...
[2021-12-11 19:33:42,483][osd5][DEBUG ] find the location of an executable
[2021-12-11 19:33:42,486][osd5][INFO  ] Running command: sudo /bin/ceph --cluster=ceph osd stat --format=json
[2021-12-11 19:33:42,651][osd5][WARNING] there are 4 OSDs down
[2021-12-11 19:33:42,651][osd5][WARNING] there are 3 OSDs out
[2021-12-11 19:33:42,651][ceph_deploy.osd][DEBUG ] Host osd5 is now ready for osd use.
[2021-12-11 19:34:16,656][ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephuser/.cephdeploy.conf
[2021-12-11 19:34:16,656][ceph_deploy.cli][INFO  ] Invoked (1.5.39): /usr/bin/ceph-deploy osd activate osd1:/dev/sdb osd2:/dev/sdb osd3:/dev/sdb osd4:/dev/sdb osd5:/dev/sdb
[2021-12-11 19:34:16,656][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2021-12-11 19:34:16,656][ceph_deploy.cli][INFO  ]  username                      : None
[2021-12-11 19:34:16,656][ceph_deploy.cli][INFO  ]  verbose                       : False
[2021-12-11 19:34:16,657][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2021-12-11 19:34:16,657][ceph_deploy.cli][INFO  ]  subcommand                    : activate
[2021-12-11 19:34:16,657][ceph_deploy.cli][INFO  ]  quiet                         : False
[2021-12-11 19:34:16,657][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f7b3c26a128>
[2021-12-11 19:34:16,657][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2021-12-11 19:34:16,657][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f7b3c2b7050>
[2021-12-11 19:34:16,657][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2021-12-11 19:34:16,657][ceph_deploy.cli][INFO  ]  default_release               : False
[2021-12-11 19:34:16,657][ceph_deploy.cli][INFO  ]  disk                          : [('osd1', '/dev/sdb', None), ('osd2', '/dev/sdb', None), ('osd3', '/dev/sdb', None), ('osd4', '/dev/sdb', None), ('osd5', '/dev/sdb', None)]
[2021-12-11 19:34:16,657][ceph_deploy.osd][DEBUG ] Activating cluster ceph disks osd1:/dev/sdb: osd2:/dev/sdb: osd3:/dev/sdb: osd4:/dev/sdb: osd5:/dev/sdb:
[2021-12-11 19:34:16,826][osd1][DEBUG ] connection detected need for sudo
[2021-12-11 19:34:17,002][osd1][DEBUG ] connected to host: osd1 
[2021-12-11 19:34:17,003][osd1][DEBUG ] detect platform information from remote host
[2021-12-11 19:34:17,021][osd1][DEBUG ] detect machine type
[2021-12-11 19:34:17,026][osd1][DEBUG ] find the location of an executable
[2021-12-11 19:34:17,028][ceph_deploy.osd][INFO  ] Distro info: CentOS Linux 7.9.2009 Core
[2021-12-11 19:34:17,028][ceph_deploy.osd][DEBUG ] activating host osd1 disk /dev/sdb
[2021-12-11 19:34:17,028][ceph_deploy.osd][DEBUG ] will use init type: systemd
[2021-12-11 19:34:17,028][osd1][DEBUG ] find the location of an executable
[2021-12-11 19:34:17,031][osd1][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v activate --mark-init systemd --mount /dev/sdb
[2021-12-11 19:34:17,150][osd1][WARNING] main_activate: path = /dev/sdb
[2021-12-11 19:34:17,150][osd1][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-11 19:34:17,150][osd1][WARNING] command: Running command: /sbin/blkid -p -s TYPE -o value -- /dev/sdb
[2021-12-11 19:34:17,150][osd1][WARNING] Traceback (most recent call last):
[2021-12-11 19:34:17,150][osd1][WARNING]   File "/usr/sbin/ceph-disk", line 9, in <module>
[2021-12-11 19:34:17,150][osd1][WARNING]     load_entry_point('ceph-disk==1.0.0', 'console_scripts', 'ceph-disk')()
[2021-12-11 19:34:17,150][osd1][WARNING]   File "/usr/lib/python2.7/site-packages/ceph_disk/main.py", line 5361, in run
[2021-12-11 19:34:17,150][osd1][WARNING]     main(sys.argv[1:])
[2021-12-11 19:34:17,150][osd1][WARNING]   File "/usr/lib/python2.7/site-packages/ceph_disk/main.py", line 5312, in main
[2021-12-11 19:34:17,151][osd1][WARNING]     args.func(args)
[2021-12-11 19:34:17,151][osd1][WARNING]   File "/usr/lib/python2.7/site-packages/ceph_disk/main.py", line 3435, in main_activate
[2021-12-11 19:34:17,151][osd1][WARNING]     reactivate=args.reactivate,
[2021-12-11 19:34:17,151][osd1][WARNING]   File "/usr/lib/python2.7/site-packages/ceph_disk/main.py", line 3162, in mount_activate
[2021-12-11 19:34:17,151][osd1][WARNING]     e,
[2021-12-11 19:34:17,151][osd1][WARNING] ceph_disk.main.FilesystemTypeError: Cannot discover filesystem type: device /dev/sdb: Line is truncated: 
[2021-12-11 19:34:17,152][osd1][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2021-12-11 19:34:17,152][ceph_deploy][ERROR ] RuntimeError: Failed to execute command: /usr/sbin/ceph-disk -v activate --mark-init systemd --mount /dev/sdb

[2021-12-11 19:34:28,042][ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephuser/.cephdeploy.conf
[2021-12-11 19:34:28,042][ceph_deploy.cli][INFO  ] Invoked (1.5.39): /usr/bin/ceph-deploy osd activate osd1:/dev/sdb1 osd2:/dev/sdb osd3:/dev/sdb osd4:/dev/sdb osd5:/dev/sdb
[2021-12-11 19:34:28,043][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2021-12-11 19:34:28,043][ceph_deploy.cli][INFO  ]  username                      : None
[2021-12-11 19:34:28,043][ceph_deploy.cli][INFO  ]  verbose                       : False
[2021-12-11 19:34:28,043][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2021-12-11 19:34:28,043][ceph_deploy.cli][INFO  ]  subcommand                    : activate
[2021-12-11 19:34:28,043][ceph_deploy.cli][INFO  ]  quiet                         : False
[2021-12-11 19:34:28,043][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f60f98bd128>
[2021-12-11 19:34:28,043][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2021-12-11 19:34:28,043][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f60f990a050>
[2021-12-11 19:34:28,043][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2021-12-11 19:34:28,043][ceph_deploy.cli][INFO  ]  default_release               : False
[2021-12-11 19:34:28,043][ceph_deploy.cli][INFO  ]  disk                          : [('osd1', '/dev/sdb1', None), ('osd2', '/dev/sdb', None), ('osd3', '/dev/sdb', None), ('osd4', '/dev/sdb', None), ('osd5', '/dev/sdb', None)]
[2021-12-11 19:34:28,044][ceph_deploy.osd][DEBUG ] Activating cluster ceph disks osd1:/dev/sdb1: osd2:/dev/sdb: osd3:/dev/sdb: osd4:/dev/sdb: osd5:/dev/sdb:
[2021-12-11 19:34:28,216][osd1][DEBUG ] connection detected need for sudo
[2021-12-11 19:34:28,396][osd1][DEBUG ] connected to host: osd1 
[2021-12-11 19:34:28,396][osd1][DEBUG ] detect platform information from remote host
[2021-12-11 19:34:28,418][osd1][DEBUG ] detect machine type
[2021-12-11 19:34:28,423][osd1][DEBUG ] find the location of an executable
[2021-12-11 19:34:28,425][ceph_deploy.osd][INFO  ] Distro info: CentOS Linux 7.9.2009 Core
[2021-12-11 19:34:28,425][ceph_deploy.osd][DEBUG ] activating host osd1 disk /dev/sdb1
[2021-12-11 19:34:28,425][ceph_deploy.osd][DEBUG ] will use init type: systemd
[2021-12-11 19:34:28,425][osd1][DEBUG ] find the location of an executable
[2021-12-11 19:34:28,429][osd1][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v activate --mark-init systemd --mount /dev/sdb1
[2021-12-11 19:34:28,548][osd1][WARNING] main_activate: path = /dev/sdb1
[2021-12-11 19:34:28,548][osd1][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb1 uuid path is /sys/dev/block/8:17/dm/uuid
[2021-12-11 19:34:28,549][osd1][WARNING] command: Running command: /sbin/blkid -o udev -p /dev/sdb1
[2021-12-11 19:34:28,549][osd1][WARNING] command: Running command: /sbin/blkid -p -s TYPE -o value -- /dev/sdb1
[2021-12-11 19:34:28,549][osd1][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[2021-12-11 19:34:28,549][osd1][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[2021-12-11 19:34:28,549][osd1][WARNING] mount: Mounting /dev/sdb1 on /var/lib/ceph/tmp/mnt.Y8AduH with options noatime,inode64
[2021-12-11 19:34:28,549][osd1][WARNING] command_check_call: Running command: /usr/bin/mount -t xfs -o noatime,inode64 -- /dev/sdb1 /var/lib/ceph/tmp/mnt.Y8AduH
[2021-12-11 19:34:28,549][osd1][WARNING] command: Running command: /sbin/restorecon /var/lib/ceph/tmp/mnt.Y8AduH
[2021-12-11 19:34:28,549][osd1][WARNING] activate: Cluster uuid is 67d9e224-a6f7-43d3-ae36-e6100c59258e
[2021-12-11 19:34:28,549][osd1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2021-12-11 19:34:28,557][osd1][WARNING] mount_activate: Failed to activate
[2021-12-11 19:34:28,557][osd1][WARNING] unmount: Unmounting /var/lib/ceph/tmp/mnt.Y8AduH
[2021-12-11 19:34:28,557][osd1][WARNING] command_check_call: Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.Y8AduH
[2021-12-11 19:34:28,572][osd1][WARNING] Traceback (most recent call last):
[2021-12-11 19:34:28,573][osd1][WARNING]   File "/usr/sbin/ceph-disk", line 9, in <module>
[2021-12-11 19:34:28,573][osd1][WARNING]     load_entry_point('ceph-disk==1.0.0', 'console_scripts', 'ceph-disk')()
[2021-12-11 19:34:28,573][osd1][WARNING]   File "/usr/lib/python2.7/site-packages/ceph_disk/main.py", line 5361, in run
[2021-12-11 19:34:28,573][osd1][WARNING]     main(sys.argv[1:])
[2021-12-11 19:34:28,573][osd1][WARNING]   File "/usr/lib/python2.7/site-packages/ceph_disk/main.py", line 5312, in main
[2021-12-11 19:34:28,573][osd1][WARNING]     args.func(args)
[2021-12-11 19:34:28,573][osd1][WARNING]   File "/usr/lib/python2.7/site-packages/ceph_disk/main.py", line 3435, in main_activate
[2021-12-11 19:34:28,573][osd1][WARNING]     reactivate=args.reactivate,
[2021-12-11 19:34:28,573][osd1][WARNING]   File "/usr/lib/python2.7/site-packages/ceph_disk/main.py", line 3192, in mount_activate
[2021-12-11 19:34:28,573][osd1][WARNING]     (osd_id, cluster) = activate(path, activate_key_template, init)
[2021-12-11 19:34:28,573][osd1][WARNING]   File "/usr/lib/python2.7/site-packages/ceph_disk/main.py", line 3339, in activate
[2021-12-11 19:34:28,574][osd1][WARNING]     ' with fsid %s' % ceph_fsid)
[2021-12-11 19:34:28,574][osd1][WARNING] ceph_disk.main.Error: Error: No cluster conf found in /etc/ceph with fsid 67d9e224-a6f7-43d3-ae36-e6100c59258e
[2021-12-11 19:34:28,574][osd1][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2021-12-11 19:34:28,574][ceph_deploy][ERROR ] RuntimeError: Failed to execute command: /usr/sbin/ceph-disk -v activate --mark-init systemd --mount /dev/sdb1

[2021-12-11 19:36:07,781][ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephuser/.cephdeploy.conf
[2021-12-11 19:36:07,781][ceph_deploy.cli][INFO  ] Invoked (1.5.39): /usr/bin/ceph-deploy mon create-initial
[2021-12-11 19:36:07,781][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2021-12-11 19:36:07,781][ceph_deploy.cli][INFO  ]  username                      : None
[2021-12-11 19:36:07,781][ceph_deploy.cli][INFO  ]  verbose                       : False
[2021-12-11 19:36:07,781][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2021-12-11 19:36:07,781][ceph_deploy.cli][INFO  ]  subcommand                    : create-initial
[2021-12-11 19:36:07,781][ceph_deploy.cli][INFO  ]  quiet                         : False
[2021-12-11 19:36:07,782][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fac97aacf38>
[2021-12-11 19:36:07,782][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2021-12-11 19:36:07,782][ceph_deploy.cli][INFO  ]  func                          : <function mon at 0x7fac97a93758>
[2021-12-11 19:36:07,782][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2021-12-11 19:36:07,782][ceph_deploy.cli][INFO  ]  default_release               : False
[2021-12-11 19:36:07,782][ceph_deploy.cli][INFO  ]  keyrings                      : None
[2021-12-11 19:36:07,782][ceph_deploy.mon][DEBUG ] Deploying mon, cluster ceph hosts mon1
[2021-12-11 19:36:07,782][ceph_deploy.mon][DEBUG ] detecting platform for host mon1 ...
[2021-12-11 19:36:07,948][mon1][DEBUG ] connection detected need for sudo
[2021-12-11 19:36:08,113][mon1][DEBUG ] connected to host: mon1 
[2021-12-11 19:36:08,113][mon1][DEBUG ] detect platform information from remote host
[2021-12-11 19:36:08,130][mon1][DEBUG ] detect machine type
[2021-12-11 19:36:08,134][mon1][DEBUG ] find the location of an executable
[2021-12-11 19:36:08,135][ceph_deploy.mon][INFO  ] distro info: CentOS Linux 7.9.2009 Core
[2021-12-11 19:36:08,135][mon1][DEBUG ] determining if provided host has same hostname in remote
[2021-12-11 19:36:08,135][mon1][DEBUG ] get remote short hostname
[2021-12-11 19:36:08,136][mon1][DEBUG ] deploying mon to mon1
[2021-12-11 19:36:08,136][mon1][DEBUG ] get remote short hostname
[2021-12-11 19:36:08,137][mon1][DEBUG ] remote hostname: mon1
[2021-12-11 19:36:08,139][mon1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2021-12-11 19:36:08,140][mon1][DEBUG ] create the mon path if it does not exist
[2021-12-11 19:36:08,141][mon1][DEBUG ] checking for done path: /var/lib/ceph/mon/ceph-mon1/done
[2021-12-11 19:36:08,142][mon1][DEBUG ] create a done file to avoid re-doing the mon deployment
[2021-12-11 19:36:08,143][mon1][DEBUG ] create the init path if it does not exist
[2021-12-11 19:36:08,146][mon1][INFO  ] Running command: sudo systemctl enable ceph.target
[2021-12-11 19:36:08,216][mon1][INFO  ] Running command: sudo systemctl enable ceph-mon@mon1
[2021-12-11 19:36:08,284][mon1][INFO  ] Running command: sudo systemctl start ceph-mon@mon1
[2021-12-11 19:36:10,298][mon1][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.mon1.asok mon_status
[2021-12-11 19:36:10,363][mon1][DEBUG ] ********************************************************************************
[2021-12-11 19:36:10,363][mon1][DEBUG ] status for monitor: mon.mon1
[2021-12-11 19:36:10,363][mon1][DEBUG ] {
[2021-12-11 19:36:10,363][mon1][DEBUG ]   "election_epoch": 16, 
[2021-12-11 19:36:10,363][mon1][DEBUG ]   "extra_probe_peers": [], 
[2021-12-11 19:36:10,363][mon1][DEBUG ]   "monmap": {
[2021-12-11 19:36:10,363][mon1][DEBUG ]     "created": "2021-11-03 20:50:42.718657", 
[2021-12-11 19:36:10,363][mon1][DEBUG ]     "epoch": 1, 
[2021-12-11 19:36:10,363][mon1][DEBUG ]     "fsid": "67d9e224-a6f7-43d3-ae36-e6100c59258e", 
[2021-12-11 19:36:10,363][mon1][DEBUG ]     "modified": "2021-11-03 20:50:42.718657", 
[2021-12-11 19:36:10,364][mon1][DEBUG ]     "mons": [
[2021-12-11 19:36:10,364][mon1][DEBUG ]       {
[2021-12-11 19:36:10,364][mon1][DEBUG ]         "addr": "192.168.1.93:6789/0", 
[2021-12-11 19:36:10,364][mon1][DEBUG ]         "name": "mon1", 
[2021-12-11 19:36:10,364][mon1][DEBUG ]         "rank": 0
[2021-12-11 19:36:10,364][mon1][DEBUG ]       }
[2021-12-11 19:36:10,364][mon1][DEBUG ]     ]
[2021-12-11 19:36:10,364][mon1][DEBUG ]   }, 
[2021-12-11 19:36:10,364][mon1][DEBUG ]   "name": "mon1", 
[2021-12-11 19:36:10,364][mon1][DEBUG ]   "outside_quorum": [], 
[2021-12-11 19:36:10,364][mon1][DEBUG ]   "quorum": [
[2021-12-11 19:36:10,364][mon1][DEBUG ]     0
[2021-12-11 19:36:10,364][mon1][DEBUG ]   ], 
[2021-12-11 19:36:10,364][mon1][DEBUG ]   "rank": 0, 
[2021-12-11 19:36:10,364][mon1][DEBUG ]   "state": "leader", 
[2021-12-11 19:36:10,364][mon1][DEBUG ]   "sync_provider": []
[2021-12-11 19:36:10,364][mon1][DEBUG ] }
[2021-12-11 19:36:10,364][mon1][DEBUG ] ********************************************************************************
[2021-12-11 19:36:10,364][mon1][INFO  ] monitor: mon.mon1 is running
[2021-12-11 19:36:10,366][mon1][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.mon1.asok mon_status
[2021-12-11 19:36:10,482][ceph_deploy.mon][INFO  ] processing monitor mon.mon1
[2021-12-11 19:36:10,646][mon1][DEBUG ] connection detected need for sudo
[2021-12-11 19:36:10,811][mon1][DEBUG ] connected to host: mon1 
[2021-12-11 19:36:10,812][mon1][DEBUG ] detect platform information from remote host
[2021-12-11 19:36:10,828][mon1][DEBUG ] detect machine type
[2021-12-11 19:36:10,833][mon1][DEBUG ] find the location of an executable
[2021-12-11 19:36:10,836][mon1][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.mon1.asok mon_status
[2021-12-11 19:36:10,951][ceph_deploy.mon][INFO  ] mon.mon1 monitor has reached quorum!
[2021-12-11 19:36:10,951][ceph_deploy.mon][INFO  ] all initial monitors are running and have formed quorum
[2021-12-11 19:36:10,951][ceph_deploy.mon][INFO  ] Running gatherkeys...
[2021-12-11 19:36:10,951][ceph_deploy.gatherkeys][INFO  ] Storing keys in temp directory /tmp/tmpUiuOcJ
[2021-12-11 19:36:11,112][mon1][DEBUG ] connection detected need for sudo
[2021-12-11 19:36:11,280][mon1][DEBUG ] connected to host: mon1 
[2021-12-11 19:36:11,281][mon1][DEBUG ] detect platform information from remote host
[2021-12-11 19:36:11,298][mon1][DEBUG ] detect machine type
[2021-12-11 19:36:11,303][mon1][DEBUG ] get remote short hostname
[2021-12-11 19:36:11,305][mon1][DEBUG ] fetch remote file
[2021-12-11 19:36:11,308][mon1][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --admin-daemon=/var/run/ceph/ceph-mon.mon1.asok mon_status
[2021-12-11 19:36:11,375][mon1][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-mon1/keyring auth get client.admin
[2021-12-11 19:36:11,541][mon1][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-mon1/keyring auth get client.bootstrap-mds
[2021-12-11 19:36:11,708][mon1][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-mon1/keyring auth get client.bootstrap-mgr
[2021-12-11 19:36:11,875][mon1][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-mon1/keyring auth get client.bootstrap-osd
[2021-12-11 19:36:12,043][mon1][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-mon1/keyring auth get client.bootstrap-rgw
[2021-12-11 19:36:12,208][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.client.admin.keyring' already exists
[2021-12-11 19:36:12,208][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.bootstrap-mds.keyring' already exists
[2021-12-11 19:36:12,208][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.bootstrap-mgr.keyring' already exists
[2021-12-11 19:36:12,209][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.mon.keyring' already exists
[2021-12-11 19:36:12,209][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.bootstrap-osd.keyring' already exists
[2021-12-11 19:36:12,209][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.bootstrap-rgw.keyring' already exists
[2021-12-11 19:36:12,209][ceph_deploy.gatherkeys][INFO  ] Destroy temp directory /tmp/tmpUiuOcJ
[2021-12-11 19:36:17,030][ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephuser/.cephdeploy.conf
[2021-12-11 19:36:17,030][ceph_deploy.cli][INFO  ] Invoked (1.5.39): /usr/bin/ceph-deploy gatherkeys mon1
[2021-12-11 19:36:17,031][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2021-12-11 19:36:17,031][ceph_deploy.cli][INFO  ]  username                      : None
[2021-12-11 19:36:17,031][ceph_deploy.cli][INFO  ]  verbose                       : False
[2021-12-11 19:36:17,031][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2021-12-11 19:36:17,031][ceph_deploy.cli][INFO  ]  quiet                         : False
[2021-12-11 19:36:17,031][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fb53866a878>
[2021-12-11 19:36:17,031][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2021-12-11 19:36:17,031][ceph_deploy.cli][INFO  ]  mon                           : ['mon1']
[2021-12-11 19:36:17,031][ceph_deploy.cli][INFO  ]  func                          : <function gatherkeys at 0x7fb53862e230>
[2021-12-11 19:36:17,031][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2021-12-11 19:36:17,031][ceph_deploy.cli][INFO  ]  default_release               : False
[2021-12-11 19:36:17,031][ceph_deploy.gatherkeys][INFO  ] Storing keys in temp directory /tmp/tmpyI6dAM
[2021-12-11 19:36:17,196][mon1][DEBUG ] connection detected need for sudo
[2021-12-11 19:36:17,360][mon1][DEBUG ] connected to host: mon1 
[2021-12-11 19:36:17,361][mon1][DEBUG ] detect platform information from remote host
[2021-12-11 19:36:17,379][mon1][DEBUG ] detect machine type
[2021-12-11 19:36:17,383][mon1][DEBUG ] get remote short hostname
[2021-12-11 19:36:17,385][mon1][DEBUG ] fetch remote file
[2021-12-11 19:36:17,387][mon1][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --admin-daemon=/var/run/ceph/ceph-mon.mon1.asok mon_status
[2021-12-11 19:36:17,456][mon1][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-mon1/keyring auth get client.admin
[2021-12-11 19:36:17,622][mon1][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-mon1/keyring auth get client.bootstrap-mds
[2021-12-11 19:36:17,789][mon1][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-mon1/keyring auth get client.bootstrap-mgr
[2021-12-11 19:36:17,956][mon1][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-mon1/keyring auth get client.bootstrap-osd
[2021-12-11 19:36:18,123][mon1][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-mon1/keyring auth get client.bootstrap-rgw
[2021-12-11 19:36:18,289][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.client.admin.keyring' already exists
[2021-12-11 19:36:18,289][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.bootstrap-mds.keyring' already exists
[2021-12-11 19:36:18,289][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.bootstrap-mgr.keyring' already exists
[2021-12-11 19:36:18,289][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.mon.keyring' already exists
[2021-12-11 19:36:18,290][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.bootstrap-osd.keyring' already exists
[2021-12-11 19:36:18,290][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.bootstrap-rgw.keyring' already exists
[2021-12-11 19:36:18,290][ceph_deploy.gatherkeys][INFO  ] Destroy temp directory /tmp/tmpyI6dAM
[2021-12-12 00:31:59,758][ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephuser/.cephdeploy.conf
[2021-12-12 00:31:59,771][ceph_deploy.cli][INFO  ] Invoked (1.5.39): /usr/bin/ceph-deploy forgetkeys
[2021-12-12 00:31:59,771][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2021-12-12 00:31:59,772][ceph_deploy.cli][INFO  ]  username                      : None
[2021-12-12 00:31:59,772][ceph_deploy.cli][INFO  ]  verbose                       : False
[2021-12-12 00:31:59,772][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2021-12-12 00:31:59,772][ceph_deploy.cli][INFO  ]  quiet                         : False
[2021-12-12 00:31:59,772][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fc401878680>
[2021-12-12 00:31:59,772][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2021-12-12 00:31:59,772][ceph_deploy.cli][INFO  ]  func                          : <function forgetkeys at 0x7fc402583aa0>
[2021-12-12 00:31:59,772][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2021-12-12 00:31:59,772][ceph_deploy.cli][INFO  ]  default_release               : False
[2021-12-12 00:33:13,818][ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephuser/.cephdeploy.conf
[2021-12-12 00:33:13,819][ceph_deploy.cli][INFO  ] Invoked (1.5.39): /usr/bin/ceph-deploy admin ceph-admin mon1 osd1 osd2 osd3 osd4 osd5 client client2 client3
[2021-12-12 00:33:13,819][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2021-12-12 00:33:13,819][ceph_deploy.cli][INFO  ]  username                      : None
[2021-12-12 00:33:13,819][ceph_deploy.cli][INFO  ]  verbose                       : False
[2021-12-12 00:33:13,819][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2021-12-12 00:33:13,819][ceph_deploy.cli][INFO  ]  quiet                         : False
[2021-12-12 00:33:13,819][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f8d6809fb48>
[2021-12-12 00:33:13,819][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2021-12-12 00:33:13,819][ceph_deploy.cli][INFO  ]  client                        : ['ceph-admin', 'mon1', 'osd1', 'osd2', 'osd3', 'osd4', 'osd5', 'client', 'client2', 'client3']
[2021-12-12 00:33:13,819][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7f8d68db8a28>
[2021-12-12 00:33:13,819][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2021-12-12 00:33:13,819][ceph_deploy.cli][INFO  ]  default_release               : False
[2021-12-12 00:33:13,888][ceph_deploy][ERROR ] RuntimeError: ceph.client.admin.keyring not found

[2021-12-12 00:33:35,261][ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephuser/.cephdeploy.conf
[2021-12-12 00:33:35,261][ceph_deploy.cli][INFO  ] Invoked (1.5.39): /usr/bin/ceph-deploy disk list osd1 osd2 osd3
[2021-12-12 00:33:35,261][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2021-12-12 00:33:35,261][ceph_deploy.cli][INFO  ]  username                      : None
[2021-12-12 00:33:35,261][ceph_deploy.cli][INFO  ]  verbose                       : False
[2021-12-12 00:33:35,261][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2021-12-12 00:33:35,261][ceph_deploy.cli][INFO  ]  subcommand                    : list
[2021-12-12 00:33:35,262][ceph_deploy.cli][INFO  ]  quiet                         : False
[2021-12-12 00:33:35,262][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f12fc68cef0>
[2021-12-12 00:33:35,262][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2021-12-12 00:33:35,262][ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7f12fc6680c8>
[2021-12-12 00:33:35,262][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2021-12-12 00:33:35,262][ceph_deploy.cli][INFO  ]  default_release               : False
[2021-12-12 00:33:35,262][ceph_deploy.cli][INFO  ]  disk                          : [('osd1', None, None), ('osd2', None, None), ('osd3', None, None)]
[2021-12-12 00:33:35,738][osd1][DEBUG ] connection detected need for sudo
[2021-12-12 00:33:35,921][osd1][DEBUG ] connected to host: osd1 
[2021-12-12 00:33:35,922][osd1][DEBUG ] detect platform information from remote host
[2021-12-12 00:33:35,948][osd1][DEBUG ] detect machine type
[2021-12-12 00:33:35,953][osd1][DEBUG ] find the location of an executable
[2021-12-12 00:33:35,954][ceph_deploy.osd][INFO  ] Distro info: CentOS Linux 7.9.2009 Core
[2021-12-12 00:33:35,954][ceph_deploy.osd][DEBUG ] Listing disks on osd1...
[2021-12-12 00:33:35,955][osd1][DEBUG ] find the location of an executable
[2021-12-12 00:33:35,957][osd1][INFO  ] Running command: sudo /usr/sbin/ceph-disk list
[2021-12-12 00:33:36,489][osd1][DEBUG ] /dev/dm-0 other, xfs, mounted on /
[2021-12-12 00:33:36,489][osd1][DEBUG ] /dev/dm-1 swap, swap
[2021-12-12 00:33:36,489][osd1][DEBUG ] /dev/sda :
[2021-12-12 00:33:36,489][osd1][DEBUG ]  /dev/sda2 other, LVM2_member
[2021-12-12 00:33:36,489][osd1][DEBUG ]  /dev/sda1 other, xfs, mounted on /boot
[2021-12-12 00:33:36,490][osd1][DEBUG ] /dev/sdb :
[2021-12-12 00:33:36,490][osd1][DEBUG ]  /dev/sdb2 ceph journal, for /dev/sdb1
[2021-12-12 00:33:36,490][osd1][DEBUG ]  /dev/sdb1 ceph data, prepared, unknown cluster 67d9e224-a6f7-43d3-ae36-e6100c59258e, osd.5, journal /dev/sdb2
[2021-12-12 00:33:36,490][osd1][DEBUG ] /dev/sr0 other, unknown
[2021-12-12 00:33:36,770][osd2][DEBUG ] connection detected need for sudo
[2021-12-12 00:33:36,981][osd2][DEBUG ] connected to host: osd2 
[2021-12-12 00:33:36,981][osd2][DEBUG ] detect platform information from remote host
[2021-12-12 00:33:37,013][osd2][DEBUG ] detect machine type
[2021-12-12 00:33:37,017][osd2][DEBUG ] find the location of an executable
[2021-12-12 00:33:37,018][ceph_deploy.osd][INFO  ] Distro info: CentOS Linux 7.9.2009 Core
[2021-12-12 00:33:37,019][ceph_deploy.osd][DEBUG ] Listing disks on osd2...
[2021-12-12 00:33:37,019][osd2][DEBUG ] find the location of an executable
[2021-12-12 00:33:37,021][osd2][INFO  ] Running command: sudo /usr/sbin/ceph-disk list
[2021-12-12 00:33:37,538][osd2][DEBUG ] /dev/dm-0 other, xfs, mounted on /
[2021-12-12 00:33:37,539][osd2][DEBUG ] /dev/dm-1 swap, swap
[2021-12-12 00:33:37,539][osd2][DEBUG ] /dev/sda :
[2021-12-12 00:33:37,539][osd2][DEBUG ]  /dev/sda2 other, LVM2_member
[2021-12-12 00:33:37,539][osd2][DEBUG ]  /dev/sda1 other, xfs, mounted on /boot
[2021-12-12 00:33:37,539][osd2][DEBUG ] /dev/sdb :
[2021-12-12 00:33:37,539][osd2][DEBUG ]  /dev/sdb2 ceph journal, for /dev/sdb1
[2021-12-12 00:33:37,539][osd2][DEBUG ]  /dev/sdb1 ceph data, prepared, unknown cluster 67d9e224-a6f7-43d3-ae36-e6100c59258e, osd.6, journal /dev/sdb2
[2021-12-12 00:33:37,539][osd2][DEBUG ] /dev/sr0 other, unknown
[2021-12-12 00:33:37,804][osd3][DEBUG ] connection detected need for sudo
[2021-12-12 00:33:37,976][osd3][DEBUG ] connected to host: osd3 
[2021-12-12 00:33:37,976][osd3][DEBUG ] detect platform information from remote host
[2021-12-12 00:33:37,996][osd3][DEBUG ] detect machine type
[2021-12-12 00:33:38,001][osd3][DEBUG ] find the location of an executable
[2021-12-12 00:33:38,002][ceph_deploy.osd][INFO  ] Distro info: CentOS Linux 7.9.2009 Core
[2021-12-12 00:33:38,002][ceph_deploy.osd][DEBUG ] Listing disks on osd3...
[2021-12-12 00:33:38,002][osd3][DEBUG ] find the location of an executable
[2021-12-12 00:33:38,005][osd3][INFO  ] Running command: sudo /usr/sbin/ceph-disk list
[2021-12-12 00:33:38,522][osd3][DEBUG ] /dev/dm-0 other, xfs, mounted on /
[2021-12-12 00:33:38,522][osd3][DEBUG ] /dev/dm-1 swap, swap
[2021-12-12 00:33:38,522][osd3][DEBUG ] /dev/sda :
[2021-12-12 00:33:38,522][osd3][DEBUG ]  /dev/sda2 other, LVM2_member
[2021-12-12 00:33:38,522][osd3][DEBUG ]  /dev/sda1 other, xfs, mounted on /boot
[2021-12-12 00:33:38,522][osd3][DEBUG ] /dev/sdb :
[2021-12-12 00:33:38,522][osd3][DEBUG ]  /dev/sdb2 ceph journal, for /dev/sdb1
[2021-12-12 00:33:38,523][osd3][DEBUG ]  /dev/sdb1 ceph data, prepared, unknown cluster 67d9e224-a6f7-43d3-ae36-e6100c59258e, osd.7, journal /dev/sdb2
[2021-12-12 00:33:47,784][ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephuser/.cephdeploy.conf
[2021-12-12 00:33:47,784][ceph_deploy.cli][INFO  ] Invoked (1.5.39): /usr/bin/ceph-deploy disk zap osd1:/dev/sdb osd2:/dev/sdb osd3:/dev/sdb
[2021-12-12 00:33:47,784][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2021-12-12 00:33:47,784][ceph_deploy.cli][INFO  ]  username                      : None
[2021-12-12 00:33:47,784][ceph_deploy.cli][INFO  ]  verbose                       : False
[2021-12-12 00:33:47,784][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2021-12-12 00:33:47,784][ceph_deploy.cli][INFO  ]  subcommand                    : zap
[2021-12-12 00:33:47,784][ceph_deploy.cli][INFO  ]  quiet                         : False
[2021-12-12 00:33:47,785][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f0c87b78ef0>
[2021-12-12 00:33:47,785][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2021-12-12 00:33:47,785][ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7f0c87b540c8>
[2021-12-12 00:33:47,785][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2021-12-12 00:33:47,785][ceph_deploy.cli][INFO  ]  default_release               : False
[2021-12-12 00:33:47,785][ceph_deploy.cli][INFO  ]  disk                          : [('osd1', '/dev/sdb', None), ('osd2', '/dev/sdb', None), ('osd3', '/dev/sdb', None)]
[2021-12-12 00:33:47,785][ceph_deploy.osd][DEBUG ] zapping /dev/sdb on osd1
[2021-12-12 00:33:47,958][osd1][DEBUG ] connection detected need for sudo
[2021-12-12 00:33:48,128][osd1][DEBUG ] connected to host: osd1 
[2021-12-12 00:33:48,129][osd1][DEBUG ] detect platform information from remote host
[2021-12-12 00:33:48,147][osd1][DEBUG ] detect machine type
[2021-12-12 00:33:48,152][osd1][DEBUG ] find the location of an executable
[2021-12-12 00:33:48,154][ceph_deploy.osd][INFO  ] Distro info: CentOS Linux 7.9.2009 Core
[2021-12-12 00:33:48,154][osd1][DEBUG ] zeroing last few blocks of device
[2021-12-12 00:33:48,155][osd1][DEBUG ] find the location of an executable
[2021-12-12 00:33:48,157][osd1][INFO  ] Running command: sudo /usr/sbin/ceph-disk zap /dev/sdb
[2021-12-12 00:33:48,276][osd1][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[2021-12-12 00:33:48,276][osd1][WARNING] backup header from main header.
[2021-12-12 00:33:48,276][osd1][WARNING] 
[2021-12-12 00:33:48,277][osd1][WARNING] Warning! Main and backup partition tables differ! Use the 'c' and 'e' options
[2021-12-12 00:33:48,278][osd1][WARNING] on the recovery & transformation menu to examine the two tables.
[2021-12-12 00:33:48,278][osd1][WARNING] 
[2021-12-12 00:33:48,278][osd1][WARNING] Warning! One or more CRCs don't match. You should repair the disk!
[2021-12-12 00:33:48,278][osd1][WARNING] 
[2021-12-12 00:33:49,495][osd1][DEBUG ] ****************************************************************************
[2021-12-12 00:33:49,495][osd1][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[2021-12-12 00:33:49,495][osd1][DEBUG ] verification and recovery are STRONGLY recommended.
[2021-12-12 00:33:49,495][osd1][DEBUG ] ****************************************************************************
[2021-12-12 00:33:49,495][osd1][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[2021-12-12 00:33:49,495][osd1][DEBUG ] other utilities.
[2021-12-12 00:33:50,662][osd1][DEBUG ] Creating new GPT entries.
[2021-12-12 00:33:50,663][osd1][DEBUG ] The operation has completed successfully.
[2021-12-12 00:33:50,695][ceph_deploy.osd][DEBUG ] zapping /dev/sdb on osd2
[2021-12-12 00:33:50,849][osd2][DEBUG ] connection detected need for sudo
[2021-12-12 00:33:51,022][osd2][DEBUG ] connected to host: osd2 
[2021-12-12 00:33:51,022][osd2][DEBUG ] detect platform information from remote host
[2021-12-12 00:33:51,039][osd2][DEBUG ] detect machine type
[2021-12-12 00:33:51,044][osd2][DEBUG ] find the location of an executable
[2021-12-12 00:33:51,045][ceph_deploy.osd][INFO  ] Distro info: CentOS Linux 7.9.2009 Core
[2021-12-12 00:33:51,045][osd2][DEBUG ] zeroing last few blocks of device
[2021-12-12 00:33:51,046][osd2][DEBUG ] find the location of an executable
[2021-12-12 00:33:51,048][osd2][INFO  ] Running command: sudo /usr/sbin/ceph-disk zap /dev/sdb
[2021-12-12 00:33:51,165][osd2][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[2021-12-12 00:33:51,165][osd2][WARNING] backup header from main header.
[2021-12-12 00:33:51,166][osd2][WARNING] 
[2021-12-12 00:33:51,166][osd2][WARNING] Warning! Main and backup partition tables differ! Use the 'c' and 'e' options
[2021-12-12 00:33:51,166][osd2][WARNING] on the recovery & transformation menu to examine the two tables.
[2021-12-12 00:33:51,166][osd2][WARNING] 
[2021-12-12 00:33:51,166][osd2][WARNING] Warning! One or more CRCs don't match. You should repair the disk!
[2021-12-12 00:33:51,166][osd2][WARNING] 
[2021-12-12 00:33:52,283][osd2][DEBUG ] ****************************************************************************
[2021-12-12 00:33:52,283][osd2][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[2021-12-12 00:33:52,283][osd2][DEBUG ] verification and recovery are STRONGLY recommended.
[2021-12-12 00:33:52,284][osd2][DEBUG ] ****************************************************************************
[2021-12-12 00:33:52,284][osd2][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[2021-12-12 00:33:52,284][osd2][DEBUG ] other utilities.
[2021-12-12 00:33:53,400][osd2][DEBUG ] Creating new GPT entries.
[2021-12-12 00:33:53,401][osd2][DEBUG ] The operation has completed successfully.
[2021-12-12 00:33:53,465][ceph_deploy.osd][DEBUG ] zapping /dev/sdb on osd3
[2021-12-12 00:33:53,619][osd3][DEBUG ] connection detected need for sudo
[2021-12-12 00:33:53,785][osd3][DEBUG ] connected to host: osd3 
[2021-12-12 00:33:53,786][osd3][DEBUG ] detect platform information from remote host
[2021-12-12 00:33:53,803][osd3][DEBUG ] detect machine type
[2021-12-12 00:33:53,808][osd3][DEBUG ] find the location of an executable
[2021-12-12 00:33:53,810][ceph_deploy.osd][INFO  ] Distro info: CentOS Linux 7.9.2009 Core
[2021-12-12 00:33:53,810][osd3][DEBUG ] zeroing last few blocks of device
[2021-12-12 00:33:53,811][osd3][DEBUG ] find the location of an executable
[2021-12-12 00:33:53,814][osd3][INFO  ] Running command: sudo /usr/sbin/ceph-disk zap /dev/sdb
[2021-12-12 00:33:53,931][osd3][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[2021-12-12 00:33:53,931][osd3][WARNING] backup header from main header.
[2021-12-12 00:33:53,932][osd3][WARNING] 
[2021-12-12 00:33:53,932][osd3][WARNING] Warning! Main and backup partition tables differ! Use the 'c' and 'e' options
[2021-12-12 00:33:53,932][osd3][WARNING] on the recovery & transformation menu to examine the two tables.
[2021-12-12 00:33:53,932][osd3][WARNING] 
[2021-12-12 00:33:53,932][osd3][WARNING] Warning! One or more CRCs don't match. You should repair the disk!
[2021-12-12 00:33:53,932][osd3][WARNING] 
[2021-12-12 00:33:54,998][osd3][DEBUG ] ****************************************************************************
[2021-12-12 00:33:54,999][osd3][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[2021-12-12 00:33:54,999][osd3][DEBUG ] verification and recovery are STRONGLY recommended.
[2021-12-12 00:33:54,999][osd3][DEBUG ] ****************************************************************************
[2021-12-12 00:33:54,999][osd3][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[2021-12-12 00:33:54,999][osd3][DEBUG ] other utilities.
[2021-12-12 00:33:56,066][osd3][DEBUG ] Creating new GPT entries.
[2021-12-12 00:33:56,066][osd3][DEBUG ] The operation has completed successfully.
[2021-12-12 00:33:57,703][ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephuser/.cephdeploy.conf
[2021-12-12 00:33:57,703][ceph_deploy.cli][INFO  ] Invoked (1.5.39): /usr/bin/ceph-deploy disk list osd1 osd2 osd3
[2021-12-12 00:33:57,703][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2021-12-12 00:33:57,703][ceph_deploy.cli][INFO  ]  username                      : None
[2021-12-12 00:33:57,703][ceph_deploy.cli][INFO  ]  verbose                       : False
[2021-12-12 00:33:57,703][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2021-12-12 00:33:57,703][ceph_deploy.cli][INFO  ]  subcommand                    : list
[2021-12-12 00:33:57,703][ceph_deploy.cli][INFO  ]  quiet                         : False
[2021-12-12 00:33:57,703][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f589a82cef0>
[2021-12-12 00:33:57,703][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2021-12-12 00:33:57,703][ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7f589a8080c8>
[2021-12-12 00:33:57,703][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2021-12-12 00:33:57,703][ceph_deploy.cli][INFO  ]  default_release               : False
[2021-12-12 00:33:57,703][ceph_deploy.cli][INFO  ]  disk                          : [('osd1', None, None), ('osd2', None, None), ('osd3', None, None)]
[2021-12-12 00:33:57,871][osd1][DEBUG ] connection detected need for sudo
[2021-12-12 00:33:58,039][osd1][DEBUG ] connected to host: osd1 
[2021-12-12 00:33:58,040][osd1][DEBUG ] detect platform information from remote host
[2021-12-12 00:33:58,058][osd1][DEBUG ] detect machine type
[2021-12-12 00:33:58,063][osd1][DEBUG ] find the location of an executable
[2021-12-12 00:33:58,064][ceph_deploy.osd][INFO  ] Distro info: CentOS Linux 7.9.2009 Core
[2021-12-12 00:33:58,064][ceph_deploy.osd][DEBUG ] Listing disks on osd1...
[2021-12-12 00:33:58,065][osd1][DEBUG ] find the location of an executable
[2021-12-12 00:33:58,069][osd1][INFO  ] Running command: sudo /usr/sbin/ceph-disk list
[2021-12-12 00:33:58,238][osd1][DEBUG ] /dev/dm-0 other, xfs, mounted on /
[2021-12-12 00:33:58,239][osd1][DEBUG ] /dev/dm-1 swap, swap
[2021-12-12 00:33:58,239][osd1][DEBUG ] /dev/sda :
[2021-12-12 00:33:58,239][osd1][DEBUG ]  /dev/sda2 other, LVM2_member
[2021-12-12 00:33:58,239][osd1][DEBUG ]  /dev/sda1 other, xfs, mounted on /boot
[2021-12-12 00:33:58,239][osd1][DEBUG ] /dev/sdb other, unknown
[2021-12-12 00:33:58,239][osd1][DEBUG ] /dev/sr0 other, unknown
[2021-12-12 00:33:58,396][osd2][DEBUG ] connection detected need for sudo
[2021-12-12 00:33:58,566][osd2][DEBUG ] connected to host: osd2 
[2021-12-12 00:33:58,567][osd2][DEBUG ] detect platform information from remote host
[2021-12-12 00:33:58,584][osd2][DEBUG ] detect machine type
[2021-12-12 00:33:58,589][osd2][DEBUG ] find the location of an executable
[2021-12-12 00:33:58,590][ceph_deploy.osd][INFO  ] Distro info: CentOS Linux 7.9.2009 Core
[2021-12-12 00:33:58,590][ceph_deploy.osd][DEBUG ] Listing disks on osd2...
[2021-12-12 00:33:58,590][osd2][DEBUG ] find the location of an executable
[2021-12-12 00:33:58,593][osd2][INFO  ] Running command: sudo /usr/sbin/ceph-disk list
[2021-12-12 00:33:58,759][osd2][DEBUG ] /dev/dm-0 other, xfs, mounted on /
[2021-12-12 00:33:58,760][osd2][DEBUG ] /dev/dm-1 swap, swap
[2021-12-12 00:33:58,760][osd2][DEBUG ] /dev/sda :
[2021-12-12 00:33:58,760][osd2][DEBUG ]  /dev/sda2 other, LVM2_member
[2021-12-12 00:33:58,760][osd2][DEBUG ]  /dev/sda1 other, xfs, mounted on /boot
[2021-12-12 00:33:58,760][osd2][DEBUG ] /dev/sdb other, unknown
[2021-12-12 00:33:58,760][osd2][DEBUG ] /dev/sr0 other, unknown
[2021-12-12 00:33:58,913][osd3][DEBUG ] connection detected need for sudo
[2021-12-12 00:33:59,082][osd3][DEBUG ] connected to host: osd3 
[2021-12-12 00:33:59,083][osd3][DEBUG ] detect platform information from remote host
[2021-12-12 00:33:59,099][osd3][DEBUG ] detect machine type
[2021-12-12 00:33:59,104][osd3][DEBUG ] find the location of an executable
[2021-12-12 00:33:59,105][ceph_deploy.osd][INFO  ] Distro info: CentOS Linux 7.9.2009 Core
[2021-12-12 00:33:59,105][ceph_deploy.osd][DEBUG ] Listing disks on osd3...
[2021-12-12 00:33:59,105][osd3][DEBUG ] find the location of an executable
[2021-12-12 00:33:59,108][osd3][INFO  ] Running command: sudo /usr/sbin/ceph-disk list
[2021-12-12 00:33:59,224][osd3][DEBUG ] /dev/dm-0 other, xfs, mounted on /
[2021-12-12 00:33:59,225][osd3][DEBUG ] /dev/dm-1 swap, swap
[2021-12-12 00:33:59,225][osd3][DEBUG ] /dev/sda :
[2021-12-12 00:33:59,225][osd3][DEBUG ]  /dev/sda2 other, LVM2_member
[2021-12-12 00:33:59,225][osd3][DEBUG ]  /dev/sda1 other, xfs, mounted on /boot
[2021-12-12 00:33:59,225][osd3][DEBUG ] /dev/sdb other, unknown
[2021-12-12 00:34:12,908][ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephuser/.cephdeploy.conf
[2021-12-12 00:34:12,908][ceph_deploy.cli][INFO  ] Invoked (1.5.39): /usr/bin/ceph-deploy disk zap osd4:/dev/sdb osd5:/dev/sdb
[2021-12-12 00:34:12,908][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2021-12-12 00:34:12,908][ceph_deploy.cli][INFO  ]  username                      : None
[2021-12-12 00:34:12,908][ceph_deploy.cli][INFO  ]  verbose                       : False
[2021-12-12 00:34:12,908][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2021-12-12 00:34:12,908][ceph_deploy.cli][INFO  ]  subcommand                    : zap
[2021-12-12 00:34:12,908][ceph_deploy.cli][INFO  ]  quiet                         : False
[2021-12-12 00:34:12,909][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f1521b72ef0>
[2021-12-12 00:34:12,909][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2021-12-12 00:34:12,909][ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7f1521b4e0c8>
[2021-12-12 00:34:12,909][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2021-12-12 00:34:12,909][ceph_deploy.cli][INFO  ]  default_release               : False
[2021-12-12 00:34:12,909][ceph_deploy.cli][INFO  ]  disk                          : [('osd4', '/dev/sdb', None), ('osd5', '/dev/sdb', None)]
[2021-12-12 00:34:12,909][ceph_deploy.osd][DEBUG ] zapping /dev/sdb on osd4
[2021-12-12 00:34:13,203][osd4][DEBUG ] connection detected need for sudo
[2021-12-12 00:34:13,379][osd4][DEBUG ] connected to host: osd4 
[2021-12-12 00:34:13,379][osd4][DEBUG ] detect platform information from remote host
[2021-12-12 00:34:13,397][osd4][DEBUG ] detect machine type
[2021-12-12 00:34:13,402][osd4][DEBUG ] find the location of an executable
[2021-12-12 00:34:13,403][ceph_deploy.osd][INFO  ] Distro info: CentOS Linux 7.9.2009 Core
[2021-12-12 00:34:13,403][osd4][DEBUG ] zeroing last few blocks of device
[2021-12-12 00:34:13,404][osd4][DEBUG ] find the location of an executable
[2021-12-12 00:34:13,406][osd4][INFO  ] Running command: sudo /usr/sbin/ceph-disk zap /dev/sdb
[2021-12-12 00:34:13,524][osd4][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[2021-12-12 00:34:13,524][osd4][WARNING] backup header from main header.
[2021-12-12 00:34:13,525][osd4][WARNING] 
[2021-12-12 00:34:13,525][osd4][WARNING] Warning! Main and backup partition tables differ! Use the 'c' and 'e' options
[2021-12-12 00:34:13,525][osd4][WARNING] on the recovery & transformation menu to examine the two tables.
[2021-12-12 00:34:13,525][osd4][WARNING] 
[2021-12-12 00:34:13,525][osd4][WARNING] Warning! One or more CRCs don't match. You should repair the disk!
[2021-12-12 00:34:13,525][osd4][WARNING] 
[2021-12-12 00:34:14,692][osd4][DEBUG ] ****************************************************************************
[2021-12-12 00:34:14,692][osd4][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[2021-12-12 00:34:14,692][osd4][DEBUG ] verification and recovery are STRONGLY recommended.
[2021-12-12 00:34:14,692][osd4][DEBUG ] ****************************************************************************
[2021-12-12 00:34:14,692][osd4][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[2021-12-12 00:34:14,692][osd4][DEBUG ] other utilities.
[2021-12-12 00:34:15,859][osd4][DEBUG ] Creating new GPT entries.
[2021-12-12 00:34:15,859][osd4][DEBUG ] The operation has completed successfully.
[2021-12-12 00:34:15,891][ceph_deploy.osd][DEBUG ] zapping /dev/sdb on osd5
[2021-12-12 00:34:16,096][osd5][DEBUG ] connection detected need for sudo
[2021-12-12 00:34:16,269][osd5][DEBUG ] connected to host: osd5 
[2021-12-12 00:34:16,269][osd5][DEBUG ] detect platform information from remote host
[2021-12-12 00:34:16,287][osd5][DEBUG ] detect machine type
[2021-12-12 00:34:16,292][osd5][DEBUG ] find the location of an executable
[2021-12-12 00:34:16,293][ceph_deploy.osd][INFO  ] Distro info: CentOS Linux 7.9.2009 Core
[2021-12-12 00:34:16,293][osd5][DEBUG ] zeroing last few blocks of device
[2021-12-12 00:34:16,294][osd5][DEBUG ] find the location of an executable
[2021-12-12 00:34:16,297][osd5][INFO  ] Running command: sudo /usr/sbin/ceph-disk zap /dev/sdb
[2021-12-12 00:34:16,414][osd5][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[2021-12-12 00:34:16,414][osd5][WARNING] backup header from main header.
[2021-12-12 00:34:16,414][osd5][WARNING] 
[2021-12-12 00:34:16,414][osd5][WARNING] Warning! Main and backup partition tables differ! Use the 'c' and 'e' options
[2021-12-12 00:34:16,414][osd5][WARNING] on the recovery & transformation menu to examine the two tables.
[2021-12-12 00:34:16,414][osd5][WARNING] 
[2021-12-12 00:34:16,414][osd5][WARNING] Warning! One or more CRCs don't match. You should repair the disk!
[2021-12-12 00:34:16,414][osd5][WARNING] 
[2021-12-12 00:34:17,481][osd5][DEBUG ] ****************************************************************************
[2021-12-12 00:34:17,481][osd5][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[2021-12-12 00:34:17,481][osd5][DEBUG ] verification and recovery are STRONGLY recommended.
[2021-12-12 00:34:17,481][osd5][DEBUG ] ****************************************************************************
[2021-12-12 00:34:17,481][osd5][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[2021-12-12 00:34:17,481][osd5][DEBUG ] other utilities.
[2021-12-12 00:34:18,548][osd5][DEBUG ] Creating new GPT entries.
[2021-12-12 00:34:18,548][osd5][DEBUG ] The operation has completed successfully.
[2021-12-12 00:34:32,091][ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephuser/.cephdeploy.conf
[2021-12-12 00:34:32,091][ceph_deploy.cli][INFO  ] Invoked (1.5.39): /usr/bin/ceph-deploy osd prepare osd1:/dev/sdb osd2:/dev/sdb osd3:/dev/sdb osd4:/dev/sdb osd5:/dev/sdb
[2021-12-12 00:34:32,091][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2021-12-12 00:34:32,091][ceph_deploy.cli][INFO  ]  username                      : None
[2021-12-12 00:34:32,091][ceph_deploy.cli][INFO  ]  block_db                      : None
[2021-12-12 00:34:32,091][ceph_deploy.cli][INFO  ]  disk                          : [('osd1', '/dev/sdb', None), ('osd2', '/dev/sdb', None), ('osd3', '/dev/sdb', None), ('osd4', '/dev/sdb', None), ('osd5', '/dev/sdb', None)]
[2021-12-12 00:34:32,091][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2021-12-12 00:34:32,091][ceph_deploy.cli][INFO  ]  verbose                       : False
[2021-12-12 00:34:32,091][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2021-12-12 00:34:32,091][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2021-12-12 00:34:32,091][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2021-12-12 00:34:32,091][ceph_deploy.cli][INFO  ]  subcommand                    : prepare
[2021-12-12 00:34:32,091][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2021-12-12 00:34:32,092][ceph_deploy.cli][INFO  ]  quiet                         : False
[2021-12-12 00:34:32,092][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f8e370b5128>
[2021-12-12 00:34:32,092][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2021-12-12 00:34:32,092][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2021-12-12 00:34:32,092][ceph_deploy.cli][INFO  ]  filestore                     : None
[2021-12-12 00:34:32,092][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f8e37102050>
[2021-12-12 00:34:32,092][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2021-12-12 00:34:32,092][ceph_deploy.cli][INFO  ]  default_release               : False
[2021-12-12 00:34:32,092][ceph_deploy.cli][INFO  ]  zap_disk                      : False
[2021-12-12 00:34:32,092][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks osd1:/dev/sdb: osd2:/dev/sdb: osd3:/dev/sdb: osd4:/dev/sdb: osd5:/dev/sdb:
[2021-12-12 00:34:32,092][ceph_deploy][ERROR ] RuntimeError: bootstrap-osd keyring not found; run 'gatherkeys'

[2021-12-12 00:34:43,948][ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephuser/.cephdeploy.conf
[2021-12-12 00:34:43,948][ceph_deploy.cli][INFO  ] Invoked (1.5.39): /usr/bin/ceph-deploy mon create-initial
[2021-12-12 00:34:43,948][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2021-12-12 00:34:43,948][ceph_deploy.cli][INFO  ]  username                      : None
[2021-12-12 00:34:43,948][ceph_deploy.cli][INFO  ]  verbose                       : False
[2021-12-12 00:34:43,948][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2021-12-12 00:34:43,948][ceph_deploy.cli][INFO  ]  subcommand                    : create-initial
[2021-12-12 00:34:43,948][ceph_deploy.cli][INFO  ]  quiet                         : False
[2021-12-12 00:34:43,948][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f0128b8af38>
[2021-12-12 00:34:43,948][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2021-12-12 00:34:43,948][ceph_deploy.cli][INFO  ]  func                          : <function mon at 0x7f0128b71758>
[2021-12-12 00:34:43,948][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2021-12-12 00:34:43,948][ceph_deploy.cli][INFO  ]  default_release               : False
[2021-12-12 00:34:43,949][ceph_deploy.cli][INFO  ]  keyrings                      : None
[2021-12-12 00:34:43,949][ceph_deploy.mon][WARNING] keyring (ceph.mon.keyring) not found, creating a new one
[2021-12-12 00:34:43,949][ceph_deploy.new][DEBUG ] Creating a random mon key...
[2021-12-12 00:34:43,949][ceph_deploy.new][DEBUG ] Writing monitor keyring to ceph.mon.keyring...
[2021-12-12 00:34:43,949][ceph_deploy.mon][DEBUG ] Deploying mon, cluster ceph hosts mon1
[2021-12-12 00:34:43,950][ceph_deploy.mon][DEBUG ] detecting platform for host mon1 ...
[2021-12-12 00:34:44,166][mon1][DEBUG ] connection detected need for sudo
[2021-12-12 00:34:44,338][mon1][DEBUG ] connected to host: mon1 
[2021-12-12 00:34:44,339][mon1][DEBUG ] detect platform information from remote host
[2021-12-12 00:34:44,356][mon1][DEBUG ] detect machine type
[2021-12-12 00:34:44,360][mon1][DEBUG ] find the location of an executable
[2021-12-12 00:34:44,361][ceph_deploy.mon][INFO  ] distro info: CentOS Linux 7.9.2009 Core
[2021-12-12 00:34:44,362][mon1][DEBUG ] determining if provided host has same hostname in remote
[2021-12-12 00:34:44,362][mon1][DEBUG ] get remote short hostname
[2021-12-12 00:34:44,362][mon1][DEBUG ] deploying mon to mon1
[2021-12-12 00:34:44,362][mon1][DEBUG ] get remote short hostname
[2021-12-12 00:34:44,363][mon1][DEBUG ] remote hostname: mon1
[2021-12-12 00:34:44,365][mon1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2021-12-12 00:34:44,366][mon1][DEBUG ] create the mon path if it does not exist
[2021-12-12 00:34:44,367][mon1][DEBUG ] checking for done path: /var/lib/ceph/mon/ceph-mon1/done
[2021-12-12 00:34:44,368][mon1][DEBUG ] done path does not exist: /var/lib/ceph/mon/ceph-mon1/done
[2021-12-12 00:34:44,370][mon1][INFO  ] creating keyring file: /var/lib/ceph/tmp/ceph-mon1.mon.keyring
[2021-12-12 00:34:44,370][mon1][DEBUG ] create the monitor keyring file
[2021-12-12 00:34:44,372][mon1][INFO  ] Running command: sudo ceph-mon --cluster ceph --mkfs -i mon1 --keyring /var/lib/ceph/tmp/ceph-mon1.mon.keyring --setuser 167 --setgroup 167
[2021-12-12 00:34:44,440][mon1][DEBUG ] ceph-mon: mon.noname-a 192.168.1.93:6789/0 is local, renaming to mon.mon1
[2021-12-12 00:34:44,441][mon1][DEBUG ] ceph-mon: set fsid to 4cad720f-fee4-46d9-8302-57987e28e293
[2021-12-12 00:34:44,605][mon1][DEBUG ] ceph-mon: created monfs at /var/lib/ceph/mon/ceph-mon1 for mon.mon1
[2021-12-12 00:34:44,605][mon1][INFO  ] unlinking keyring file /var/lib/ceph/tmp/ceph-mon1.mon.keyring
[2021-12-12 00:34:44,607][mon1][DEBUG ] create a done file to avoid re-doing the mon deployment
[2021-12-12 00:34:44,608][mon1][DEBUG ] create the init path if it does not exist
[2021-12-12 00:34:44,610][mon1][INFO  ] Running command: sudo systemctl enable ceph.target
[2021-12-12 00:34:44,678][mon1][INFO  ] Running command: sudo systemctl enable ceph-mon@mon1
[2021-12-12 00:34:44,746][mon1][INFO  ] Running command: sudo systemctl start ceph-mon@mon1
[2021-12-12 00:34:46,817][mon1][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.mon1.asok mon_status
[2021-12-12 00:34:46,881][mon1][DEBUG ] ********************************************************************************
[2021-12-12 00:34:46,882][mon1][DEBUG ] status for monitor: mon.mon1
[2021-12-12 00:34:46,882][mon1][DEBUG ] {
[2021-12-12 00:34:46,882][mon1][DEBUG ]   "election_epoch": 3, 
[2021-12-12 00:34:46,882][mon1][DEBUG ]   "extra_probe_peers": [], 
[2021-12-12 00:34:46,882][mon1][DEBUG ]   "monmap": {
[2021-12-12 00:34:46,882][mon1][DEBUG ]     "created": "2021-12-12 00:34:44.419358", 
[2021-12-12 00:34:46,882][mon1][DEBUG ]     "epoch": 1, 
[2021-12-12 00:34:46,882][mon1][DEBUG ]     "fsid": "4cad720f-fee4-46d9-8302-57987e28e293", 
[2021-12-12 00:34:46,882][mon1][DEBUG ]     "modified": "2021-12-12 00:34:44.419358", 
[2021-12-12 00:34:46,882][mon1][DEBUG ]     "mons": [
[2021-12-12 00:34:46,882][mon1][DEBUG ]       {
[2021-12-12 00:34:46,882][mon1][DEBUG ]         "addr": "192.168.1.93:6789/0", 
[2021-12-12 00:34:46,882][mon1][DEBUG ]         "name": "mon1", 
[2021-12-12 00:34:46,882][mon1][DEBUG ]         "rank": 0
[2021-12-12 00:34:46,882][mon1][DEBUG ]       }
[2021-12-12 00:34:46,882][mon1][DEBUG ]     ]
[2021-12-12 00:34:46,882][mon1][DEBUG ]   }, 
[2021-12-12 00:34:46,882][mon1][DEBUG ]   "name": "mon1", 
[2021-12-12 00:34:46,882][mon1][DEBUG ]   "outside_quorum": [], 
[2021-12-12 00:34:46,882][mon1][DEBUG ]   "quorum": [
[2021-12-12 00:34:46,883][mon1][DEBUG ]     0
[2021-12-12 00:34:46,883][mon1][DEBUG ]   ], 
[2021-12-12 00:34:46,883][mon1][DEBUG ]   "rank": 0, 
[2021-12-12 00:34:46,883][mon1][DEBUG ]   "state": "leader", 
[2021-12-12 00:34:46,883][mon1][DEBUG ]   "sync_provider": []
[2021-12-12 00:34:46,883][mon1][DEBUG ] }
[2021-12-12 00:34:46,883][mon1][DEBUG ] ********************************************************************************
[2021-12-12 00:34:46,883][mon1][INFO  ] monitor: mon.mon1 is running
[2021-12-12 00:34:46,885][mon1][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.mon1.asok mon_status
[2021-12-12 00:34:46,950][ceph_deploy.mon][INFO  ] processing monitor mon.mon1
[2021-12-12 00:34:47,110][mon1][DEBUG ] connection detected need for sudo
[2021-12-12 00:34:47,273][mon1][DEBUG ] connected to host: mon1 
[2021-12-12 00:34:47,273][mon1][DEBUG ] detect platform information from remote host
[2021-12-12 00:34:47,290][mon1][DEBUG ] detect machine type
[2021-12-12 00:34:47,295][mon1][DEBUG ] find the location of an executable
[2021-12-12 00:34:47,297][mon1][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.mon1.asok mon_status
[2021-12-12 00:34:47,412][ceph_deploy.mon][INFO  ] mon.mon1 monitor has reached quorum!
[2021-12-12 00:34:47,413][ceph_deploy.mon][INFO  ] all initial monitors are running and have formed quorum
[2021-12-12 00:34:47,413][ceph_deploy.mon][INFO  ] Running gatherkeys...
[2021-12-12 00:34:47,413][ceph_deploy.gatherkeys][INFO  ] Storing keys in temp directory /tmp/tmpJPJTpT
[2021-12-12 00:34:47,577][mon1][DEBUG ] connection detected need for sudo
[2021-12-12 00:34:47,743][mon1][DEBUG ] connected to host: mon1 
[2021-12-12 00:34:47,743][mon1][DEBUG ] detect platform information from remote host
[2021-12-12 00:34:47,759][mon1][DEBUG ] detect machine type
[2021-12-12 00:34:47,764][mon1][DEBUG ] get remote short hostname
[2021-12-12 00:34:47,765][mon1][DEBUG ] fetch remote file
[2021-12-12 00:34:47,768][mon1][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --admin-daemon=/var/run/ceph/ceph-mon.mon1.asok mon_status
[2021-12-12 00:34:47,834][mon1][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-mon1/keyring auth get client.admin
[2021-12-12 00:34:48,002][mon1][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-mon1/keyring auth get client.bootstrap-mds
[2021-12-12 00:34:48,169][mon1][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-mon1/keyring auth get client.bootstrap-mgr
[2021-12-12 00:34:48,336][mon1][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-mon1/keyring auth get-or-create client.bootstrap-mgr mon allow profile bootstrap-mgr
[2021-12-12 00:34:48,704][mon1][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-mon1/keyring auth get client.bootstrap-osd
[2021-12-12 00:34:48,871][mon1][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-mon1/keyring auth get client.bootstrap-rgw
[2021-12-12 00:34:49,036][ceph_deploy.gatherkeys][INFO  ] Storing ceph.client.admin.keyring
[2021-12-12 00:34:49,036][ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-mds.keyring
[2021-12-12 00:34:49,066][ceph_deploy.gatherkeys][INFO  ] Replacing 'ceph.bootstrap-mgr.keyring' and backing up old key as 'ceph.bootstrap-mgr.keyring-20211212003449'
[2021-12-12 00:34:49,067][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.mon.keyring' already exists
[2021-12-12 00:34:49,067][ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-osd.keyring
[2021-12-12 00:34:49,067][ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-rgw.keyring
[2021-12-12 00:34:49,067][ceph_deploy.gatherkeys][INFO  ] Destroy temp directory /tmp/tmpJPJTpT
[2021-12-12 00:34:52,627][ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephuser/.cephdeploy.conf
[2021-12-12 00:34:52,627][ceph_deploy.cli][INFO  ] Invoked (1.5.39): /usr/bin/ceph-deploy gatherkeys mon1
[2021-12-12 00:34:52,627][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2021-12-12 00:34:52,628][ceph_deploy.cli][INFO  ]  username                      : None
[2021-12-12 00:34:52,628][ceph_deploy.cli][INFO  ]  verbose                       : False
[2021-12-12 00:34:52,628][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2021-12-12 00:34:52,628][ceph_deploy.cli][INFO  ]  quiet                         : False
[2021-12-12 00:34:52,628][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fc41e702878>
[2021-12-12 00:34:52,628][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2021-12-12 00:34:52,628][ceph_deploy.cli][INFO  ]  mon                           : ['mon1']
[2021-12-12 00:34:52,628][ceph_deploy.cli][INFO  ]  func                          : <function gatherkeys at 0x7fc41e6c6230>
[2021-12-12 00:34:52,628][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2021-12-12 00:34:52,628][ceph_deploy.cli][INFO  ]  default_release               : False
[2021-12-12 00:34:52,628][ceph_deploy.gatherkeys][INFO  ] Storing keys in temp directory /tmp/tmpzg9bsX
[2021-12-12 00:34:52,794][mon1][DEBUG ] connection detected need for sudo
[2021-12-12 00:34:52,959][mon1][DEBUG ] connected to host: mon1 
[2021-12-12 00:34:52,960][mon1][DEBUG ] detect platform information from remote host
[2021-12-12 00:34:52,976][mon1][DEBUG ] detect machine type
[2021-12-12 00:34:52,980][mon1][DEBUG ] get remote short hostname
[2021-12-12 00:34:52,981][mon1][DEBUG ] fetch remote file
[2021-12-12 00:34:52,984][mon1][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --admin-daemon=/var/run/ceph/ceph-mon.mon1.asok mon_status
[2021-12-12 00:34:53,053][mon1][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-mon1/keyring auth get client.admin
[2021-12-12 00:34:53,220][mon1][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-mon1/keyring auth get client.bootstrap-mds
[2021-12-12 00:34:53,386][mon1][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-mon1/keyring auth get client.bootstrap-mgr
[2021-12-12 00:34:53,553][mon1][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-mon1/keyring auth get client.bootstrap-osd
[2021-12-12 00:34:53,720][mon1][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-mon1/keyring auth get client.bootstrap-rgw
[2021-12-12 00:34:53,885][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.client.admin.keyring' already exists
[2021-12-12 00:34:53,885][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.bootstrap-mds.keyring' already exists
[2021-12-12 00:34:53,885][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.bootstrap-mgr.keyring' already exists
[2021-12-12 00:34:53,886][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.mon.keyring' already exists
[2021-12-12 00:34:53,886][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.bootstrap-osd.keyring' already exists
[2021-12-12 00:34:53,886][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.bootstrap-rgw.keyring' already exists
[2021-12-12 00:34:53,886][ceph_deploy.gatherkeys][INFO  ] Destroy temp directory /tmp/tmpzg9bsX
[2021-12-12 00:35:00,317][ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephuser/.cephdeploy.conf
[2021-12-12 00:35:00,317][ceph_deploy.cli][INFO  ] Invoked (1.5.39): /usr/bin/ceph-deploy osd prepare osd1:/dev/sdb osd2:/dev/sdb osd3:/dev/sdb osd4:/dev/sdb osd5:/dev/sdb
[2021-12-12 00:35:00,317][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2021-12-12 00:35:00,317][ceph_deploy.cli][INFO  ]  username                      : None
[2021-12-12 00:35:00,317][ceph_deploy.cli][INFO  ]  block_db                      : None
[2021-12-12 00:35:00,317][ceph_deploy.cli][INFO  ]  disk                          : [('osd1', '/dev/sdb', None), ('osd2', '/dev/sdb', None), ('osd3', '/dev/sdb', None), ('osd4', '/dev/sdb', None), ('osd5', '/dev/sdb', None)]
[2021-12-12 00:35:00,317][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2021-12-12 00:35:00,318][ceph_deploy.cli][INFO  ]  verbose                       : False
[2021-12-12 00:35:00,318][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2021-12-12 00:35:00,318][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2021-12-12 00:35:00,318][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2021-12-12 00:35:00,318][ceph_deploy.cli][INFO  ]  subcommand                    : prepare
[2021-12-12 00:35:00,318][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2021-12-12 00:35:00,318][ceph_deploy.cli][INFO  ]  quiet                         : False
[2021-12-12 00:35:00,318][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fd5bf675128>
[2021-12-12 00:35:00,318][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2021-12-12 00:35:00,318][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2021-12-12 00:35:00,318][ceph_deploy.cli][INFO  ]  filestore                     : None
[2021-12-12 00:35:00,318][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fd5bf6c2050>
[2021-12-12 00:35:00,319][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2021-12-12 00:35:00,319][ceph_deploy.cli][INFO  ]  default_release               : False
[2021-12-12 00:35:00,319][ceph_deploy.cli][INFO  ]  zap_disk                      : False
[2021-12-12 00:35:00,319][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks osd1:/dev/sdb: osd2:/dev/sdb: osd3:/dev/sdb: osd4:/dev/sdb: osd5:/dev/sdb:
[2021-12-12 00:35:00,486][osd1][DEBUG ] connection detected need for sudo
[2021-12-12 00:35:00,655][osd1][DEBUG ] connected to host: osd1 
[2021-12-12 00:35:00,656][osd1][DEBUG ] detect platform information from remote host
[2021-12-12 00:35:00,674][osd1][DEBUG ] detect machine type
[2021-12-12 00:35:00,679][osd1][DEBUG ] find the location of an executable
[2021-12-12 00:35:00,680][ceph_deploy.osd][INFO  ] Distro info: CentOS Linux 7.9.2009 Core
[2021-12-12 00:35:00,680][ceph_deploy.osd][DEBUG ] Deploying osd to osd1
[2021-12-12 00:35:00,680][osd1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2021-12-12 00:35:00,682][osd1][WARNING] osd keyring does not exist yet, creating one
[2021-12-12 00:35:00,682][osd1][DEBUG ] create a keyring file
[2021-12-12 00:35:00,684][ceph_deploy.osd][DEBUG ] Preparing host osd1 disk /dev/sdb journal None activate False
[2021-12-12 00:35:00,684][osd1][DEBUG ] find the location of an executable
[2021-12-12 00:35:00,687][osd1][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdb
[2021-12-12 00:35:00,755][osd1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2021-12-12 00:35:00,787][osd1][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --log-file $run_dir/$cluster-osd-check.log --cluster ceph --setuser ceph --setgroup ceph
[2021-12-12 00:35:00,795][osd1][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --log-file $run_dir/$cluster-osd-check.log --cluster ceph --setuser ceph --setgroup ceph
[2021-12-12 00:35:00,827][osd1][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --log-file $run_dir/$cluster-osd-check.log --cluster ceph --setuser ceph --setgroup ceph
[2021-12-12 00:35:00,830][osd1][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-12 00:35:00,830][osd1][WARNING] set_type: Will colocate journal with data on /dev/sdb
[2021-12-12 00:35:00,831][osd1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2021-12-12 00:35:00,846][osd1][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-12 00:35:00,846][osd1][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-12 00:35:00,846][osd1][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-12 00:35:00,846][osd1][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[2021-12-12 00:35:00,854][osd1][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[2021-12-12 00:35:00,870][osd1][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[2021-12-12 00:35:00,871][osd1][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[2021-12-12 00:35:00,887][osd1][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-12 00:35:00,887][osd1][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-12 00:35:00,887][osd1][WARNING] ptype_tobe_for_name: name = journal
[2021-12-12 00:35:00,887][osd1][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-12 00:35:00,887][osd1][WARNING] create_partition: Creating journal partition num 2 size 5120 on /dev/sdb
[2021-12-12 00:35:00,887][osd1][WARNING] command_check_call: Running command: /sbin/sgdisk --new=2:0:+5120M --change-name=2:ceph journal --partition-guid=2:ec6858a6-17e7-4cbf-a6ae-e4ed575c0ed8 --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sdb
[2021-12-12 00:35:02,054][osd1][DEBUG ] Setting name!
[2021-12-12 00:35:02,054][osd1][DEBUG ] partNum is 1
[2021-12-12 00:35:02,054][osd1][DEBUG ] REALLY setting name!
[2021-12-12 00:35:02,054][osd1][DEBUG ] The operation has completed successfully.
[2021-12-12 00:35:02,054][osd1][WARNING] update_partition: Calling partprobe on created device /dev/sdb
[2021-12-12 00:35:02,054][osd1][WARNING] command_check_call: Running command: /usr/bin/udevadm settle --timeout=600
[2021-12-12 00:35:02,168][osd1][WARNING] command: Running command: /usr/bin/flock -s /dev/sdb /sbin/partprobe /dev/sdb
[2021-12-12 00:35:02,232][osd1][WARNING] command_check_call: Running command: /usr/bin/udevadm settle --timeout=600
[2021-12-12 00:35:02,347][osd1][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-12 00:35:02,347][osd1][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-12 00:35:02,347][osd1][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb2 uuid path is /sys/dev/block/8:18/dm/uuid
[2021-12-12 00:35:02,347][osd1][WARNING] prepare_device: Journal is GPT partition /dev/disk/by-partuuid/ec6858a6-17e7-4cbf-a6ae-e4ed575c0ed8
[2021-12-12 00:35:02,347][osd1][WARNING] prepare_device: Journal is GPT partition /dev/disk/by-partuuid/ec6858a6-17e7-4cbf-a6ae-e4ed575c0ed8
[2021-12-12 00:35:02,347][osd1][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-12 00:35:02,347][osd1][WARNING] set_data_partition: Creating osd partition on /dev/sdb
[2021-12-12 00:35:02,347][osd1][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-12 00:35:02,347][osd1][WARNING] ptype_tobe_for_name: name = data
[2021-12-12 00:35:02,347][osd1][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-12 00:35:02,347][osd1][WARNING] create_partition: Creating data partition num 1 size 0 on /dev/sdb
[2021-12-12 00:35:02,347][osd1][WARNING] command_check_call: Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:b9a04588-4b94-48ef-8a05-aae80eb3f680 --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be --mbrtogpt -- /dev/sdb
[2021-12-12 00:35:03,565][osd1][DEBUG ] Setting name!
[2021-12-12 00:35:03,565][osd1][DEBUG ] partNum is 0
[2021-12-12 00:35:03,565][osd1][DEBUG ] REALLY setting name!
[2021-12-12 00:35:03,565][osd1][DEBUG ] The operation has completed successfully.
[2021-12-12 00:35:03,565][osd1][WARNING] update_partition: Calling partprobe on created device /dev/sdb
[2021-12-12 00:35:03,565][osd1][WARNING] command_check_call: Running command: /usr/bin/udevadm settle --timeout=600
[2021-12-12 00:35:03,679][osd1][WARNING] command: Running command: /usr/bin/flock -s /dev/sdb /sbin/partprobe /dev/sdb
[2021-12-12 00:35:03,743][osd1][WARNING] command_check_call: Running command: /usr/bin/udevadm settle --timeout=600
[2021-12-12 00:35:03,908][osd1][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-12 00:35:03,908][osd1][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-12 00:35:03,908][osd1][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb1 uuid path is /sys/dev/block/8:17/dm/uuid
[2021-12-12 00:35:03,908][osd1][WARNING] populate_data_path_device: Creating xfs fs on /dev/sdb1
[2021-12-12 00:35:03,908][osd1][WARNING] command_check_call: Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/sdb1
[2021-12-12 00:35:04,122][osd1][DEBUG ] Discarding blocks...Done.
[2021-12-12 00:35:04,123][osd1][DEBUG ] meta-data=/dev/sdb1              isize=2048   agcount=4, agsize=196543 blks
[2021-12-12 00:35:04,123][osd1][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=1
[2021-12-12 00:35:04,123][osd1][DEBUG ]          =                       crc=1        finobt=0, sparse=0
[2021-12-12 00:35:04,123][osd1][DEBUG ] data     =                       bsize=4096   blocks=786171, imaxpct=25
[2021-12-12 00:35:04,123][osd1][DEBUG ]          =                       sunit=0      swidth=0 blks
[2021-12-12 00:35:04,123][osd1][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0 ftype=1
[2021-12-12 00:35:04,123][osd1][DEBUG ] log      =internal log           bsize=4096   blocks=2560, version=2
[2021-12-12 00:35:04,123][osd1][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[2021-12-12 00:35:04,123][osd1][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[2021-12-12 00:35:04,123][osd1][WARNING] mount: Mounting /dev/sdb1 on /var/lib/ceph/tmp/mnt.QKLs49 with options noatime,inode64
[2021-12-12 00:35:04,123][osd1][WARNING] command_check_call: Running command: /usr/bin/mount -t xfs -o noatime,inode64 -- /dev/sdb1 /var/lib/ceph/tmp/mnt.QKLs49
[2021-12-12 00:35:04,123][osd1][WARNING] command: Running command: /sbin/restorecon /var/lib/ceph/tmp/mnt.QKLs49
[2021-12-12 00:35:04,126][osd1][WARNING] populate_data_path: Preparing osd data dir /var/lib/ceph/tmp/mnt.QKLs49
[2021-12-12 00:35:04,190][osd1][WARNING] command: Running command: /sbin/restorecon -R /var/lib/ceph/tmp/mnt.QKLs49/ceph_fsid.1719.tmp
[2021-12-12 00:35:04,194][osd1][WARNING] command: Running command: /usr/bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.QKLs49/ceph_fsid.1719.tmp
[2021-12-12 00:35:04,226][osd1][WARNING] command: Running command: /sbin/restorecon -R /var/lib/ceph/tmp/mnt.QKLs49/fsid.1719.tmp
[2021-12-12 00:35:04,226][osd1][WARNING] command: Running command: /usr/bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.QKLs49/fsid.1719.tmp
[2021-12-12 00:35:04,234][osd1][WARNING] command: Running command: /sbin/restorecon -R /var/lib/ceph/tmp/mnt.QKLs49/magic.1719.tmp
[2021-12-12 00:35:04,235][osd1][WARNING] command: Running command: /usr/bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.QKLs49/magic.1719.tmp
[2021-12-12 00:35:04,267][osd1][WARNING] command: Running command: /sbin/restorecon -R /var/lib/ceph/tmp/mnt.QKLs49/journal_uuid.1719.tmp
[2021-12-12 00:35:04,267][osd1][WARNING] command: Running command: /usr/bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.QKLs49/journal_uuid.1719.tmp
[2021-12-12 00:35:04,267][osd1][WARNING] adjust_symlink: Creating symlink /var/lib/ceph/tmp/mnt.QKLs49/journal -> /dev/disk/by-partuuid/ec6858a6-17e7-4cbf-a6ae-e4ed575c0ed8
[2021-12-12 00:35:04,267][osd1][WARNING] command: Running command: /sbin/restorecon -R /var/lib/ceph/tmp/mnt.QKLs49
[2021-12-12 00:35:04,267][osd1][WARNING] command: Running command: /usr/bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.QKLs49
[2021-12-12 00:35:04,268][osd1][WARNING] unmount: Unmounting /var/lib/ceph/tmp/mnt.QKLs49
[2021-12-12 00:35:04,268][osd1][WARNING] command_check_call: Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.QKLs49
[2021-12-12 00:35:04,382][osd1][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-12 00:35:04,382][osd1][WARNING] command_check_call: Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/sdb
[2021-12-12 00:35:05,499][osd1][DEBUG ] Warning: The kernel is still using the old partition table.
[2021-12-12 00:35:05,499][osd1][DEBUG ] The new table will be used at the next reboot.
[2021-12-12 00:35:05,499][osd1][DEBUG ] The operation has completed successfully.
[2021-12-12 00:35:05,499][osd1][WARNING] update_partition: Calling partprobe on prepared device /dev/sdb
[2021-12-12 00:35:05,499][osd1][WARNING] command_check_call: Running command: /usr/bin/udevadm settle --timeout=600
[2021-12-12 00:35:05,499][osd1][WARNING] command: Running command: /usr/bin/flock -s /dev/sdb /sbin/partprobe /dev/sdb
[2021-12-12 00:35:05,613][osd1][WARNING] command_check_call: Running command: /usr/bin/udevadm settle --timeout=600
[2021-12-12 00:35:05,727][osd1][WARNING] command_check_call: Running command: /usr/bin/udevadm trigger --action=add --sysname-match sdb1
[2021-12-12 00:35:10,733][osd1][INFO  ] checking OSD status...
[2021-12-12 00:35:10,733][osd1][DEBUG ] find the location of an executable
[2021-12-12 00:35:10,736][osd1][INFO  ] Running command: sudo /bin/ceph --cluster=ceph osd stat --format=json
[2021-12-12 00:35:10,851][ceph_deploy.osd][DEBUG ] Host osd1 is now ready for osd use.
[2021-12-12 00:35:11,015][osd2][DEBUG ] connection detected need for sudo
[2021-12-12 00:35:11,187][osd2][DEBUG ] connected to host: osd2 
[2021-12-12 00:35:11,188][osd2][DEBUG ] detect platform information from remote host
[2021-12-12 00:35:11,205][osd2][DEBUG ] detect machine type
[2021-12-12 00:35:11,210][osd2][DEBUG ] find the location of an executable
[2021-12-12 00:35:11,211][ceph_deploy.osd][INFO  ] Distro info: CentOS Linux 7.9.2009 Core
[2021-12-12 00:35:11,211][ceph_deploy.osd][DEBUG ] Deploying osd to osd2
[2021-12-12 00:35:11,212][osd2][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2021-12-12 00:35:11,213][osd2][WARNING] osd keyring does not exist yet, creating one
[2021-12-12 00:35:11,214][osd2][DEBUG ] create a keyring file
[2021-12-12 00:35:11,215][ceph_deploy.osd][DEBUG ] Preparing host osd2 disk /dev/sdb journal None activate False
[2021-12-12 00:35:11,215][osd2][DEBUG ] find the location of an executable
[2021-12-12 00:35:11,220][osd2][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdb
[2021-12-12 00:35:11,337][osd2][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2021-12-12 00:35:11,337][osd2][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --log-file $run_dir/$cluster-osd-check.log --cluster ceph --setuser ceph --setgroup ceph
[2021-12-12 00:35:11,337][osd2][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --log-file $run_dir/$cluster-osd-check.log --cluster ceph --setuser ceph --setgroup ceph
[2021-12-12 00:35:11,352][osd2][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --log-file $run_dir/$cluster-osd-check.log --cluster ceph --setuser ceph --setgroup ceph
[2021-12-12 00:35:11,368][osd2][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-12 00:35:11,368][osd2][WARNING] set_type: Will colocate journal with data on /dev/sdb
[2021-12-12 00:35:11,368][osd2][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2021-12-12 00:35:11,384][osd2][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-12 00:35:11,384][osd2][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-12 00:35:11,384][osd2][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-12 00:35:11,384][osd2][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[2021-12-12 00:35:11,392][osd2][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[2021-12-12 00:35:11,408][osd2][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[2021-12-12 00:35:11,409][osd2][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[2021-12-12 00:35:11,421][osd2][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-12 00:35:11,422][osd2][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-12 00:35:11,422][osd2][WARNING] ptype_tobe_for_name: name = journal
[2021-12-12 00:35:11,422][osd2][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-12 00:35:11,422][osd2][WARNING] create_partition: Creating journal partition num 2 size 5120 on /dev/sdb
[2021-12-12 00:35:11,422][osd2][WARNING] command_check_call: Running command: /sbin/sgdisk --new=2:0:+5120M --change-name=2:ceph journal --partition-guid=2:290bb198-6345-492b-818a-56c8604cbb3e --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sdb
[2021-12-12 00:35:12,589][osd2][DEBUG ] Setting name!
[2021-12-12 00:35:12,589][osd2][DEBUG ] partNum is 1
[2021-12-12 00:35:12,589][osd2][DEBUG ] REALLY setting name!
[2021-12-12 00:35:12,589][osd2][DEBUG ] The operation has completed successfully.
[2021-12-12 00:35:12,589][osd2][WARNING] update_partition: Calling partprobe on created device /dev/sdb
[2021-12-12 00:35:12,589][osd2][WARNING] command_check_call: Running command: /usr/bin/udevadm settle --timeout=600
[2021-12-12 00:35:12,703][osd2][WARNING] command: Running command: /usr/bin/flock -s /dev/sdb /sbin/partprobe /dev/sdb
[2021-12-12 00:35:12,768][osd2][WARNING] command_check_call: Running command: /usr/bin/udevadm settle --timeout=600
[2021-12-12 00:35:12,882][osd2][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-12 00:35:12,882][osd2][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-12 00:35:12,882][osd2][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb2 uuid path is /sys/dev/block/8:18/dm/uuid
[2021-12-12 00:35:12,882][osd2][WARNING] prepare_device: Journal is GPT partition /dev/disk/by-partuuid/290bb198-6345-492b-818a-56c8604cbb3e
[2021-12-12 00:35:12,882][osd2][WARNING] prepare_device: Journal is GPT partition /dev/disk/by-partuuid/290bb198-6345-492b-818a-56c8604cbb3e
[2021-12-12 00:35:12,882][osd2][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-12 00:35:12,882][osd2][WARNING] set_data_partition: Creating osd partition on /dev/sdb
[2021-12-12 00:35:12,882][osd2][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-12 00:35:12,883][osd2][WARNING] ptype_tobe_for_name: name = data
[2021-12-12 00:35:12,883][osd2][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-12 00:35:12,883][osd2][WARNING] create_partition: Creating data partition num 1 size 0 on /dev/sdb
[2021-12-12 00:35:12,883][osd2][WARNING] command_check_call: Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:dfc14f42-6b0f-4d7d-b0f7-9baeaed736a1 --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be --mbrtogpt -- /dev/sdb
[2021-12-12 00:35:14,050][osd2][DEBUG ] Setting name!
[2021-12-12 00:35:14,050][osd2][DEBUG ] partNum is 0
[2021-12-12 00:35:14,050][osd2][DEBUG ] REALLY setting name!
[2021-12-12 00:35:14,050][osd2][DEBUG ] The operation has completed successfully.
[2021-12-12 00:35:14,050][osd2][WARNING] update_partition: Calling partprobe on created device /dev/sdb
[2021-12-12 00:35:14,050][osd2][WARNING] command_check_call: Running command: /usr/bin/udevadm settle --timeout=600
[2021-12-12 00:35:14,165][osd2][WARNING] command: Running command: /usr/bin/flock -s /dev/sdb /sbin/partprobe /dev/sdb
[2021-12-12 00:35:14,229][osd2][WARNING] command_check_call: Running command: /usr/bin/udevadm settle --timeout=600
[2021-12-12 00:35:14,343][osd2][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-12 00:35:14,343][osd2][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-12 00:35:14,343][osd2][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb1 uuid path is /sys/dev/block/8:17/dm/uuid
[2021-12-12 00:35:14,343][osd2][WARNING] populate_data_path_device: Creating xfs fs on /dev/sdb1
[2021-12-12 00:35:14,343][osd2][WARNING] command_check_call: Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/sdb1
[2021-12-12 00:35:14,608][osd2][DEBUG ] Discarding blocks...Done.
[2021-12-12 00:35:14,608][osd2][DEBUG ] meta-data=/dev/sdb1              isize=2048   agcount=4, agsize=196543 blks
[2021-12-12 00:35:14,608][osd2][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=1
[2021-12-12 00:35:14,608][osd2][DEBUG ]          =                       crc=1        finobt=0, sparse=0
[2021-12-12 00:35:14,608][osd2][DEBUG ] data     =                       bsize=4096   blocks=786171, imaxpct=25
[2021-12-12 00:35:14,608][osd2][DEBUG ]          =                       sunit=0      swidth=0 blks
[2021-12-12 00:35:14,608][osd2][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0 ftype=1
[2021-12-12 00:35:14,608][osd2][DEBUG ] log      =internal log           bsize=4096   blocks=2560, version=2
[2021-12-12 00:35:14,608][osd2][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[2021-12-12 00:35:14,609][osd2][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[2021-12-12 00:35:14,609][osd2][WARNING] mount: Mounting /dev/sdb1 on /var/lib/ceph/tmp/mnt.PqYN2J with options noatime,inode64
[2021-12-12 00:35:14,609][osd2][WARNING] command_check_call: Running command: /usr/bin/mount -t xfs -o noatime,inode64 -- /dev/sdb1 /var/lib/ceph/tmp/mnt.PqYN2J
[2021-12-12 00:35:14,624][osd2][WARNING] command: Running command: /sbin/restorecon /var/lib/ceph/tmp/mnt.PqYN2J
[2021-12-12 00:35:14,624][osd2][WARNING] populate_data_path: Preparing osd data dir /var/lib/ceph/tmp/mnt.PqYN2J
[2021-12-12 00:35:14,738][osd2][WARNING] command: Running command: /sbin/restorecon -R /var/lib/ceph/tmp/mnt.PqYN2J/ceph_fsid.1675.tmp
[2021-12-12 00:35:14,739][osd2][WARNING] command: Running command: /usr/bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.PqYN2J/ceph_fsid.1675.tmp
[2021-12-12 00:35:14,739][osd2][WARNING] command: Running command: /sbin/restorecon -R /var/lib/ceph/tmp/mnt.PqYN2J/fsid.1675.tmp
[2021-12-12 00:35:14,739][osd2][WARNING] command: Running command: /usr/bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.PqYN2J/fsid.1675.tmp
[2021-12-12 00:35:14,771][osd2][WARNING] command: Running command: /sbin/restorecon -R /var/lib/ceph/tmp/mnt.PqYN2J/magic.1675.tmp
[2021-12-12 00:35:14,771][osd2][WARNING] command: Running command: /usr/bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.PqYN2J/magic.1675.tmp
[2021-12-12 00:35:14,786][osd2][WARNING] command: Running command: /sbin/restorecon -R /var/lib/ceph/tmp/mnt.PqYN2J/journal_uuid.1675.tmp
[2021-12-12 00:35:14,788][osd2][WARNING] command: Running command: /usr/bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.PqYN2J/journal_uuid.1675.tmp
[2021-12-12 00:35:14,791][osd2][WARNING] adjust_symlink: Creating symlink /var/lib/ceph/tmp/mnt.PqYN2J/journal -> /dev/disk/by-partuuid/290bb198-6345-492b-818a-56c8604cbb3e
[2021-12-12 00:35:14,791][osd2][WARNING] command: Running command: /sbin/restorecon -R /var/lib/ceph/tmp/mnt.PqYN2J
[2021-12-12 00:35:14,794][osd2][WARNING] command: Running command: /usr/bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.PqYN2J
[2021-12-12 00:35:14,798][osd2][WARNING] unmount: Unmounting /var/lib/ceph/tmp/mnt.PqYN2J
[2021-12-12 00:35:14,798][osd2][WARNING] command_check_call: Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.PqYN2J
[2021-12-12 00:35:15,013][osd2][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-12 00:35:15,013][osd2][WARNING] command_check_call: Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/sdb
[2021-12-12 00:35:16,180][osd2][DEBUG ] Warning: The kernel is still using the old partition table.
[2021-12-12 00:35:16,180][osd2][DEBUG ] The new table will be used at the next reboot.
[2021-12-12 00:35:16,180][osd2][DEBUG ] The operation has completed successfully.
[2021-12-12 00:35:16,180][osd2][WARNING] update_partition: Calling partprobe on prepared device /dev/sdb
[2021-12-12 00:35:16,180][osd2][WARNING] command_check_call: Running command: /usr/bin/udevadm settle --timeout=600
[2021-12-12 00:35:16,180][osd2][WARNING] command: Running command: /usr/bin/flock -s /dev/sdb /sbin/partprobe /dev/sdb
[2021-12-12 00:35:16,196][osd2][WARNING] command_check_call: Running command: /usr/bin/udevadm settle --timeout=600
[2021-12-12 00:35:16,310][osd2][WARNING] command_check_call: Running command: /usr/bin/udevadm trigger --action=add --sysname-match sdb1
[2021-12-12 00:35:21,316][osd2][INFO  ] checking OSD status...
[2021-12-12 00:35:21,316][osd2][DEBUG ] find the location of an executable
[2021-12-12 00:35:21,319][osd2][INFO  ] Running command: sudo /bin/ceph --cluster=ceph osd stat --format=json
[2021-12-12 00:35:21,434][ceph_deploy.osd][DEBUG ] Host osd2 is now ready for osd use.
[2021-12-12 00:35:21,588][osd3][DEBUG ] connection detected need for sudo
[2021-12-12 00:35:21,757][osd3][DEBUG ] connected to host: osd3 
[2021-12-12 00:35:21,758][osd3][DEBUG ] detect platform information from remote host
[2021-12-12 00:35:21,774][osd3][DEBUG ] detect machine type
[2021-12-12 00:35:21,779][osd3][DEBUG ] find the location of an executable
[2021-12-12 00:35:21,780][ceph_deploy.osd][INFO  ] Distro info: CentOS Linux 7.9.2009 Core
[2021-12-12 00:35:21,781][ceph_deploy.osd][DEBUG ] Deploying osd to osd3
[2021-12-12 00:35:21,781][osd3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2021-12-12 00:35:21,783][osd3][WARNING] osd keyring does not exist yet, creating one
[2021-12-12 00:35:21,784][osd3][DEBUG ] create a keyring file
[2021-12-12 00:35:21,785][ceph_deploy.osd][DEBUG ] Preparing host osd3 disk /dev/sdb journal None activate False
[2021-12-12 00:35:21,785][osd3][DEBUG ] find the location of an executable
[2021-12-12 00:35:21,788][osd3][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdb
[2021-12-12 00:35:21,855][osd3][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2021-12-12 00:35:21,887][osd3][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --log-file $run_dir/$cluster-osd-check.log --cluster ceph --setuser ceph --setgroup ceph
[2021-12-12 00:35:21,894][osd3][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --log-file $run_dir/$cluster-osd-check.log --cluster ceph --setuser ceph --setgroup ceph
[2021-12-12 00:35:21,910][osd3][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --log-file $run_dir/$cluster-osd-check.log --cluster ceph --setuser ceph --setgroup ceph
[2021-12-12 00:35:21,925][osd3][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-12 00:35:21,926][osd3][WARNING] set_type: Will colocate journal with data on /dev/sdb
[2021-12-12 00:35:21,926][osd3][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2021-12-12 00:35:21,941][osd3][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-12 00:35:21,941][osd3][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-12 00:35:21,941][osd3][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-12 00:35:21,941][osd3][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[2021-12-12 00:35:21,949][osd3][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[2021-12-12 00:35:21,956][osd3][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[2021-12-12 00:35:21,964][osd3][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[2021-12-12 00:35:21,972][osd3][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-12 00:35:21,972][osd3][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-12 00:35:21,972][osd3][WARNING] ptype_tobe_for_name: name = journal
[2021-12-12 00:35:21,972][osd3][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-12 00:35:21,972][osd3][WARNING] create_partition: Creating journal partition num 2 size 5120 on /dev/sdb
[2021-12-12 00:35:21,972][osd3][WARNING] command_check_call: Running command: /sbin/sgdisk --new=2:0:+5120M --change-name=2:ceph journal --partition-guid=2:72a31311-9807-4c48-ad39-53b55db3059f --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sdb
[2021-12-12 00:35:23,089][osd3][DEBUG ] Setting name!
[2021-12-12 00:35:23,089][osd3][DEBUG ] partNum is 1
[2021-12-12 00:35:23,089][osd3][DEBUG ] REALLY setting name!
[2021-12-12 00:35:23,089][osd3][DEBUG ] The operation has completed successfully.
[2021-12-12 00:35:23,089][osd3][WARNING] update_partition: Calling partprobe on created device /dev/sdb
[2021-12-12 00:35:23,089][osd3][WARNING] command_check_call: Running command: /usr/bin/udevadm settle --timeout=600
[2021-12-12 00:35:23,203][osd3][WARNING] command: Running command: /usr/bin/flock -s /dev/sdb /sbin/partprobe /dev/sdb
[2021-12-12 00:35:23,219][osd3][WARNING] command_check_call: Running command: /usr/bin/udevadm settle --timeout=600
[2021-12-12 00:35:23,383][osd3][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-12 00:35:23,384][osd3][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-12 00:35:23,384][osd3][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb2 uuid path is /sys/dev/block/8:18/dm/uuid
[2021-12-12 00:35:23,384][osd3][WARNING] prepare_device: Journal is GPT partition /dev/disk/by-partuuid/72a31311-9807-4c48-ad39-53b55db3059f
[2021-12-12 00:35:23,384][osd3][WARNING] prepare_device: Journal is GPT partition /dev/disk/by-partuuid/72a31311-9807-4c48-ad39-53b55db3059f
[2021-12-12 00:35:23,384][osd3][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-12 00:35:23,384][osd3][WARNING] set_data_partition: Creating osd partition on /dev/sdb
[2021-12-12 00:35:23,384][osd3][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-12 00:35:23,384][osd3][WARNING] ptype_tobe_for_name: name = data
[2021-12-12 00:35:23,384][osd3][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-12 00:35:23,384][osd3][WARNING] create_partition: Creating data partition num 1 size 0 on /dev/sdb
[2021-12-12 00:35:23,384][osd3][WARNING] command_check_call: Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:d04f6c35-8523-449b-8af6-46a343b9bcb8 --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be --mbrtogpt -- /dev/sdb
[2021-12-12 00:35:24,651][osd3][DEBUG ] Setting name!
[2021-12-12 00:35:24,652][osd3][DEBUG ] partNum is 0
[2021-12-12 00:35:24,652][osd3][DEBUG ] REALLY setting name!
[2021-12-12 00:35:24,652][osd3][DEBUG ] The operation has completed successfully.
[2021-12-12 00:35:24,652][osd3][WARNING] update_partition: Calling partprobe on created device /dev/sdb
[2021-12-12 00:35:24,652][osd3][WARNING] command_check_call: Running command: /usr/bin/udevadm settle --timeout=600
[2021-12-12 00:35:24,766][osd3][WARNING] command: Running command: /usr/bin/flock -s /dev/sdb /sbin/partprobe /dev/sdb
[2021-12-12 00:35:24,798][osd3][WARNING] command_check_call: Running command: /usr/bin/udevadm settle --timeout=600
[2021-12-12 00:35:24,962][osd3][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-12 00:35:24,962][osd3][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-12 00:35:24,962][osd3][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb1 uuid path is /sys/dev/block/8:17/dm/uuid
[2021-12-12 00:35:24,962][osd3][WARNING] populate_data_path_device: Creating xfs fs on /dev/sdb1
[2021-12-12 00:35:24,962][osd3][WARNING] command_check_call: Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/sdb1
[2021-12-12 00:35:25,227][osd3][DEBUG ] Discarding blocks...Done.
[2021-12-12 00:35:25,227][osd3][DEBUG ] meta-data=/dev/sdb1              isize=2048   agcount=4, agsize=196543 blks
[2021-12-12 00:35:25,227][osd3][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=1
[2021-12-12 00:35:25,227][osd3][DEBUG ]          =                       crc=1        finobt=0, sparse=0
[2021-12-12 00:35:25,227][osd3][DEBUG ] data     =                       bsize=4096   blocks=786171, imaxpct=25
[2021-12-12 00:35:25,227][osd3][DEBUG ]          =                       sunit=0      swidth=0 blks
[2021-12-12 00:35:25,227][osd3][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0 ftype=1
[2021-12-12 00:35:25,227][osd3][DEBUG ] log      =internal log           bsize=4096   blocks=2560, version=2
[2021-12-12 00:35:25,227][osd3][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[2021-12-12 00:35:25,227][osd3][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[2021-12-12 00:35:25,227][osd3][WARNING] mount: Mounting /dev/sdb1 on /var/lib/ceph/tmp/mnt.3GXDYT with options noatime,inode64
[2021-12-12 00:35:25,227][osd3][WARNING] command_check_call: Running command: /usr/bin/mount -t xfs -o noatime,inode64 -- /dev/sdb1 /var/lib/ceph/tmp/mnt.3GXDYT
[2021-12-12 00:35:25,243][osd3][WARNING] command: Running command: /sbin/restorecon /var/lib/ceph/tmp/mnt.3GXDYT
[2021-12-12 00:35:25,243][osd3][WARNING] populate_data_path: Preparing osd data dir /var/lib/ceph/tmp/mnt.3GXDYT
[2021-12-12 00:35:25,357][osd3][WARNING] command: Running command: /sbin/restorecon -R /var/lib/ceph/tmp/mnt.3GXDYT/ceph_fsid.1452.tmp
[2021-12-12 00:35:25,358][osd3][WARNING] command: Running command: /usr/bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.3GXDYT/ceph_fsid.1452.tmp
[2021-12-12 00:35:25,358][osd3][WARNING] command: Running command: /sbin/restorecon -R /var/lib/ceph/tmp/mnt.3GXDYT/fsid.1452.tmp
[2021-12-12 00:35:25,358][osd3][WARNING] command: Running command: /usr/bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.3GXDYT/fsid.1452.tmp
[2021-12-12 00:35:25,361][osd3][WARNING] command: Running command: /sbin/restorecon -R /var/lib/ceph/tmp/mnt.3GXDYT/magic.1452.tmp
[2021-12-12 00:35:25,365][osd3][WARNING] command: Running command: /usr/bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.3GXDYT/magic.1452.tmp
[2021-12-12 00:35:25,396][osd3][WARNING] command: Running command: /sbin/restorecon -R /var/lib/ceph/tmp/mnt.3GXDYT/journal_uuid.1452.tmp
[2021-12-12 00:35:25,396][osd3][WARNING] command: Running command: /usr/bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.3GXDYT/journal_uuid.1452.tmp
[2021-12-12 00:35:25,397][osd3][WARNING] adjust_symlink: Creating symlink /var/lib/ceph/tmp/mnt.3GXDYT/journal -> /dev/disk/by-partuuid/72a31311-9807-4c48-ad39-53b55db3059f
[2021-12-12 00:35:25,397][osd3][WARNING] command: Running command: /sbin/restorecon -R /var/lib/ceph/tmp/mnt.3GXDYT
[2021-12-12 00:35:25,397][osd3][WARNING] command: Running command: /usr/bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.3GXDYT
[2021-12-12 00:35:25,397][osd3][WARNING] unmount: Unmounting /var/lib/ceph/tmp/mnt.3GXDYT
[2021-12-12 00:35:25,397][osd3][WARNING] command_check_call: Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.3GXDYT
[2021-12-12 00:35:25,511][osd3][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-12 00:35:25,511][osd3][WARNING] command_check_call: Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/sdb
[2021-12-12 00:35:26,628][osd3][DEBUG ] Warning: The kernel is still using the old partition table.
[2021-12-12 00:35:26,628][osd3][DEBUG ] The new table will be used at the next reboot.
[2021-12-12 00:35:26,628][osd3][DEBUG ] The operation has completed successfully.
[2021-12-12 00:35:26,628][osd3][WARNING] update_partition: Calling partprobe on prepared device /dev/sdb
[2021-12-12 00:35:26,628][osd3][WARNING] command_check_call: Running command: /usr/bin/udevadm settle --timeout=600
[2021-12-12 00:35:26,628][osd3][WARNING] command: Running command: /usr/bin/flock -s /dev/sdb /sbin/partprobe /dev/sdb
[2021-12-12 00:35:26,628][osd3][WARNING] command_check_call: Running command: /usr/bin/udevadm settle --timeout=600
[2021-12-12 00:35:26,842][osd3][WARNING] command_check_call: Running command: /usr/bin/udevadm trigger --action=add --sysname-match sdb1
[2021-12-12 00:35:31,848][osd3][INFO  ] checking OSD status...
[2021-12-12 00:35:31,848][osd3][DEBUG ] find the location of an executable
[2021-12-12 00:35:31,852][osd3][INFO  ] Running command: sudo /bin/ceph --cluster=ceph osd stat --format=json
[2021-12-12 00:35:31,967][ceph_deploy.osd][DEBUG ] Host osd3 is now ready for osd use.
[2021-12-12 00:35:32,124][osd4][DEBUG ] connection detected need for sudo
[2021-12-12 00:35:32,293][osd4][DEBUG ] connected to host: osd4 
[2021-12-12 00:35:32,294][osd4][DEBUG ] detect platform information from remote host
[2021-12-12 00:35:32,311][osd4][DEBUG ] detect machine type
[2021-12-12 00:35:32,316][osd4][DEBUG ] find the location of an executable
[2021-12-12 00:35:32,317][ceph_deploy.osd][INFO  ] Distro info: CentOS Linux 7.9.2009 Core
[2021-12-12 00:35:32,317][ceph_deploy.osd][DEBUG ] Deploying osd to osd4
[2021-12-12 00:35:32,317][osd4][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2021-12-12 00:35:32,320][osd4][WARNING] osd keyring does not exist yet, creating one
[2021-12-12 00:35:32,320][osd4][DEBUG ] create a keyring file
[2021-12-12 00:35:32,321][ceph_deploy.osd][DEBUG ] Preparing host osd4 disk /dev/sdb journal None activate False
[2021-12-12 00:35:32,321][osd4][DEBUG ] find the location of an executable
[2021-12-12 00:35:32,323][osd4][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdb
[2021-12-12 00:35:32,390][osd4][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2021-12-12 00:35:32,421][osd4][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --log-file $run_dir/$cluster-osd-check.log --cluster ceph --setuser ceph --setgroup ceph
[2021-12-12 00:35:32,429][osd4][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --log-file $run_dir/$cluster-osd-check.log --cluster ceph --setuser ceph --setgroup ceph
[2021-12-12 00:35:32,461][osd4][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --log-file $run_dir/$cluster-osd-check.log --cluster ceph --setuser ceph --setgroup ceph
[2021-12-12 00:35:32,464][osd4][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-12 00:35:32,464][osd4][WARNING] set_type: Will colocate journal with data on /dev/sdb
[2021-12-12 00:35:32,464][osd4][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2021-12-12 00:35:32,480][osd4][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-12 00:35:32,480][osd4][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-12 00:35:32,480][osd4][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-12 00:35:32,480][osd4][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[2021-12-12 00:35:32,488][osd4][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[2021-12-12 00:35:32,495][osd4][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[2021-12-12 00:35:32,511][osd4][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[2021-12-12 00:35:32,514][osd4][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-12 00:35:32,514][osd4][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-12 00:35:32,514][osd4][WARNING] ptype_tobe_for_name: name = journal
[2021-12-12 00:35:32,514][osd4][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-12 00:35:32,514][osd4][WARNING] create_partition: Creating journal partition num 2 size 5120 on /dev/sdb
[2021-12-12 00:35:32,514][osd4][WARNING] command_check_call: Running command: /sbin/sgdisk --new=2:0:+5120M --change-name=2:ceph journal --partition-guid=2:37ee1f4f-477b-4ad5-92c3-77b1dc6d6e76 --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sdb
[2021-12-12 00:35:33,631][osd4][DEBUG ] Setting name!
[2021-12-12 00:35:33,631][osd4][DEBUG ] partNum is 1
[2021-12-12 00:35:33,631][osd4][DEBUG ] REALLY setting name!
[2021-12-12 00:35:33,631][osd4][DEBUG ] The operation has completed successfully.
[2021-12-12 00:35:33,631][osd4][WARNING] update_partition: Calling partprobe on created device /dev/sdb
[2021-12-12 00:35:33,631][osd4][WARNING] command_check_call: Running command: /usr/bin/udevadm settle --timeout=600
[2021-12-12 00:35:33,745][osd4][WARNING] command: Running command: /usr/bin/flock -s /dev/sdb /sbin/partprobe /dev/sdb
[2021-12-12 00:35:33,809][osd4][WARNING] command_check_call: Running command: /usr/bin/udevadm settle --timeout=600
[2021-12-12 00:35:33,873][osd4][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-12 00:35:33,873][osd4][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-12 00:35:33,874][osd4][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb2 uuid path is /sys/dev/block/8:18/dm/uuid
[2021-12-12 00:35:33,874][osd4][WARNING] prepare_device: Journal is GPT partition /dev/disk/by-partuuid/37ee1f4f-477b-4ad5-92c3-77b1dc6d6e76
[2021-12-12 00:35:33,874][osd4][WARNING] prepare_device: Journal is GPT partition /dev/disk/by-partuuid/37ee1f4f-477b-4ad5-92c3-77b1dc6d6e76
[2021-12-12 00:35:33,874][osd4][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-12 00:35:33,874][osd4][WARNING] set_data_partition: Creating osd partition on /dev/sdb
[2021-12-12 00:35:33,874][osd4][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-12 00:35:33,874][osd4][WARNING] ptype_tobe_for_name: name = data
[2021-12-12 00:35:33,874][osd4][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-12 00:35:33,874][osd4][WARNING] create_partition: Creating data partition num 1 size 0 on /dev/sdb
[2021-12-12 00:35:33,874][osd4][WARNING] command_check_call: Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:5fbc51e0-ed25-4bce-8912-013732f8abd3 --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be --mbrtogpt -- /dev/sdb
[2021-12-12 00:35:35,145][osd4][DEBUG ] Setting name!
[2021-12-12 00:35:35,145][osd4][DEBUG ] partNum is 0
[2021-12-12 00:35:35,145][osd4][DEBUG ] REALLY setting name!
[2021-12-12 00:35:35,146][osd4][DEBUG ] The operation has completed successfully.
[2021-12-12 00:35:35,146][osd4][WARNING] update_partition: Calling partprobe on created device /dev/sdb
[2021-12-12 00:35:35,146][osd4][WARNING] command_check_call: Running command: /usr/bin/udevadm settle --timeout=600
[2021-12-12 00:35:35,262][osd4][WARNING] command: Running command: /usr/bin/flock -s /dev/sdb /sbin/partprobe /dev/sdb
[2021-12-12 00:35:35,376][osd4][WARNING] command_check_call: Running command: /usr/bin/udevadm settle --timeout=600
[2021-12-12 00:35:35,490][osd4][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-12 00:35:35,490][osd4][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-12 00:35:35,490][osd4][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb1 uuid path is /sys/dev/block/8:17/dm/uuid
[2021-12-12 00:35:35,490][osd4][WARNING] populate_data_path_device: Creating xfs fs on /dev/sdb1
[2021-12-12 00:35:35,491][osd4][WARNING] command_check_call: Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/sdb1
[2021-12-12 00:35:35,805][osd4][DEBUG ] Discarding blocks...Done.
[2021-12-12 00:35:35,805][osd4][DEBUG ] meta-data=/dev/sdb1              isize=2048   agcount=4, agsize=196543 blks
[2021-12-12 00:35:35,805][osd4][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=1
[2021-12-12 00:35:35,806][osd4][DEBUG ]          =                       crc=1        finobt=0, sparse=0
[2021-12-12 00:35:35,806][osd4][DEBUG ] data     =                       bsize=4096   blocks=786171, imaxpct=25
[2021-12-12 00:35:35,806][osd4][DEBUG ]          =                       sunit=0      swidth=0 blks
[2021-12-12 00:35:35,806][osd4][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0 ftype=1
[2021-12-12 00:35:35,806][osd4][DEBUG ] log      =internal log           bsize=4096   blocks=2560, version=2
[2021-12-12 00:35:35,806][osd4][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[2021-12-12 00:35:35,806][osd4][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[2021-12-12 00:35:35,806][osd4][WARNING] mount: Mounting /dev/sdb1 on /var/lib/ceph/tmp/mnt.k1A1TA with options noatime,inode64
[2021-12-12 00:35:35,806][osd4][WARNING] command_check_call: Running command: /usr/bin/mount -t xfs -o noatime,inode64 -- /dev/sdb1 /var/lib/ceph/tmp/mnt.k1A1TA
[2021-12-12 00:35:35,806][osd4][WARNING] command: Running command: /sbin/restorecon /var/lib/ceph/tmp/mnt.k1A1TA
[2021-12-12 00:35:35,806][osd4][WARNING] populate_data_path: Preparing osd data dir /var/lib/ceph/tmp/mnt.k1A1TA
[2021-12-12 00:35:35,920][osd4][WARNING] command: Running command: /sbin/restorecon -R /var/lib/ceph/tmp/mnt.k1A1TA/ceph_fsid.1321.tmp
[2021-12-12 00:35:35,921][osd4][WARNING] command: Running command: /usr/bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.k1A1TA/ceph_fsid.1321.tmp
[2021-12-12 00:35:35,953][osd4][WARNING] command: Running command: /sbin/restorecon -R /var/lib/ceph/tmp/mnt.k1A1TA/fsid.1321.tmp
[2021-12-12 00:35:35,953][osd4][WARNING] command: Running command: /usr/bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.k1A1TA/fsid.1321.tmp
[2021-12-12 00:35:35,969][osd4][WARNING] command: Running command: /sbin/restorecon -R /var/lib/ceph/tmp/mnt.k1A1TA/magic.1321.tmp
[2021-12-12 00:35:35,969][osd4][WARNING] command: Running command: /usr/bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.k1A1TA/magic.1321.tmp
[2021-12-12 00:35:36,033][osd4][WARNING] command: Running command: /sbin/restorecon -R /var/lib/ceph/tmp/mnt.k1A1TA/journal_uuid.1321.tmp
[2021-12-12 00:35:36,033][osd4][WARNING] command: Running command: /usr/bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.k1A1TA/journal_uuid.1321.tmp
[2021-12-12 00:35:36,033][osd4][WARNING] adjust_symlink: Creating symlink /var/lib/ceph/tmp/mnt.k1A1TA/journal -> /dev/disk/by-partuuid/37ee1f4f-477b-4ad5-92c3-77b1dc6d6e76
[2021-12-12 00:35:36,033][osd4][WARNING] command: Running command: /sbin/restorecon -R /var/lib/ceph/tmp/mnt.k1A1TA
[2021-12-12 00:35:36,033][osd4][WARNING] command: Running command: /usr/bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.k1A1TA
[2021-12-12 00:35:36,033][osd4][WARNING] unmount: Unmounting /var/lib/ceph/tmp/mnt.k1A1TA
[2021-12-12 00:35:36,033][osd4][WARNING] command_check_call: Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.k1A1TA
[2021-12-12 00:35:36,197][osd4][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-12 00:35:36,198][osd4][WARNING] command_check_call: Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/sdb
[2021-12-12 00:35:37,315][osd4][DEBUG ] Warning: The kernel is still using the old partition table.
[2021-12-12 00:35:37,315][osd4][DEBUG ] The new table will be used at the next reboot.
[2021-12-12 00:35:37,315][osd4][DEBUG ] The operation has completed successfully.
[2021-12-12 00:35:37,315][osd4][WARNING] update_partition: Calling partprobe on prepared device /dev/sdb
[2021-12-12 00:35:37,315][osd4][WARNING] command_check_call: Running command: /usr/bin/udevadm settle --timeout=600
[2021-12-12 00:35:37,315][osd4][WARNING] command: Running command: /usr/bin/flock -s /dev/sdb /sbin/partprobe /dev/sdb
[2021-12-12 00:35:37,379][osd4][WARNING] command_check_call: Running command: /usr/bin/udevadm settle --timeout=600
[2021-12-12 00:35:37,386][osd4][WARNING] command_check_call: Running command: /usr/bin/udevadm trigger --action=add --sysname-match sdb1
[2021-12-12 00:35:42,407][osd4][INFO  ] checking OSD status...
[2021-12-12 00:35:42,408][osd4][DEBUG ] find the location of an executable
[2021-12-12 00:35:42,410][osd4][INFO  ] Running command: sudo /bin/ceph --cluster=ceph osd stat --format=json
[2021-12-12 00:35:42,525][ceph_deploy.osd][DEBUG ] Host osd4 is now ready for osd use.
[2021-12-12 00:35:42,678][osd5][DEBUG ] connection detected need for sudo
[2021-12-12 00:35:42,846][osd5][DEBUG ] connected to host: osd5 
[2021-12-12 00:35:42,847][osd5][DEBUG ] detect platform information from remote host
[2021-12-12 00:35:42,864][osd5][DEBUG ] detect machine type
[2021-12-12 00:35:42,869][osd5][DEBUG ] find the location of an executable
[2021-12-12 00:35:42,871][ceph_deploy.osd][INFO  ] Distro info: CentOS Linux 7.9.2009 Core
[2021-12-12 00:35:42,871][ceph_deploy.osd][DEBUG ] Deploying osd to osd5
[2021-12-12 00:35:42,871][osd5][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2021-12-12 00:35:42,874][osd5][WARNING] osd keyring does not exist yet, creating one
[2021-12-12 00:35:42,874][osd5][DEBUG ] create a keyring file
[2021-12-12 00:35:42,876][ceph_deploy.osd][DEBUG ] Preparing host osd5 disk /dev/sdb journal None activate False
[2021-12-12 00:35:42,876][osd5][DEBUG ] find the location of an executable
[2021-12-12 00:35:42,879][osd5][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdb
[2021-12-12 00:35:42,945][osd5][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2021-12-12 00:35:42,961][osd5][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --log-file $run_dir/$cluster-osd-check.log --cluster ceph --setuser ceph --setgroup ceph
[2021-12-12 00:35:42,977][osd5][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --log-file $run_dir/$cluster-osd-check.log --cluster ceph --setuser ceph --setgroup ceph
[2021-12-12 00:35:42,992][osd5][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --log-file $run_dir/$cluster-osd-check.log --cluster ceph --setuser ceph --setgroup ceph
[2021-12-12 00:35:43,008][osd5][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-12 00:35:43,008][osd5][WARNING] set_type: Will colocate journal with data on /dev/sdb
[2021-12-12 00:35:43,008][osd5][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2021-12-12 00:35:43,024][osd5][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-12 00:35:43,024][osd5][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-12 00:35:43,024][osd5][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-12 00:35:43,024][osd5][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[2021-12-12 00:35:43,031][osd5][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[2021-12-12 00:35:43,039][osd5][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[2021-12-12 00:35:43,047][osd5][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[2021-12-12 00:35:43,054][osd5][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-12 00:35:43,054][osd5][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-12 00:35:43,054][osd5][WARNING] ptype_tobe_for_name: name = journal
[2021-12-12 00:35:43,054][osd5][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-12 00:35:43,055][osd5][WARNING] create_partition: Creating journal partition num 2 size 5120 on /dev/sdb
[2021-12-12 00:35:43,055][osd5][WARNING] command_check_call: Running command: /sbin/sgdisk --new=2:0:+5120M --change-name=2:ceph journal --partition-guid=2:e8423030-aeeb-4af5-bd12-db6d4468eab4 --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sdb
[2021-12-12 00:35:44,122][osd5][DEBUG ] Setting name!
[2021-12-12 00:35:44,122][osd5][DEBUG ] partNum is 1
[2021-12-12 00:35:44,122][osd5][DEBUG ] REALLY setting name!
[2021-12-12 00:35:44,122][osd5][DEBUG ] The operation has completed successfully.
[2021-12-12 00:35:44,122][osd5][WARNING] update_partition: Calling partprobe on created device /dev/sdb
[2021-12-12 00:35:44,122][osd5][WARNING] command_check_call: Running command: /usr/bin/udevadm settle --timeout=600
[2021-12-12 00:35:44,236][osd5][WARNING] command: Running command: /usr/bin/flock -s /dev/sdb /sbin/partprobe /dev/sdb
[2021-12-12 00:35:44,300][osd5][WARNING] command_check_call: Running command: /usr/bin/udevadm settle --timeout=600
[2021-12-12 00:35:44,332][osd5][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-12 00:35:44,332][osd5][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-12 00:35:44,332][osd5][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb2 uuid path is /sys/dev/block/8:18/dm/uuid
[2021-12-12 00:35:44,332][osd5][WARNING] prepare_device: Journal is GPT partition /dev/disk/by-partuuid/e8423030-aeeb-4af5-bd12-db6d4468eab4
[2021-12-12 00:35:44,332][osd5][WARNING] prepare_device: Journal is GPT partition /dev/disk/by-partuuid/e8423030-aeeb-4af5-bd12-db6d4468eab4
[2021-12-12 00:35:44,333][osd5][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-12 00:35:44,333][osd5][WARNING] set_data_partition: Creating osd partition on /dev/sdb
[2021-12-12 00:35:44,333][osd5][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-12 00:35:44,333][osd5][WARNING] ptype_tobe_for_name: name = data
[2021-12-12 00:35:44,333][osd5][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-12 00:35:44,333][osd5][WARNING] create_partition: Creating data partition num 1 size 0 on /dev/sdb
[2021-12-12 00:35:44,333][osd5][WARNING] command_check_call: Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:fe9d2e1f-2b37-4657-a533-d0bd5c6e7aae --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be --mbrtogpt -- /dev/sdb
[2021-12-12 00:35:45,450][osd5][DEBUG ] Setting name!
[2021-12-12 00:35:45,450][osd5][DEBUG ] partNum is 0
[2021-12-12 00:35:45,450][osd5][DEBUG ] REALLY setting name!
[2021-12-12 00:35:45,450][osd5][DEBUG ] The operation has completed successfully.
[2021-12-12 00:35:45,450][osd5][WARNING] update_partition: Calling partprobe on created device /dev/sdb
[2021-12-12 00:35:45,450][osd5][WARNING] command_check_call: Running command: /usr/bin/udevadm settle --timeout=600
[2021-12-12 00:35:45,564][osd5][WARNING] command: Running command: /usr/bin/flock -s /dev/sdb /sbin/partprobe /dev/sdb
[2021-12-12 00:35:45,628][osd5][WARNING] command_check_call: Running command: /usr/bin/udevadm settle --timeout=600
[2021-12-12 00:35:45,742][osd5][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-12 00:35:45,742][osd5][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-12 00:35:45,743][osd5][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb1 uuid path is /sys/dev/block/8:17/dm/uuid
[2021-12-12 00:35:45,743][osd5][WARNING] populate_data_path_device: Creating xfs fs on /dev/sdb1
[2021-12-12 00:35:45,743][osd5][WARNING] command_check_call: Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/sdb1
[2021-12-12 00:35:46,007][osd5][DEBUG ] Discarding blocks...Done.
[2021-12-12 00:35:46,007][osd5][DEBUG ] meta-data=/dev/sdb1              isize=2048   agcount=4, agsize=196543 blks
[2021-12-12 00:35:46,007][osd5][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=1
[2021-12-12 00:35:46,008][osd5][DEBUG ]          =                       crc=1        finobt=0, sparse=0
[2021-12-12 00:35:46,008][osd5][DEBUG ] data     =                       bsize=4096   blocks=786171, imaxpct=25
[2021-12-12 00:35:46,008][osd5][DEBUG ]          =                       sunit=0      swidth=0 blks
[2021-12-12 00:35:46,008][osd5][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0 ftype=1
[2021-12-12 00:35:46,008][osd5][DEBUG ] log      =internal log           bsize=4096   blocks=2560, version=2
[2021-12-12 00:35:46,008][osd5][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[2021-12-12 00:35:46,008][osd5][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[2021-12-12 00:35:46,008][osd5][WARNING] mount: Mounting /dev/sdb1 on /var/lib/ceph/tmp/mnt.9Bv1iw with options noatime,inode64
[2021-12-12 00:35:46,008][osd5][WARNING] command_check_call: Running command: /usr/bin/mount -t xfs -o noatime,inode64 -- /dev/sdb1 /var/lib/ceph/tmp/mnt.9Bv1iw
[2021-12-12 00:35:46,008][osd5][WARNING] command: Running command: /sbin/restorecon /var/lib/ceph/tmp/mnt.9Bv1iw
[2021-12-12 00:35:46,008][osd5][WARNING] populate_data_path: Preparing osd data dir /var/lib/ceph/tmp/mnt.9Bv1iw
[2021-12-12 00:35:46,072][osd5][WARNING] command: Running command: /sbin/restorecon -R /var/lib/ceph/tmp/mnt.9Bv1iw/ceph_fsid.1537.tmp
[2021-12-12 00:35:46,076][osd5][WARNING] command: Running command: /usr/bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.9Bv1iw/ceph_fsid.1537.tmp
[2021-12-12 00:35:46,140][osd5][WARNING] command: Running command: /sbin/restorecon -R /var/lib/ceph/tmp/mnt.9Bv1iw/fsid.1537.tmp
[2021-12-12 00:35:46,140][osd5][WARNING] command: Running command: /usr/bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.9Bv1iw/fsid.1537.tmp
[2021-12-12 00:35:46,140][osd5][WARNING] command: Running command: /sbin/restorecon -R /var/lib/ceph/tmp/mnt.9Bv1iw/magic.1537.tmp
[2021-12-12 00:35:46,140][osd5][WARNING] command: Running command: /usr/bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.9Bv1iw/magic.1537.tmp
[2021-12-12 00:35:46,172][osd5][WARNING] command: Running command: /sbin/restorecon -R /var/lib/ceph/tmp/mnt.9Bv1iw/journal_uuid.1537.tmp
[2021-12-12 00:35:46,172][osd5][WARNING] command: Running command: /usr/bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.9Bv1iw/journal_uuid.1537.tmp
[2021-12-12 00:35:46,172][osd5][WARNING] adjust_symlink: Creating symlink /var/lib/ceph/tmp/mnt.9Bv1iw/journal -> /dev/disk/by-partuuid/e8423030-aeeb-4af5-bd12-db6d4468eab4
[2021-12-12 00:35:46,172][osd5][WARNING] command: Running command: /sbin/restorecon -R /var/lib/ceph/tmp/mnt.9Bv1iw
[2021-12-12 00:35:46,172][osd5][WARNING] command: Running command: /usr/bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.9Bv1iw
[2021-12-12 00:35:46,174][osd5][WARNING] unmount: Unmounting /var/lib/ceph/tmp/mnt.9Bv1iw
[2021-12-12 00:35:46,174][osd5][WARNING] command_check_call: Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.9Bv1iw
[2021-12-12 00:35:46,288][osd5][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-12 00:35:46,288][osd5][WARNING] command_check_call: Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/sdb
[2021-12-12 00:35:47,405][osd5][DEBUG ] Warning: The kernel is still using the old partition table.
[2021-12-12 00:35:47,405][osd5][DEBUG ] The new table will be used at the next reboot.
[2021-12-12 00:35:47,405][osd5][DEBUG ] The operation has completed successfully.
[2021-12-12 00:35:47,405][osd5][WARNING] update_partition: Calling partprobe on prepared device /dev/sdb
[2021-12-12 00:35:47,405][osd5][WARNING] command_check_call: Running command: /usr/bin/udevadm settle --timeout=600
[2021-12-12 00:35:47,405][osd5][WARNING] command: Running command: /usr/bin/flock -s /dev/sdb /sbin/partprobe /dev/sdb
[2021-12-12 00:35:47,405][osd5][WARNING] command_check_call: Running command: /usr/bin/udevadm settle --timeout=600
[2021-12-12 00:35:47,519][osd5][WARNING] command_check_call: Running command: /usr/bin/udevadm trigger --action=add --sysname-match sdb1
[2021-12-12 00:35:52,532][osd5][INFO  ] checking OSD status...
[2021-12-12 00:35:52,532][osd5][DEBUG ] find the location of an executable
[2021-12-12 00:35:52,535][osd5][INFO  ] Running command: sudo /bin/ceph --cluster=ceph osd stat --format=json
[2021-12-12 00:35:52,650][ceph_deploy.osd][DEBUG ] Host osd5 is now ready for osd use.
[2021-12-12 00:36:18,922][ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephuser/.cephdeploy.conf
[2021-12-12 00:36:18,922][ceph_deploy.cli][INFO  ] Invoked (1.5.39): /usr/bin/ceph-deploy admin ceph-admin mon1 osd1 osd2 osd3 osd4 osd5 client client2 client3
[2021-12-12 00:36:18,922][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2021-12-12 00:36:18,922][ceph_deploy.cli][INFO  ]  username                      : None
[2021-12-12 00:36:18,922][ceph_deploy.cli][INFO  ]  verbose                       : False
[2021-12-12 00:36:18,922][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2021-12-12 00:36:18,922][ceph_deploy.cli][INFO  ]  quiet                         : False
[2021-12-12 00:36:18,922][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f65b5f79b48>
[2021-12-12 00:36:18,922][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2021-12-12 00:36:18,922][ceph_deploy.cli][INFO  ]  client                        : ['ceph-admin', 'mon1', 'osd1', 'osd2', 'osd3', 'osd4', 'osd5', 'client', 'client2', 'client3']
[2021-12-12 00:36:18,922][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7f65b6c92a28>
[2021-12-12 00:36:18,922][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2021-12-12 00:36:18,922][ceph_deploy.cli][INFO  ]  default_release               : False
[2021-12-12 00:36:18,923][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to ceph-admin
[2021-12-12 00:36:19,021][ceph-admin][DEBUG ] connection detected need for sudo
[2021-12-12 00:36:19,047][ceph-admin][DEBUG ] connected to host: ceph-admin 
[2021-12-12 00:36:19,047][ceph-admin][DEBUG ] detect platform information from remote host
[2021-12-12 00:36:19,067][ceph-admin][DEBUG ] detect machine type
[2021-12-12 00:36:19,071][ceph-admin][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2021-12-12 00:36:19,072][ceph_deploy.admin][ERROR ] RuntimeError: config file /etc/ceph/ceph.conf exists with different content; use --overwrite-conf to overwrite
[2021-12-12 00:36:19,072][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to mon1
[2021-12-12 00:36:19,222][mon1][DEBUG ] connection detected need for sudo
[2021-12-12 00:36:19,391][mon1][DEBUG ] connected to host: mon1 
[2021-12-12 00:36:19,392][mon1][DEBUG ] detect platform information from remote host
[2021-12-12 00:36:19,409][mon1][DEBUG ] detect machine type
[2021-12-12 00:36:19,414][mon1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2021-12-12 00:36:19,417][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to osd1
[2021-12-12 00:36:19,575][osd1][DEBUG ] connection detected need for sudo
[2021-12-12 00:36:19,744][osd1][DEBUG ] connected to host: osd1 
[2021-12-12 00:36:19,744][osd1][DEBUG ] detect platform information from remote host
[2021-12-12 00:36:19,762][osd1][DEBUG ] detect machine type
[2021-12-12 00:36:19,766][osd1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2021-12-12 00:36:19,769][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to osd2
[2021-12-12 00:36:19,926][osd2][DEBUG ] connection detected need for sudo
[2021-12-12 00:36:20,098][osd2][DEBUG ] connected to host: osd2 
[2021-12-12 00:36:20,099][osd2][DEBUG ] detect platform information from remote host
[2021-12-12 00:36:20,117][osd2][DEBUG ] detect machine type
[2021-12-12 00:36:20,122][osd2][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2021-12-12 00:36:20,124][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to osd3
[2021-12-12 00:36:20,279][osd3][DEBUG ] connection detected need for sudo
[2021-12-12 00:36:20,449][osd3][DEBUG ] connected to host: osd3 
[2021-12-12 00:36:20,449][osd3][DEBUG ] detect platform information from remote host
[2021-12-12 00:36:20,470][osd3][DEBUG ] detect machine type
[2021-12-12 00:36:20,475][osd3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2021-12-12 00:36:20,479][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to osd4
[2021-12-12 00:36:20,634][osd4][DEBUG ] connection detected need for sudo
[2021-12-12 00:36:20,803][osd4][DEBUG ] connected to host: osd4 
[2021-12-12 00:36:20,803][osd4][DEBUG ] detect platform information from remote host
[2021-12-12 00:36:20,820][osd4][DEBUG ] detect machine type
[2021-12-12 00:36:20,825][osd4][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2021-12-12 00:36:20,827][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to osd5
[2021-12-12 00:36:20,984][osd5][DEBUG ] connection detected need for sudo
[2021-12-12 00:36:21,157][osd5][DEBUG ] connected to host: osd5 
[2021-12-12 00:36:21,158][osd5][DEBUG ] detect platform information from remote host
[2021-12-12 00:36:21,176][osd5][DEBUG ] detect machine type
[2021-12-12 00:36:21,182][osd5][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2021-12-12 00:36:21,189][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to client
[2021-12-12 00:36:21,616][client][DEBUG ] connection detected need for sudo
[2021-12-12 00:36:21,866][client][DEBUG ] connected to host: client 
[2021-12-12 00:36:21,866][client][DEBUG ] detect platform information from remote host
[2021-12-12 00:36:21,891][client][DEBUG ] detect machine type
[2021-12-12 00:36:21,896][client][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2021-12-12 00:36:21,898][ceph_deploy.admin][ERROR ] RuntimeError: config file /etc/ceph/ceph.conf exists with different content; use --overwrite-conf to overwrite
[2021-12-12 00:36:21,898][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to client2
[2021-12-12 00:36:22,099][client2][DEBUG ] connection detected need for sudo
[2021-12-12 00:36:22,270][client2][DEBUG ] connected to host: client2 
[2021-12-12 00:36:22,271][client2][DEBUG ] detect platform information from remote host
[2021-12-12 00:36:22,288][client2][DEBUG ] detect machine type
[2021-12-12 00:36:22,293][client2][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2021-12-12 00:36:22,295][ceph_deploy.admin][ERROR ] RuntimeError: config file /etc/ceph/ceph.conf exists with different content; use --overwrite-conf to overwrite
[2021-12-12 00:36:22,296][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to client3
[2021-12-12 00:36:22,699][client3][DEBUG ] connection detected need for sudo
[2021-12-12 00:36:22,969][client3][DEBUG ] connected to host: client3 
[2021-12-12 00:36:22,969][client3][DEBUG ] detect platform information from remote host
[2021-12-12 00:36:23,013][client3][DEBUG ] detect machine type
[2021-12-12 00:36:23,018][client3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2021-12-12 00:36:23,020][ceph_deploy.admin][ERROR ] RuntimeError: config file /etc/ceph/ceph.conf exists with different content; use --overwrite-conf to overwrite
[2021-12-12 00:36:23,020][ceph_deploy][ERROR ] GenericError: Failed to configure 4 admin hosts

[2021-12-12 00:36:30,519][ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephuser/.cephdeploy.conf
[2021-12-12 00:36:30,519][ceph_deploy.cli][INFO  ] Invoked (1.5.39): /usr/bin/ceph-deploy --overwrite-conf admin ceph-admin mon1 osd1 osd2 osd3 osd4 osd5 client client2 client3
[2021-12-12 00:36:30,519][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2021-12-12 00:36:30,519][ceph_deploy.cli][INFO  ]  username                      : None
[2021-12-12 00:36:30,519][ceph_deploy.cli][INFO  ]  verbose                       : False
[2021-12-12 00:36:30,519][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2021-12-12 00:36:30,519][ceph_deploy.cli][INFO  ]  quiet                         : False
[2021-12-12 00:36:30,519][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f8066064b48>
[2021-12-12 00:36:30,519][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2021-12-12 00:36:30,519][ceph_deploy.cli][INFO  ]  client                        : ['ceph-admin', 'mon1', 'osd1', 'osd2', 'osd3', 'osd4', 'osd5', 'client', 'client2', 'client3']
[2021-12-12 00:36:30,520][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7f8066d7da28>
[2021-12-12 00:36:30,520][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2021-12-12 00:36:30,520][ceph_deploy.cli][INFO  ]  default_release               : False
[2021-12-12 00:36:30,520][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to ceph-admin
[2021-12-12 00:36:30,552][ceph-admin][DEBUG ] connection detected need for sudo
[2021-12-12 00:36:30,577][ceph-admin][DEBUG ] connected to host: ceph-admin 
[2021-12-12 00:36:30,577][ceph-admin][DEBUG ] detect platform information from remote host
[2021-12-12 00:36:30,594][ceph-admin][DEBUG ] detect machine type
[2021-12-12 00:36:30,598][ceph-admin][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2021-12-12 00:36:30,599][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to mon1
[2021-12-12 00:36:30,749][mon1][DEBUG ] connection detected need for sudo
[2021-12-12 00:36:30,912][mon1][DEBUG ] connected to host: mon1 
[2021-12-12 00:36:30,912][mon1][DEBUG ] detect platform information from remote host
[2021-12-12 00:36:30,930][mon1][DEBUG ] detect machine type
[2021-12-12 00:36:30,935][mon1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2021-12-12 00:36:30,938][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to osd1
[2021-12-12 00:36:31,093][osd1][DEBUG ] connection detected need for sudo
[2021-12-12 00:36:31,261][osd1][DEBUG ] connected to host: osd1 
[2021-12-12 00:36:31,262][osd1][DEBUG ] detect platform information from remote host
[2021-12-12 00:36:31,280][osd1][DEBUG ] detect machine type
[2021-12-12 00:36:31,288][osd1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2021-12-12 00:36:31,292][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to osd2
[2021-12-12 00:36:31,485][osd2][DEBUG ] connection detected need for sudo
[2021-12-12 00:36:31,656][osd2][DEBUG ] connected to host: osd2 
[2021-12-12 00:36:31,657][osd2][DEBUG ] detect platform information from remote host
[2021-12-12 00:36:31,674][osd2][DEBUG ] detect machine type
[2021-12-12 00:36:31,678][osd2][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2021-12-12 00:36:31,680][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to osd3
[2021-12-12 00:36:31,835][osd3][DEBUG ] connection detected need for sudo
[2021-12-12 00:36:32,007][osd3][DEBUG ] connected to host: osd3 
[2021-12-12 00:36:32,007][osd3][DEBUG ] detect platform information from remote host
[2021-12-12 00:36:32,025][osd3][DEBUG ] detect machine type
[2021-12-12 00:36:32,030][osd3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2021-12-12 00:36:32,032][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to osd4
[2021-12-12 00:36:32,188][osd4][DEBUG ] connection detected need for sudo
[2021-12-12 00:36:32,359][osd4][DEBUG ] connected to host: osd4 
[2021-12-12 00:36:32,360][osd4][DEBUG ] detect platform information from remote host
[2021-12-12 00:36:32,377][osd4][DEBUG ] detect machine type
[2021-12-12 00:36:32,381][osd4][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2021-12-12 00:36:32,384][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to osd5
[2021-12-12 00:36:32,536][osd5][DEBUG ] connection detected need for sudo
[2021-12-12 00:36:32,702][osd5][DEBUG ] connected to host: osd5 
[2021-12-12 00:36:32,702][osd5][DEBUG ] detect platform information from remote host
[2021-12-12 00:36:32,719][osd5][DEBUG ] detect machine type
[2021-12-12 00:36:32,723][osd5][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2021-12-12 00:36:32,726][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to client
[2021-12-12 00:36:32,879][client][DEBUG ] connection detected need for sudo
[2021-12-12 00:36:33,047][client][DEBUG ] connected to host: client 
[2021-12-12 00:36:33,047][client][DEBUG ] detect platform information from remote host
[2021-12-12 00:36:33,065][client][DEBUG ] detect machine type
[2021-12-12 00:36:33,070][client][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2021-12-12 00:36:33,073][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to client2
[2021-12-12 00:36:33,223][client2][DEBUG ] connection detected need for sudo
[2021-12-12 00:36:33,388][client2][DEBUG ] connected to host: client2 
[2021-12-12 00:36:33,389][client2][DEBUG ] detect platform information from remote host
[2021-12-12 00:36:33,405][client2][DEBUG ] detect machine type
[2021-12-12 00:36:33,410][client2][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2021-12-12 00:36:33,412][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to client3
[2021-12-12 00:36:33,566][client3][DEBUG ] connection detected need for sudo
[2021-12-12 00:36:33,737][client3][DEBUG ] connected to host: client3 
[2021-12-12 00:36:33,737][client3][DEBUG ] detect platform information from remote host
[2021-12-12 00:36:33,756][client3][DEBUG ] detect machine type
[2021-12-12 00:36:33,761][client3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2021-12-12 18:09:25,316][ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephuser/.cephdeploy.conf
[2021-12-12 18:09:25,347][ceph_deploy.cli][INFO  ] Invoked (1.5.39): /usr/bin/ceph-deploy disk list osd6
[2021-12-12 18:09:25,347][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2021-12-12 18:09:25,347][ceph_deploy.cli][INFO  ]  username                      : None
[2021-12-12 18:09:25,347][ceph_deploy.cli][INFO  ]  verbose                       : False
[2021-12-12 18:09:25,347][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2021-12-12 18:09:25,347][ceph_deploy.cli][INFO  ]  subcommand                    : list
[2021-12-12 18:09:25,347][ceph_deploy.cli][INFO  ]  quiet                         : False
[2021-12-12 18:09:25,347][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f31722efef0>
[2021-12-12 18:09:25,348][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2021-12-12 18:09:25,348][ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7f31722cb0c8>
[2021-12-12 18:09:25,348][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2021-12-12 18:09:25,348][ceph_deploy.cli][INFO  ]  default_release               : False
[2021-12-12 18:09:25,348][ceph_deploy.cli][INFO  ]  disk                          : [('osd6', None, None)]
[2021-12-12 18:09:35,418][ceph_deploy][ERROR ] RuntimeError: connecting to host: osd6 resulted in errors: HostNotFound osd6

[2021-12-12 18:09:55,059][ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephuser/.cephdeploy.conf
[2021-12-12 18:09:55,060][ceph_deploy.cli][INFO  ] Invoked (1.5.39): /usr/bin/ceph-deploy disk list osd6
[2021-12-12 18:09:55,060][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2021-12-12 18:09:55,060][ceph_deploy.cli][INFO  ]  username                      : None
[2021-12-12 18:09:55,060][ceph_deploy.cli][INFO  ]  verbose                       : False
[2021-12-12 18:09:55,060][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2021-12-12 18:09:55,060][ceph_deploy.cli][INFO  ]  subcommand                    : list
[2021-12-12 18:09:55,060][ceph_deploy.cli][INFO  ]  quiet                         : False
[2021-12-12 18:09:55,060][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fb50b54aef0>
[2021-12-12 18:09:55,060][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2021-12-12 18:09:55,060][ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7fb50b5260c8>
[2021-12-12 18:09:55,060][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2021-12-12 18:09:55,060][ceph_deploy.cli][INFO  ]  default_release               : False
[2021-12-12 18:09:55,060][ceph_deploy.cli][INFO  ]  disk                          : [('osd6', None, None)]
[2021-12-12 18:09:55,240][osd6][DEBUG ] connection detected need for sudo
[2021-12-12 18:09:55,455][osd6][DEBUG ] connected to host: osd6 
[2021-12-12 18:09:55,490][osd6][DEBUG ] detect platform information from remote host
[2021-12-12 18:09:55,513][osd6][DEBUG ] detect machine type
[2021-12-12 18:09:55,532][osd6][DEBUG ] find the location of an executable
[2021-12-12 18:09:55,534][ceph_deploy.osd][INFO  ] Distro info: CentOS Linux 7.9.2009 Core
[2021-12-12 18:09:55,534][ceph_deploy.osd][DEBUG ] Listing disks on osd6...
[2021-12-12 18:09:55,534][osd6][DEBUG ] find the location of an executable
[2021-12-12 18:09:55,537][osd6][INFO  ] Running command: sudo /usr/sbin/ceph-disk list
[2021-12-12 18:09:55,829][osd6][DEBUG ] /dev/dm-0 other, xfs, mounted on /
[2021-12-12 18:09:55,829][osd6][DEBUG ] /dev/dm-1 swap, swap
[2021-12-12 18:09:55,829][osd6][DEBUG ] /dev/sda :
[2021-12-12 18:09:55,829][osd6][DEBUG ]  /dev/sda2 other, LVM2_member
[2021-12-12 18:09:55,829][osd6][DEBUG ]  /dev/sda1 other, xfs, mounted on /boot
[2021-12-12 18:09:55,829][osd6][DEBUG ] /dev/sdb :
[2021-12-12 18:09:55,829][osd6][DEBUG ]  /dev/sdb2 ceph journal, for /dev/sdb1
[2021-12-12 18:09:55,829][osd6][DEBUG ]  /dev/sdb1 ceph data, active, cluster ceph, osd.4, journal /dev/sdb2
[2021-12-12 18:10:51,288][ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephuser/.cephdeploy.conf
[2021-12-12 18:10:51,289][ceph_deploy.cli][INFO  ] Invoked (1.5.39): /usr/bin/ceph-deploy disk zap osd6:/dev/sdb
[2021-12-12 18:10:51,289][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2021-12-12 18:10:51,289][ceph_deploy.cli][INFO  ]  username                      : None
[2021-12-12 18:10:51,289][ceph_deploy.cli][INFO  ]  verbose                       : False
[2021-12-12 18:10:51,289][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2021-12-12 18:10:51,289][ceph_deploy.cli][INFO  ]  subcommand                    : zap
[2021-12-12 18:10:51,289][ceph_deploy.cli][INFO  ]  quiet                         : False
[2021-12-12 18:10:51,289][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fcb45d0def0>
[2021-12-12 18:10:51,289][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2021-12-12 18:10:51,289][ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7fcb45ce90c8>
[2021-12-12 18:10:51,289][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2021-12-12 18:10:51,289][ceph_deploy.cli][INFO  ]  default_release               : False
[2021-12-12 18:10:51,289][ceph_deploy.cli][INFO  ]  disk                          : [('osd6', '/dev/sdb', None)]
[2021-12-12 18:10:51,289][ceph_deploy.osd][DEBUG ] zapping /dev/sdb on osd6
[2021-12-12 18:10:51,494][osd6][DEBUG ] connection detected need for sudo
[2021-12-12 18:10:51,668][osd6][DEBUG ] connected to host: osd6 
[2021-12-12 18:10:51,669][osd6][DEBUG ] detect platform information from remote host
[2021-12-12 18:10:51,685][osd6][DEBUG ] detect machine type
[2021-12-12 18:10:51,689][osd6][DEBUG ] find the location of an executable
[2021-12-12 18:10:51,690][ceph_deploy.osd][INFO  ] Distro info: CentOS Linux 7.9.2009 Core
[2021-12-12 18:10:51,690][osd6][DEBUG ] zeroing last few blocks of device
[2021-12-12 18:10:51,691][osd6][DEBUG ] find the location of an executable
[2021-12-12 18:10:51,694][osd6][INFO  ] Running command: sudo /usr/sbin/ceph-disk zap /dev/sdb
[2021-12-12 18:10:51,811][osd6][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[2021-12-12 18:10:51,812][osd6][WARNING] backup header from main header.
[2021-12-12 18:10:51,812][osd6][WARNING] 
[2021-12-12 18:10:51,812][osd6][WARNING] Warning! Main and backup partition tables differ! Use the 'c' and 'e' options
[2021-12-12 18:10:51,812][osd6][WARNING] on the recovery & transformation menu to examine the two tables.
[2021-12-12 18:10:51,812][osd6][WARNING] 
[2021-12-12 18:10:51,812][osd6][WARNING] Warning! One or more CRCs don't match. You should repair the disk!
[2021-12-12 18:10:51,812][osd6][WARNING] 
[2021-12-12 18:10:52,840][osd6][DEBUG ] ****************************************************************************
[2021-12-12 18:10:52,841][osd6][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[2021-12-12 18:10:52,841][osd6][DEBUG ] verification and recovery are STRONGLY recommended.
[2021-12-12 18:10:52,841][osd6][DEBUG ] ****************************************************************************
[2021-12-12 18:10:52,841][osd6][DEBUG ] Warning: The kernel is still using the old partition table.
[2021-12-12 18:10:52,841][osd6][DEBUG ] The new table will be used at the next reboot.
[2021-12-12 18:10:52,841][osd6][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[2021-12-12 18:10:52,841][osd6][DEBUG ] other utilities.
[2021-12-12 18:10:53,972][osd6][DEBUG ] Creating new GPT entries.
[2021-12-12 18:10:53,972][osd6][DEBUG ] Warning: The kernel is still using the old partition table.
[2021-12-12 18:10:53,972][osd6][DEBUG ] The new table will be used at the next reboot.
[2021-12-12 18:10:53,972][osd6][DEBUG ] The operation has completed successfully.
[2021-12-12 18:11:21,011][ceph_deploy][ERROR ] KeyboardInterrupt

[2021-12-12 18:12:11,775][ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephuser/.cephdeploy.conf
[2021-12-12 18:12:11,775][ceph_deploy.cli][INFO  ] Invoked (1.5.39): /usr/bin/ceph-deploy osd prepare osd6:/dev/sdb1
[2021-12-12 18:12:11,775][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2021-12-12 18:12:11,775][ceph_deploy.cli][INFO  ]  username                      : None
[2021-12-12 18:12:11,775][ceph_deploy.cli][INFO  ]  block_db                      : None
[2021-12-12 18:12:11,775][ceph_deploy.cli][INFO  ]  disk                          : [('osd6', '/dev/sdb1', None)]
[2021-12-12 18:12:11,775][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2021-12-12 18:12:11,775][ceph_deploy.cli][INFO  ]  verbose                       : False
[2021-12-12 18:12:11,775][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2021-12-12 18:12:11,775][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2021-12-12 18:12:11,775][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2021-12-12 18:12:11,775][ceph_deploy.cli][INFO  ]  subcommand                    : prepare
[2021-12-12 18:12:11,775][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2021-12-12 18:12:11,775][ceph_deploy.cli][INFO  ]  quiet                         : False
[2021-12-12 18:12:11,776][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f18dbf84128>
[2021-12-12 18:12:11,776][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2021-12-12 18:12:11,776][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2021-12-12 18:12:11,776][ceph_deploy.cli][INFO  ]  filestore                     : None
[2021-12-12 18:12:11,776][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f18dbfd1050>
[2021-12-12 18:12:11,776][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2021-12-12 18:12:11,776][ceph_deploy.cli][INFO  ]  default_release               : False
[2021-12-12 18:12:11,776][ceph_deploy.cli][INFO  ]  zap_disk                      : False
[2021-12-12 18:12:11,776][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks osd6:/dev/sdb1:
[2021-12-12 18:12:11,992][osd6][DEBUG ] connection detected need for sudo
[2021-12-12 18:12:12,155][osd6][DEBUG ] connected to host: osd6 
[2021-12-12 18:12:12,155][osd6][DEBUG ] detect platform information from remote host
[2021-12-12 18:12:12,171][osd6][DEBUG ] detect machine type
[2021-12-12 18:12:12,176][osd6][DEBUG ] find the location of an executable
[2021-12-12 18:12:12,177][ceph_deploy.osd][INFO  ] Distro info: CentOS Linux 7.9.2009 Core
[2021-12-12 18:12:12,177][ceph_deploy.osd][DEBUG ] Deploying osd to osd6
[2021-12-12 18:12:12,178][osd6][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2021-12-12 18:12:12,180][ceph_deploy.osd][DEBUG ] Preparing host osd6 disk /dev/sdb1 journal None activate False
[2021-12-12 18:12:12,180][osd6][DEBUG ] find the location of an executable
[2021-12-12 18:12:12,182][osd6][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdb1
[2021-12-12 18:12:12,250][osd6][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2021-12-12 18:12:12,265][osd6][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --log-file $run_dir/$cluster-osd-check.log --cluster ceph --setuser ceph --setgroup ceph
[2021-12-12 18:12:12,281][osd6][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --log-file $run_dir/$cluster-osd-check.log --cluster ceph --setuser ceph --setgroup ceph
[2021-12-12 18:12:12,296][osd6][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --log-file $run_dir/$cluster-osd-check.log --cluster ceph --setuser ceph --setgroup ceph
[2021-12-12 18:12:12,312][osd6][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb1 uuid path is /sys/dev/block/8:17/dm/uuid
[2021-12-12 18:12:12,312][osd6][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2021-12-12 18:12:12,327][osd6][WARNING] Traceback (most recent call last):
[2021-12-12 18:12:12,328][osd6][WARNING]   File "/usr/sbin/ceph-disk", line 9, in <module>
[2021-12-12 18:12:12,328][osd6][WARNING]     load_entry_point('ceph-disk==1.0.0', 'console_scripts', 'ceph-disk')()
[2021-12-12 18:12:12,328][osd6][WARNING]   File "/usr/lib/python2.7/site-packages/ceph_disk/main.py", line 5361, in run
[2021-12-12 18:12:12,359][osd6][WARNING]     main(sys.argv[1:])
[2021-12-12 18:12:12,359][osd6][WARNING]   File "/usr/lib/python2.7/site-packages/ceph_disk/main.py", line 5312, in main
[2021-12-12 18:12:12,359][osd6][WARNING]     args.func(args)
[2021-12-12 18:12:12,360][osd6][WARNING]   File "/usr/lib/python2.7/site-packages/ceph_disk/main.py", line 1890, in main
[2021-12-12 18:12:12,360][osd6][WARNING]     Prepare.factory(args).prepare()
[2021-12-12 18:12:12,360][osd6][WARNING]   File "/usr/lib/python2.7/site-packages/ceph_disk/main.py", line 1879, in prepare
[2021-12-12 18:12:12,360][osd6][WARNING]     self.prepare_locked()
[2021-12-12 18:12:12,360][osd6][WARNING]   File "/usr/lib/python2.7/site-packages/ceph_disk/main.py", line 1910, in prepare_locked
[2021-12-12 18:12:12,360][osd6][WARNING]     self.data.prepare(self.journal)
[2021-12-12 18:12:12,360][osd6][WARNING]   File "/usr/lib/python2.7/site-packages/ceph_disk/main.py", line 2578, in prepare
[2021-12-12 18:12:12,360][osd6][WARNING]     self.prepare_device(*to_prepare_list)
[2021-12-12 18:12:12,360][osd6][WARNING]   File "/usr/lib/python2.7/site-packages/ceph_disk/main.py", line 2738, in prepare_device
[2021-12-12 18:12:12,360][osd6][WARNING]     super(PrepareFilestoreData, self).prepare_device(*to_prepare_list)
[2021-12-12 18:12:12,360][osd6][WARNING]   File "/usr/lib/python2.7/site-packages/ceph_disk/main.py", line 2640, in prepare_device
[2021-12-12 18:12:12,360][osd6][WARNING]     self.sanity_checks()
[2021-12-12 18:12:12,360][osd6][WARNING]   File "/usr/lib/python2.7/site-packages/ceph_disk/main.py", line 2603, in sanity_checks
[2021-12-12 18:12:12,360][osd6][WARNING]     check_partitions=not self.args.dmcrypt)
[2021-12-12 18:12:12,360][osd6][WARNING]   File "/usr/lib/python2.7/site-packages/ceph_disk/main.py", line 877, in verify_not_in_use
[2021-12-12 18:12:12,360][osd6][WARNING]     raise Error('Device is mounted', dev)
[2021-12-12 18:12:12,360][osd6][WARNING] ceph_disk.main.Error: Error: Device is mounted: /dev/sdb1
[2021-12-12 18:12:12,423][osd6][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2021-12-12 18:12:12,423][ceph_deploy.osd][ERROR ] Failed to execute command: /usr/sbin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdb1
[2021-12-12 18:12:12,423][ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

[2021-12-12 18:12:36,114][ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephuser/.cephdeploy.conf
[2021-12-12 18:12:36,115][ceph_deploy.cli][INFO  ] Invoked (1.5.39): /usr/bin/ceph-deploy disk zap osd6:/dev/sdb
[2021-12-12 18:12:36,115][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2021-12-12 18:12:36,115][ceph_deploy.cli][INFO  ]  username                      : None
[2021-12-12 18:12:36,116][ceph_deploy.cli][INFO  ]  verbose                       : False
[2021-12-12 18:12:36,116][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2021-12-12 18:12:36,116][ceph_deploy.cli][INFO  ]  subcommand                    : zap
[2021-12-12 18:12:36,116][ceph_deploy.cli][INFO  ]  quiet                         : False
[2021-12-12 18:12:36,116][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f3634b59ef0>
[2021-12-12 18:12:36,116][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2021-12-12 18:12:36,116][ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7f3634b350c8>
[2021-12-12 18:12:36,116][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2021-12-12 18:12:36,116][ceph_deploy.cli][INFO  ]  default_release               : False
[2021-12-12 18:12:36,116][ceph_deploy.cli][INFO  ]  disk                          : [('osd6', '/dev/sdb', None)]
[2021-12-12 18:12:36,116][ceph_deploy.osd][DEBUG ] zapping /dev/sdb on osd6
[2021-12-12 18:12:36,327][osd6][DEBUG ] connection detected need for sudo
[2021-12-12 18:12:36,508][osd6][DEBUG ] connected to host: osd6 
[2021-12-12 18:12:36,508][osd6][DEBUG ] detect platform information from remote host
[2021-12-12 18:12:36,535][osd6][DEBUG ] detect machine type
[2021-12-12 18:12:36,550][osd6][DEBUG ] find the location of an executable
[2021-12-12 18:12:36,551][ceph_deploy.osd][INFO  ] Distro info: CentOS Linux 7.9.2009 Core
[2021-12-12 18:12:36,551][osd6][DEBUG ] zeroing last few blocks of device
[2021-12-12 18:12:36,552][osd6][DEBUG ] find the location of an executable
[2021-12-12 18:12:36,555][osd6][INFO  ] Running command: sudo /usr/sbin/ceph-disk zap /dev/sdb
[2021-12-12 18:12:36,630][osd6][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[2021-12-12 18:12:36,630][osd6][WARNING] backup header from main header.
[2021-12-12 18:12:36,630][osd6][WARNING] 
[2021-12-12 18:12:37,776][osd6][DEBUG ] ****************************************************************************
[2021-12-12 18:12:37,776][osd6][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[2021-12-12 18:12:37,776][osd6][DEBUG ] verification and recovery are STRONGLY recommended.
[2021-12-12 18:12:37,776][osd6][DEBUG ] ****************************************************************************
[2021-12-12 18:12:37,776][osd6][DEBUG ] Warning: The kernel is still using the old partition table.
[2021-12-12 18:12:37,776][osd6][DEBUG ] The new table will be used at the next reboot.
[2021-12-12 18:12:37,776][osd6][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[2021-12-12 18:12:37,776][osd6][DEBUG ] other utilities.
[2021-12-12 18:12:38,813][osd6][DEBUG ] Creating new GPT entries.
[2021-12-12 18:12:38,813][osd6][DEBUG ] Warning: The kernel is still using the old partition table.
[2021-12-12 18:12:38,813][osd6][DEBUG ] The new table will be used at the next reboot.
[2021-12-12 18:12:38,813][osd6][DEBUG ] The operation has completed successfully.
[2021-12-12 18:13:33,933][ceph_deploy][ERROR ] KeyboardInterrupt

[2021-12-12 18:13:46,587][ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephuser/.cephdeploy.conf
[2021-12-12 18:13:46,588][ceph_deploy.cli][INFO  ] Invoked (1.5.39): /usr/bin/ceph-deploy osd prepare osd6:/dev/sdb
[2021-12-12 18:13:46,588][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2021-12-12 18:13:46,588][ceph_deploy.cli][INFO  ]  username                      : None
[2021-12-12 18:13:46,588][ceph_deploy.cli][INFO  ]  block_db                      : None
[2021-12-12 18:13:46,588][ceph_deploy.cli][INFO  ]  disk                          : [('osd6', '/dev/sdb', None)]
[2021-12-12 18:13:46,588][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2021-12-12 18:13:46,588][ceph_deploy.cli][INFO  ]  verbose                       : False
[2021-12-12 18:13:46,588][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2021-12-12 18:13:46,588][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2021-12-12 18:13:46,588][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2021-12-12 18:13:46,588][ceph_deploy.cli][INFO  ]  subcommand                    : prepare
[2021-12-12 18:13:46,588][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2021-12-12 18:13:46,588][ceph_deploy.cli][INFO  ]  quiet                         : False
[2021-12-12 18:13:46,588][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fa27a545128>
[2021-12-12 18:13:46,588][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2021-12-12 18:13:46,588][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2021-12-12 18:13:46,588][ceph_deploy.cli][INFO  ]  filestore                     : None
[2021-12-12 18:13:46,588][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fa27a592050>
[2021-12-12 18:13:46,596][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2021-12-12 18:13:46,596][ceph_deploy.cli][INFO  ]  default_release               : False
[2021-12-12 18:13:46,596][ceph_deploy.cli][INFO  ]  zap_disk                      : False
[2021-12-12 18:13:46,597][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks osd6:/dev/sdb:
[2021-12-12 18:13:46,759][osd6][DEBUG ] connection detected need for sudo
[2021-12-12 18:13:46,920][osd6][DEBUG ] connected to host: osd6 
[2021-12-12 18:13:46,921][osd6][DEBUG ] detect platform information from remote host
[2021-12-12 18:13:46,937][osd6][DEBUG ] detect machine type
[2021-12-12 18:13:46,942][osd6][DEBUG ] find the location of an executable
[2021-12-12 18:13:46,943][ceph_deploy.osd][INFO  ] Distro info: CentOS Linux 7.9.2009 Core
[2021-12-12 18:13:46,943][ceph_deploy.osd][DEBUG ] Deploying osd to osd6
[2021-12-12 18:13:46,943][osd6][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2021-12-12 18:13:46,945][ceph_deploy.osd][DEBUG ] Preparing host osd6 disk /dev/sdb journal None activate False
[2021-12-12 18:13:46,945][osd6][DEBUG ] find the location of an executable
[2021-12-12 18:13:46,947][osd6][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdb
[2021-12-12 18:13:47,015][osd6][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2021-12-12 18:13:47,030][osd6][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --log-file $run_dir/$cluster-osd-check.log --cluster ceph --setuser ceph --setgroup ceph
[2021-12-12 18:13:47,046][osd6][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --log-file $run_dir/$cluster-osd-check.log --cluster ceph --setuser ceph --setgroup ceph
[2021-12-12 18:13:47,062][osd6][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --log-file $run_dir/$cluster-osd-check.log --cluster ceph --setuser ceph --setgroup ceph
[2021-12-12 18:13:47,077][osd6][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-12 18:13:47,077][osd6][WARNING] set_type: Will colocate journal with data on /dev/sdb
[2021-12-12 18:13:47,077][osd6][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2021-12-12 18:13:47,109][osd6][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-12 18:13:47,109][osd6][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-12 18:13:47,109][osd6][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-12 18:13:47,109][osd6][WARNING] Traceback (most recent call last):
[2021-12-12 18:13:47,109][osd6][WARNING]   File "/usr/sbin/ceph-disk", line 9, in <module>
[2021-12-12 18:13:47,109][osd6][WARNING]     load_entry_point('ceph-disk==1.0.0', 'console_scripts', 'ceph-disk')()
[2021-12-12 18:13:47,109][osd6][WARNING]   File "/usr/lib/python2.7/site-packages/ceph_disk/main.py", line 5361, in run
[2021-12-12 18:13:47,109][osd6][WARNING]     main(sys.argv[1:])
[2021-12-12 18:13:47,109][osd6][WARNING]   File "/usr/lib/python2.7/site-packages/ceph_disk/main.py", line 5312, in main
[2021-12-12 18:13:47,109][osd6][WARNING]     args.func(args)
[2021-12-12 18:13:47,109][osd6][WARNING]   File "/usr/lib/python2.7/site-packages/ceph_disk/main.py", line 1890, in main
[2021-12-12 18:13:47,109][osd6][WARNING]     Prepare.factory(args).prepare()
[2021-12-12 18:13:47,109][osd6][WARNING]   File "/usr/lib/python2.7/site-packages/ceph_disk/main.py", line 1879, in prepare
[2021-12-12 18:13:47,110][osd6][WARNING]     self.prepare_locked()
[2021-12-12 18:13:47,110][osd6][WARNING]   File "/usr/lib/python2.7/site-packages/ceph_disk/main.py", line 1910, in prepare_locked
[2021-12-12 18:13:47,110][osd6][WARNING]     self.data.prepare(self.journal)
[2021-12-12 18:13:47,110][osd6][WARNING]   File "/usr/lib/python2.7/site-packages/ceph_disk/main.py", line 2578, in prepare
[2021-12-12 18:13:47,110][osd6][WARNING]     self.prepare_device(*to_prepare_list)
[2021-12-12 18:13:47,110][osd6][WARNING]   File "/usr/lib/python2.7/site-packages/ceph_disk/main.py", line 2738, in prepare_device
[2021-12-12 18:13:47,110][osd6][WARNING]     super(PrepareFilestoreData, self).prepare_device(*to_prepare_list)
[2021-12-12 18:13:47,110][osd6][WARNING]   File "/usr/lib/python2.7/site-packages/ceph_disk/main.py", line 2640, in prepare_device
[2021-12-12 18:13:47,110][osd6][WARNING]     self.sanity_checks()
[2021-12-12 18:13:47,110][osd6][WARNING]   File "/usr/lib/python2.7/site-packages/ceph_disk/main.py", line 2603, in sanity_checks
[2021-12-12 18:13:47,110][osd6][WARNING]     check_partitions=not self.args.dmcrypt)
[2021-12-12 18:13:47,110][osd6][WARNING]   File "/usr/lib/python2.7/site-packages/ceph_disk/main.py", line 887, in verify_not_in_use
[2021-12-12 18:13:47,110][osd6][WARNING]     raise Error('Device is mounted', partition)
[2021-12-12 18:13:47,110][osd6][WARNING] ceph_disk.main.Error: Error: Device is mounted: /dev/sdb1
[2021-12-12 18:13:47,110][osd6][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2021-12-12 18:13:47,110][ceph_deploy.osd][ERROR ] Failed to execute command: /usr/sbin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdb
[2021-12-12 18:13:47,111][ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

[2021-12-12 18:15:23,410][ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephuser/.cephdeploy.conf
[2021-12-12 18:15:23,411][ceph_deploy.cli][INFO  ] Invoked (1.5.39): /usr/bin/ceph-deploy osd prepare osd6:/dev/sdb
[2021-12-12 18:15:23,411][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2021-12-12 18:15:23,411][ceph_deploy.cli][INFO  ]  username                      : None
[2021-12-12 18:15:23,411][ceph_deploy.cli][INFO  ]  block_db                      : None
[2021-12-12 18:15:23,411][ceph_deploy.cli][INFO  ]  disk                          : [('osd6', '/dev/sdb', None)]
[2021-12-12 18:15:23,411][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2021-12-12 18:15:23,411][ceph_deploy.cli][INFO  ]  verbose                       : False
[2021-12-12 18:15:23,411][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2021-12-12 18:15:23,411][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2021-12-12 18:15:23,411][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2021-12-12 18:15:23,411][ceph_deploy.cli][INFO  ]  subcommand                    : prepare
[2021-12-12 18:15:23,411][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2021-12-12 18:15:23,411][ceph_deploy.cli][INFO  ]  quiet                         : False
[2021-12-12 18:15:23,411][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fdf355b2128>
[2021-12-12 18:15:23,411][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2021-12-12 18:15:23,411][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2021-12-12 18:15:23,411][ceph_deploy.cli][INFO  ]  filestore                     : None
[2021-12-12 18:15:23,412][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fdf355ff050>
[2021-12-12 18:15:23,412][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2021-12-12 18:15:23,412][ceph_deploy.cli][INFO  ]  default_release               : False
[2021-12-12 18:15:23,412][ceph_deploy.cli][INFO  ]  zap_disk                      : False
[2021-12-12 18:15:23,412][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks osd6:/dev/sdb:
[2021-12-12 18:15:23,628][osd6][DEBUG ] connection detected need for sudo
[2021-12-12 18:15:23,829][osd6][DEBUG ] connected to host: osd6 
[2021-12-12 18:15:23,829][osd6][DEBUG ] detect platform information from remote host
[2021-12-12 18:15:23,846][osd6][DEBUG ] detect machine type
[2021-12-12 18:15:23,872][osd6][DEBUG ] find the location of an executable
[2021-12-12 18:15:23,888][ceph_deploy.osd][INFO  ] Distro info: CentOS Linux 7.9.2009 Core
[2021-12-12 18:15:23,888][ceph_deploy.osd][DEBUG ] Deploying osd to osd6
[2021-12-12 18:15:23,888][osd6][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2021-12-12 18:15:23,892][ceph_deploy.osd][DEBUG ] Preparing host osd6 disk /dev/sdb journal None activate False
[2021-12-12 18:15:23,892][osd6][DEBUG ] find the location of an executable
[2021-12-12 18:15:23,896][osd6][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdb
[2021-12-12 18:15:23,988][osd6][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2021-12-12 18:15:23,988][osd6][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --log-file $run_dir/$cluster-osd-check.log --cluster ceph --setuser ceph --setgroup ceph
[2021-12-12 18:15:24,005][osd6][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --log-file $run_dir/$cluster-osd-check.log --cluster ceph --setuser ceph --setgroup ceph
[2021-12-12 18:15:24,009][osd6][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --log-file $run_dir/$cluster-osd-check.log --cluster ceph --setuser ceph --setgroup ceph
[2021-12-12 18:15:24,029][osd6][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-12 18:15:24,029][osd6][WARNING] set_type: Will colocate journal with data on /dev/sdb
[2021-12-12 18:15:24,029][osd6][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2021-12-12 18:15:24,060][osd6][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-12 18:15:24,061][osd6][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-12 18:15:24,061][osd6][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-12 18:15:24,061][osd6][WARNING] Traceback (most recent call last):
[2021-12-12 18:15:24,061][osd6][WARNING]   File "/usr/sbin/ceph-disk", line 9, in <module>
[2021-12-12 18:15:24,061][osd6][WARNING]     load_entry_point('ceph-disk==1.0.0', 'console_scripts', 'ceph-disk')()
[2021-12-12 18:15:24,061][osd6][WARNING]   File "/usr/lib/python2.7/site-packages/ceph_disk/main.py", line 5361, in run
[2021-12-12 18:15:24,061][osd6][WARNING]     main(sys.argv[1:])
[2021-12-12 18:15:24,061][osd6][WARNING]   File "/usr/lib/python2.7/site-packages/ceph_disk/main.py", line 5312, in main
[2021-12-12 18:15:24,061][osd6][WARNING]     args.func(args)
[2021-12-12 18:15:24,061][osd6][WARNING]   File "/usr/lib/python2.7/site-packages/ceph_disk/main.py", line 1890, in main
[2021-12-12 18:15:24,061][osd6][WARNING]     Prepare.factory(args).prepare()
[2021-12-12 18:15:24,061][osd6][WARNING]   File "/usr/lib/python2.7/site-packages/ceph_disk/main.py", line 1879, in prepare
[2021-12-12 18:15:24,061][osd6][WARNING]     self.prepare_locked()
[2021-12-12 18:15:24,061][osd6][WARNING]   File "/usr/lib/python2.7/site-packages/ceph_disk/main.py", line 1910, in prepare_locked
[2021-12-12 18:15:24,061][osd6][WARNING]     self.data.prepare(self.journal)
[2021-12-12 18:15:24,061][osd6][WARNING]   File "/usr/lib/python2.7/site-packages/ceph_disk/main.py", line 2578, in prepare
[2021-12-12 18:15:24,061][osd6][WARNING]     self.prepare_device(*to_prepare_list)
[2021-12-12 18:15:24,061][osd6][WARNING]   File "/usr/lib/python2.7/site-packages/ceph_disk/main.py", line 2738, in prepare_device
[2021-12-12 18:15:24,062][osd6][WARNING]     super(PrepareFilestoreData, self).prepare_device(*to_prepare_list)
[2021-12-12 18:15:24,062][osd6][WARNING]   File "/usr/lib/python2.7/site-packages/ceph_disk/main.py", line 2640, in prepare_device
[2021-12-12 18:15:24,062][osd6][WARNING]     self.sanity_checks()
[2021-12-12 18:15:24,062][osd6][WARNING]   File "/usr/lib/python2.7/site-packages/ceph_disk/main.py", line 2603, in sanity_checks
[2021-12-12 18:15:24,062][osd6][WARNING]     check_partitions=not self.args.dmcrypt)
[2021-12-12 18:15:24,062][osd6][WARNING]   File "/usr/lib/python2.7/site-packages/ceph_disk/main.py", line 887, in verify_not_in_use
[2021-12-12 18:15:24,062][osd6][WARNING]     raise Error('Device is mounted', partition)
[2021-12-12 18:15:24,062][osd6][WARNING] ceph_disk.main.Error: Error: Device is mounted: /dev/sdb1
[2021-12-12 18:15:24,062][osd6][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2021-12-12 18:15:24,062][ceph_deploy.osd][ERROR ] Failed to execute command: /usr/sbin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdb
[2021-12-12 18:15:24,062][ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

[2021-12-12 18:15:44,393][ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephuser/.cephdeploy.conf
[2021-12-12 18:15:44,393][ceph_deploy.cli][INFO  ] Invoked (1.5.39): /usr/bin/ceph-deploy osd prepare osd6:/dev/sdb1
[2021-12-12 18:15:44,393][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2021-12-12 18:15:44,393][ceph_deploy.cli][INFO  ]  username                      : None
[2021-12-12 18:15:44,393][ceph_deploy.cli][INFO  ]  block_db                      : None
[2021-12-12 18:15:44,394][ceph_deploy.cli][INFO  ]  disk                          : [('osd6', '/dev/sdb1', None)]
[2021-12-12 18:15:44,394][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2021-12-12 18:15:44,395][ceph_deploy.cli][INFO  ]  verbose                       : False
[2021-12-12 18:15:44,395][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2021-12-12 18:15:44,395][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2021-12-12 18:15:44,395][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2021-12-12 18:15:44,395][ceph_deploy.cli][INFO  ]  subcommand                    : prepare
[2021-12-12 18:15:44,395][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2021-12-12 18:15:44,395][ceph_deploy.cli][INFO  ]  quiet                         : False
[2021-12-12 18:15:44,395][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f3c91a53128>
[2021-12-12 18:15:44,395][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2021-12-12 18:15:44,395][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2021-12-12 18:15:44,395][ceph_deploy.cli][INFO  ]  filestore                     : None
[2021-12-12 18:15:44,395][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f3c91aa0050>
[2021-12-12 18:15:44,395][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2021-12-12 18:15:44,395][ceph_deploy.cli][INFO  ]  default_release               : False
[2021-12-12 18:15:44,395][ceph_deploy.cli][INFO  ]  zap_disk                      : False
[2021-12-12 18:15:44,395][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks osd6:/dev/sdb1:
[2021-12-12 18:15:44,606][osd6][DEBUG ] connection detected need for sudo
[2021-12-12 18:15:44,792][osd6][DEBUG ] connected to host: osd6 
[2021-12-12 18:15:44,792][osd6][DEBUG ] detect platform information from remote host
[2021-12-12 18:15:44,815][osd6][DEBUG ] detect machine type
[2021-12-12 18:15:44,835][osd6][DEBUG ] find the location of an executable
[2021-12-12 18:15:44,836][ceph_deploy.osd][INFO  ] Distro info: CentOS Linux 7.9.2009 Core
[2021-12-12 18:15:44,836][ceph_deploy.osd][DEBUG ] Deploying osd to osd6
[2021-12-12 18:15:44,837][osd6][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2021-12-12 18:15:44,839][ceph_deploy.osd][DEBUG ] Preparing host osd6 disk /dev/sdb1 journal None activate False
[2021-12-12 18:15:44,839][osd6][DEBUG ] find the location of an executable
[2021-12-12 18:15:44,848][osd6][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdb1
[2021-12-12 18:15:44,946][osd6][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2021-12-12 18:15:44,946][osd6][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --log-file $run_dir/$cluster-osd-check.log --cluster ceph --setuser ceph --setgroup ceph
[2021-12-12 18:15:44,946][osd6][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --log-file $run_dir/$cluster-osd-check.log --cluster ceph --setuser ceph --setgroup ceph
[2021-12-12 18:15:44,959][osd6][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --log-file $run_dir/$cluster-osd-check.log --cluster ceph --setuser ceph --setgroup ceph
[2021-12-12 18:15:44,992][osd6][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb1 uuid path is /sys/dev/block/8:17/dm/uuid
[2021-12-12 18:15:44,992][osd6][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2021-12-12 18:15:45,004][osd6][WARNING] Traceback (most recent call last):
[2021-12-12 18:15:45,004][osd6][WARNING]   File "/usr/sbin/ceph-disk", line 9, in <module>
[2021-12-12 18:15:45,005][osd6][WARNING]     load_entry_point('ceph-disk==1.0.0', 'console_scripts', 'ceph-disk')()
[2021-12-12 18:15:45,005][osd6][WARNING]   File "/usr/lib/python2.7/site-packages/ceph_disk/main.py", line 5361, in run
[2021-12-12 18:15:45,005][osd6][WARNING]     main(sys.argv[1:])
[2021-12-12 18:15:45,005][osd6][WARNING]   File "/usr/lib/python2.7/site-packages/ceph_disk/main.py", line 5312, in main
[2021-12-12 18:15:45,005][osd6][WARNING]     args.func(args)
[2021-12-12 18:15:45,005][osd6][WARNING]   File "/usr/lib/python2.7/site-packages/ceph_disk/main.py", line 1890, in main
[2021-12-12 18:15:45,005][osd6][WARNING]     Prepare.factory(args).prepare()
[2021-12-12 18:15:45,005][osd6][WARNING]   File "/usr/lib/python2.7/site-packages/ceph_disk/main.py", line 1879, in prepare
[2021-12-12 18:15:45,005][osd6][WARNING]     self.prepare_locked()
[2021-12-12 18:15:45,005][osd6][WARNING]   File "/usr/lib/python2.7/site-packages/ceph_disk/main.py", line 1910, in prepare_locked
[2021-12-12 18:15:45,005][osd6][WARNING]     self.data.prepare(self.journal)
[2021-12-12 18:15:45,005][osd6][WARNING]   File "/usr/lib/python2.7/site-packages/ceph_disk/main.py", line 2578, in prepare
[2021-12-12 18:15:45,005][osd6][WARNING]     self.prepare_device(*to_prepare_list)
[2021-12-12 18:15:45,005][osd6][WARNING]   File "/usr/lib/python2.7/site-packages/ceph_disk/main.py", line 2738, in prepare_device
[2021-12-12 18:15:45,005][osd6][WARNING]     super(PrepareFilestoreData, self).prepare_device(*to_prepare_list)
[2021-12-12 18:15:45,005][osd6][WARNING]   File "/usr/lib/python2.7/site-packages/ceph_disk/main.py", line 2640, in prepare_device
[2021-12-12 18:15:45,005][osd6][WARNING]     self.sanity_checks()
[2021-12-12 18:15:45,005][osd6][WARNING]   File "/usr/lib/python2.7/site-packages/ceph_disk/main.py", line 2603, in sanity_checks
[2021-12-12 18:15:45,006][osd6][WARNING]     check_partitions=not self.args.dmcrypt)
[2021-12-12 18:15:45,006][osd6][WARNING]   File "/usr/lib/python2.7/site-packages/ceph_disk/main.py", line 877, in verify_not_in_use
[2021-12-12 18:15:45,006][osd6][WARNING]     raise Error('Device is mounted', dev)
[2021-12-12 18:15:45,006][osd6][WARNING] ceph_disk.main.Error: Error: Device is mounted: /dev/sdb1
[2021-12-12 18:15:45,006][osd6][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2021-12-12 18:15:45,006][ceph_deploy.osd][ERROR ] Failed to execute command: /usr/sbin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdb1
[2021-12-12 18:15:45,006][ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

[2021-12-12 18:20:50,589][ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephuser/.cephdeploy.conf
[2021-12-12 18:20:50,589][ceph_deploy.cli][INFO  ] Invoked (1.5.39): /usr/bin/ceph-deploy osd prepare osd6:/dev/sdb
[2021-12-12 18:20:50,589][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2021-12-12 18:20:50,589][ceph_deploy.cli][INFO  ]  username                      : None
[2021-12-12 18:20:50,589][ceph_deploy.cli][INFO  ]  block_db                      : None
[2021-12-12 18:20:50,590][ceph_deploy.cli][INFO  ]  disk                          : [('osd6', '/dev/sdb', None)]
[2021-12-12 18:20:50,590][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2021-12-12 18:20:50,590][ceph_deploy.cli][INFO  ]  verbose                       : False
[2021-12-12 18:20:50,590][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2021-12-12 18:20:50,590][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2021-12-12 18:20:50,590][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2021-12-12 18:20:50,590][ceph_deploy.cli][INFO  ]  subcommand                    : prepare
[2021-12-12 18:20:50,590][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2021-12-12 18:20:50,590][ceph_deploy.cli][INFO  ]  quiet                         : False
[2021-12-12 18:20:50,590][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f732e33c128>
[2021-12-12 18:20:50,590][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2021-12-12 18:20:50,590][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2021-12-12 18:20:50,590][ceph_deploy.cli][INFO  ]  filestore                     : None
[2021-12-12 18:20:50,590][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f732e389050>
[2021-12-12 18:20:50,590][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2021-12-12 18:20:50,590][ceph_deploy.cli][INFO  ]  default_release               : False
[2021-12-12 18:20:50,590][ceph_deploy.cli][INFO  ]  zap_disk                      : False
[2021-12-12 18:20:50,590][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks osd6:/dev/sdb:
[2021-12-12 18:20:50,753][osd6][DEBUG ] connection detected need for sudo
[2021-12-12 18:20:50,917][osd6][DEBUG ] connected to host: osd6 
[2021-12-12 18:20:50,918][osd6][DEBUG ] detect platform information from remote host
[2021-12-12 18:20:50,934][osd6][DEBUG ] detect machine type
[2021-12-12 18:20:50,939][osd6][DEBUG ] find the location of an executable
[2021-12-12 18:20:50,940][ceph_deploy.osd][INFO  ] Distro info: CentOS Linux 7.9.2009 Core
[2021-12-12 18:20:50,940][ceph_deploy.osd][DEBUG ] Deploying osd to osd6
[2021-12-12 18:20:50,940][osd6][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2021-12-12 18:20:50,953][ceph_deploy.osd][DEBUG ] Preparing host osd6 disk /dev/sdb journal None activate False
[2021-12-12 18:20:50,953][osd6][DEBUG ] find the location of an executable
[2021-12-12 18:20:50,966][osd6][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdb
[2021-12-12 18:20:51,064][osd6][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2021-12-12 18:20:51,064][osd6][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --log-file $run_dir/$cluster-osd-check.log --cluster ceph --setuser ceph --setgroup ceph
[2021-12-12 18:20:51,075][osd6][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --log-file $run_dir/$cluster-osd-check.log --cluster ceph --setuser ceph --setgroup ceph
[2021-12-12 18:20:51,086][osd6][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --log-file $run_dir/$cluster-osd-check.log --cluster ceph --setuser ceph --setgroup ceph
[2021-12-12 18:20:51,097][osd6][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-12 18:20:51,097][osd6][WARNING] set_type: Will colocate journal with data on /dev/sdb
[2021-12-12 18:20:51,097][osd6][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2021-12-12 18:20:51,128][osd6][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-12 18:20:51,128][osd6][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-12 18:20:51,128][osd6][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-12 18:20:51,128][osd6][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb1 uuid path is /sys/dev/block/8:17/dm/uuid
[2021-12-12 18:20:51,128][osd6][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[2021-12-12 18:20:51,128][osd6][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[2021-12-12 18:20:51,139][osd6][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[2021-12-12 18:20:51,145][osd6][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[2021-12-12 18:20:51,145][osd6][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-12 18:20:51,145][osd6][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-12 18:20:51,145][osd6][WARNING] ptype_tobe_for_name: name = journal
[2021-12-12 18:20:51,145][osd6][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-12 18:20:51,145][osd6][WARNING] create_partition: Creating journal partition num 2 size 5120 on /dev/sdb
[2021-12-12 18:20:51,145][osd6][WARNING] command_check_call: Running command: /sbin/sgdisk --new=2:0:+5120M --change-name=2:ceph journal --partition-guid=2:6f5cb5ed-1e69-4116-b84f-1b7014c7e6eb --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sdb
[2021-12-12 18:20:52,358][osd6][DEBUG ] Setting name!
[2021-12-12 18:20:52,358][osd6][DEBUG ] partNum is 1
[2021-12-12 18:20:52,358][osd6][DEBUG ] REALLY setting name!
[2021-12-12 18:20:52,358][osd6][DEBUG ] The operation has completed successfully.
[2021-12-12 18:20:52,358][osd6][WARNING] update_partition: Calling partprobe on created device /dev/sdb
[2021-12-12 18:20:52,358][osd6][WARNING] command_check_call: Running command: /usr/bin/udevadm settle --timeout=600
[2021-12-12 18:20:52,422][osd6][WARNING] command: Running command: /usr/bin/flock -s /dev/sdb /sbin/partprobe /dev/sdb
[2021-12-12 18:20:52,486][osd6][WARNING] command_check_call: Running command: /usr/bin/udevadm settle --timeout=600
[2021-12-12 18:20:52,550][osd6][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-12 18:20:52,550][osd6][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-12 18:20:52,550][osd6][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb2 uuid path is /sys/dev/block/8:18/dm/uuid
[2021-12-12 18:20:52,550][osd6][WARNING] prepare_device: Journal is GPT partition /dev/disk/by-partuuid/6f5cb5ed-1e69-4116-b84f-1b7014c7e6eb
[2021-12-12 18:20:52,550][osd6][WARNING] prepare_device: Journal is GPT partition /dev/disk/by-partuuid/6f5cb5ed-1e69-4116-b84f-1b7014c7e6eb
[2021-12-12 18:20:52,550][osd6][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-12 18:20:52,550][osd6][WARNING] set_data_partition: Creating osd partition on /dev/sdb
[2021-12-12 18:20:52,550][osd6][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-12 18:20:52,550][osd6][WARNING] ptype_tobe_for_name: name = data
[2021-12-12 18:20:52,550][osd6][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-12 18:20:52,550][osd6][WARNING] create_partition: Creating data partition num 1 size 0 on /dev/sdb
[2021-12-12 18:20:52,551][osd6][WARNING] command_check_call: Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:ee291648-12cf-4e24-951e-ed1f84e9b10a --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be --mbrtogpt -- /dev/sdb
[2021-12-12 18:20:53,709][osd6][DEBUG ] Setting name!
[2021-12-12 18:20:53,709][osd6][DEBUG ] partNum is 0
[2021-12-12 18:20:53,709][osd6][DEBUG ] REALLY setting name!
[2021-12-12 18:20:53,709][osd6][DEBUG ] The operation has completed successfully.
[2021-12-12 18:20:53,709][osd6][WARNING] update_partition: Calling partprobe on created device /dev/sdb
[2021-12-12 18:20:53,709][osd6][WARNING] command_check_call: Running command: /usr/bin/udevadm settle --timeout=600
[2021-12-12 18:20:53,808][osd6][WARNING] command: Running command: /usr/bin/flock -s /dev/sdb /sbin/partprobe /dev/sdb
[2021-12-12 18:20:53,881][osd6][WARNING] command_check_call: Running command: /usr/bin/udevadm settle --timeout=600
[2021-12-12 18:20:54,019][osd6][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-12 18:20:54,019][osd6][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-12 18:20:54,019][osd6][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb1 uuid path is /sys/dev/block/8:17/dm/uuid
[2021-12-12 18:20:54,019][osd6][WARNING] populate_data_path_device: Creating xfs fs on /dev/sdb1
[2021-12-12 18:20:54,019][osd6][WARNING] command_check_call: Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/sdb1
[2021-12-12 18:20:54,219][osd6][DEBUG ] Discarding blocks...Done.
[2021-12-12 18:20:54,219][osd6][DEBUG ] meta-data=/dev/sdb1              isize=2048   agcount=4, agsize=196543 blks
[2021-12-12 18:20:54,219][osd6][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=1
[2021-12-12 18:20:54,219][osd6][DEBUG ]          =                       crc=1        finobt=0, sparse=0
[2021-12-12 18:20:54,219][osd6][DEBUG ] data     =                       bsize=4096   blocks=786171, imaxpct=25
[2021-12-12 18:20:54,219][osd6][DEBUG ]          =                       sunit=0      swidth=0 blks
[2021-12-12 18:20:54,219][osd6][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0 ftype=1
[2021-12-12 18:20:54,219][osd6][DEBUG ] log      =internal log           bsize=4096   blocks=2560, version=2
[2021-12-12 18:20:54,219][osd6][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[2021-12-12 18:20:54,219][osd6][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[2021-12-12 18:20:54,219][osd6][WARNING] mount: Mounting /dev/sdb1 on /var/lib/ceph/tmp/mnt.PDnoWk with options noatime,inode64
[2021-12-12 18:20:54,219][osd6][WARNING] command_check_call: Running command: /usr/bin/mount -t xfs -o noatime,inode64 -- /dev/sdb1 /var/lib/ceph/tmp/mnt.PDnoWk
[2021-12-12 18:20:54,235][osd6][WARNING] command: Running command: /sbin/restorecon /var/lib/ceph/tmp/mnt.PDnoWk
[2021-12-12 18:20:54,235][osd6][WARNING] populate_data_path: Preparing osd data dir /var/lib/ceph/tmp/mnt.PDnoWk
[2021-12-12 18:20:54,399][osd6][WARNING] command: Running command: /sbin/restorecon -R /var/lib/ceph/tmp/mnt.PDnoWk/ceph_fsid.2888.tmp
[2021-12-12 18:20:54,399][osd6][WARNING] command: Running command: /usr/bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.PDnoWk/ceph_fsid.2888.tmp
[2021-12-12 18:20:54,431][osd6][WARNING] command: Running command: /sbin/restorecon -R /var/lib/ceph/tmp/mnt.PDnoWk/fsid.2888.tmp
[2021-12-12 18:20:54,431][osd6][WARNING] command: Running command: /usr/bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.PDnoWk/fsid.2888.tmp
[2021-12-12 18:20:54,446][osd6][WARNING] command: Running command: /sbin/restorecon -R /var/lib/ceph/tmp/mnt.PDnoWk/magic.2888.tmp
[2021-12-12 18:20:54,446][osd6][WARNING] command: Running command: /usr/bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.PDnoWk/magic.2888.tmp
[2021-12-12 18:20:54,462][osd6][WARNING] command: Running command: /sbin/restorecon -R /var/lib/ceph/tmp/mnt.PDnoWk/journal_uuid.2888.tmp
[2021-12-12 18:20:54,465][osd6][WARNING] command: Running command: /usr/bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.PDnoWk/journal_uuid.2888.tmp
[2021-12-12 18:20:54,468][osd6][WARNING] adjust_symlink: Creating symlink /var/lib/ceph/tmp/mnt.PDnoWk/journal -> /dev/disk/by-partuuid/6f5cb5ed-1e69-4116-b84f-1b7014c7e6eb
[2021-12-12 18:20:54,468][osd6][WARNING] command: Running command: /sbin/restorecon -R /var/lib/ceph/tmp/mnt.PDnoWk
[2021-12-12 18:20:54,470][osd6][WARNING] command: Running command: /usr/bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.PDnoWk
[2021-12-12 18:20:54,473][osd6][WARNING] unmount: Unmounting /var/lib/ceph/tmp/mnt.PDnoWk
[2021-12-12 18:20:54,473][osd6][WARNING] command_check_call: Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.PDnoWk
[2021-12-12 18:20:54,787][osd6][WARNING] get_dm_uuid: get_dm_uuid /dev/sdb uuid path is /sys/dev/block/8:16/dm/uuid
[2021-12-12 18:20:54,788][osd6][WARNING] command_check_call: Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/sdb
[2021-12-12 18:20:55,946][osd6][DEBUG ] Warning: The kernel is still using the old partition table.
[2021-12-12 18:20:55,946][osd6][DEBUG ] The new table will be used at the next reboot.
[2021-12-12 18:20:55,946][osd6][DEBUG ] The operation has completed successfully.
[2021-12-12 18:20:55,946][osd6][WARNING] update_partition: Calling partprobe on prepared device /dev/sdb
[2021-12-12 18:20:55,946][osd6][WARNING] command_check_call: Running command: /usr/bin/udevadm settle --timeout=600
[2021-12-12 18:20:55,946][osd6][WARNING] command: Running command: /usr/bin/flock -s /dev/sdb /sbin/partprobe /dev/sdb
[2021-12-12 18:20:56,032][osd6][WARNING] command_check_call: Running command: /usr/bin/udevadm settle --timeout=600
[2021-12-12 18:20:56,187][osd6][WARNING] command_check_call: Running command: /usr/bin/udevadm trigger --action=add --sysname-match sdb1
[2021-12-12 18:21:01,192][osd6][INFO  ] checking OSD status...
[2021-12-12 18:21:01,192][osd6][DEBUG ] find the location of an executable
[2021-12-12 18:21:01,195][osd6][INFO  ] Running command: sudo /bin/ceph --cluster=ceph osd stat --format=json
[2021-12-12 18:21:01,410][ceph_deploy.osd][DEBUG ] Host osd6 is now ready for osd use.
